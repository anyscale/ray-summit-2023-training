{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c399be64-44d3-45e4-9b43-3757a9b92daf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import serve\n",
    "import requests, json\n",
    "from starlette.requests import Request\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7617b253-6d2a-4c56-b29d-246615337f69",
   "metadata": {},
   "source": [
    "# Building Complex Services with Ray Serve\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "__Roadmap to building services__\n",
    "\n",
    "1. Create deployments which require specific resources (e.g., GPUs)\n",
    "1. Understand how to connect (compose) deployments\n",
    "1. Specify runtime environments for specific deployments, if needed\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077a4917-c20b-4da2-b27c-0517b99e27ce",
   "metadata": {},
   "source": [
    "## Specifying deployment resources\n",
    "\n",
    "Resources can be specified on a per-deployment basis and, if we want, in fractional units, via the `ray_actor_options` parameter on the `@serve.deployment` decorator.\n",
    "\n",
    "As a realistic example, we can upgrade the \"hello world\" chatbot to use a Huggingface LLM employing GPU resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81f88fe-73db-41b7-a0d0-df0e454c21d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment(ray_actor_options={'num_gpus': 0.5})\n",
    "class Chat:\n",
    "    def __init__(self, model: str):\n",
    "        # configure stateful elements of our service such as loading a model\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        self._model =  AutoModelForSeq2SeqLM.from_pretrained(model).to(0)\n",
    "\n",
    "    async def __call__(self, request: Request) -> dict:\n",
    "        # path to handle HTTP requests\n",
    "        data = await request.json()\n",
    "        data = json.loads(data)\n",
    "        # after decoding the payload, we delegate to get_response for logic\n",
    "        return {'response': self.get_response(data['user_input'], data['history']) }\n",
    "    \n",
    "    def get_response(self, user_input: str, history: list[str]) -> str:\n",
    "        # this method receives calls directly (from Python) or from __call__ (from HTTP)\n",
    "        history.append(user_input)\n",
    "        # the history is client-side state and will be a list of raw strings;\n",
    "        # for the default config of the model and tokenizer, history should be joined with '</s><s>'\n",
    "        inputs = self._tokenizer('</s><s>'.join(history), return_tensors='pt').to(0)\n",
    "        reply_ids = self._model.generate(**inputs, max_new_tokens=500)\n",
    "        response = self._tokenizer.batch_decode(reply_ids.cpu(), skip_special_tokens=True)[0]\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae819eab-26bf-409c-92de-7f6a8c79cf65",
   "metadata": {
    "tags": []
   },
   "source": [
    "Resources can include\n",
    "* `num_cpus`\n",
    "* `num_gpus`\n",
    "* `resources` dictionary containing custom resources\n",
    "    * custom resources are tracked and accounted as symbols (or tags) in order to match actors to workers\n",
    "    \n",
    "Example\n",
    "```python\n",
    "@serve.deployment(ray_actor_options={'num_cpus' : 2, 'num_gpus' : 2, resources : {\"my_super_accelerator\": 1}})\n",
    "class Demo:\n",
    "    ...\n",
    "```\n",
    "\n",
    "The purpose of the declarative resource mechanism is to allow Ray to place code on suitable nodes in a heterogeneous cluster without our having know which nodes have which resources to where our code should run.\n",
    "\n",
    "> Best practice: if some nodes have a distinguising feature, request it as a resource, rather than trying to determine which nodes are present and where your code will run.\n",
    "\n",
    "For more details, see https://docs.ray.io/en/releases-2.6.1/serve/scaling-and-resource-allocation.html#resource-management-cpus-gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0a4535-fdf4-4bb1-858f-203a20eb7afa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat = Chat.bind(model='facebook/blenderbot-400M-distill')\n",
    "\n",
    "handle = serve.run(chat, name='basic_chat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de784e9a-c007-4611-a2ea-eeba1e8fa8a8",
   "metadata": {},
   "source": [
    "This deployment handles both HTTP ingress and model inference -- we'll separate those in subsequent improvements.\n",
    "\n",
    "Since the current example serves as an ingress deployment, we can call it via HTTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c387364-7173-4343-af71-6bb2753a8c51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_json = '{ \"user_input\" : \"hello\", \"history\" : [] }'\n",
    "requests.post(\"http://localhost:8000/\", json = sample_json).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9694c32a-55f0-4f1a-a5bf-0f38c026ac05",
   "metadata": {},
   "source": [
    "To make it faster and simpler to experiment and iterate, we'll use the `ServeHandle` to directly call methods instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12f2cad-1123-4607-8f02-8873ec2609d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "message = 'My friends are cool but they eat too many carbs.'\n",
    "history = []\n",
    "response_handle = handle.get_response.remote(message, history)\n",
    "response = ray.get(response_handle)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcca11a1-e4d7-450e-844a-367504a98f05",
   "metadata": {},
   "source": [
    "We prepare a message and a chat history list and call our chat service via Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e5b70e-df82-4f9d-84b5-3e27887e87b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "34549288-e4a2-4fbb-b41f-e7ea70f51d13",
    "outputId": "8c682533-9ebd-4965-dc75-b0661cf1f3d6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "history += [message, response]\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959662cb-cdd0-4d02-9fbb-572c8735b996",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "id": "e1b448ae-3662-4211-bde6-72734f80ad30",
    "outputId": "28f4ea8b-2ce7-4aee-d1df-5546a74a5f9b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "message = \"I'm not sure.\"\n",
    "response_handle = handle.get_response.remote(message, history)\n",
    "response = ray.get(response_handle)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6818e8bd-cdba-4fe5-84b4-3c62f2a2358f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.delete('basic_chat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb611ab-85d7-40c6-b1fd-45a3c4e2144e",
   "metadata": {
    "id": "b54a5b65-7b86-4db0-821c-f3f9ce143316",
    "tags": []
   },
   "source": [
    "## Composing deployments with Ray for chatbots en Français: roadmap\n",
    "\n",
    "The underlying chatbot model we’ve used only supports English interaction. But we can learn more about Serve while adding French language support.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "__Roadmap to English & French chat__\n",
    "\n",
    "1. Implement translation functionality between French and English\n",
    "1. Implement a language detection deployment\n",
    "1. Implement a routing (dispatch) deployment:\n",
    "    1. If the incoming prompt is French, then\n",
    "        1. Route the inbound prompt through the FR-EN translator\n",
    "        1. Pass the EN prompt to the chat model\n",
    "        1. Pass the EN output from the chat model through the EN-FR translator\n",
    "        1. Return the French response\n",
    "    1. Otherwise (if the prompt is in English), pass it straight to the chatbot as we did earlier and return the (English) response\n",
    "</div>\n",
    "\n",
    "| <img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Serve/system-overview-multilingual-chat.jpg\" width=\"70%\" loading=\"lazy\"> |\n",
    "|:---------------------------------------------------------------------------------------------------------------------------------------------------:|\n",
    "|                                                         Multilingual-chat - system overview                                                         |\n",
    "\n",
    "Let’s look at using Ray Serve to implement model inference with these composed and conditional-flow elements (https://docs.ray.io/en/releases-2.6.1/serve/key-concepts.html#servehandle-composing-deployments).\n",
    "\n",
    "We’ll implement parts 1 and 2 first…\n",
    "\n",
    "## Translation functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197bb616-f1ab-4398-8748-d6ff78058cca",
   "metadata": {},
   "source": [
    "### Runtime environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db24df1-13f6-4f69-b11e-1119be71e6a5",
   "metadata": {},
   "source": [
    "We have many options for managing dependencies -- e.g., Python libraries and versions, resource file, etc.\n",
    "\n",
    "Dependencies can be provided at the level of Node/VM/container, Ray jobs, actors, tasks, and more.\n",
    "\n",
    "With Ray Serve, we can optionally specify environment requirements at the `Deployment` level, and Ray will ensure that the specified environment is available to that deployment.\n",
    "\n",
    "In the following example, we'll create \n",
    "* some services that use libraries available in our general Ray environment\n",
    "* a service that requires a specific Python library (a language detector library) to illustrate the custom runtime environment feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0c808e-62ec-4d7b-ad57-fb31d1ac32b9",
   "metadata": {},
   "source": [
    "Since we are discussing dependencies, it's important to remember that it's a good practice to keep as many dependencies as possible in our general Ray worker environments, and to import them as usual.\n",
    "\n",
    "> Just because we *can* create lots of custom environments in our code doesn't mean we *should*\n",
    "\n",
    "In this first service, we import `pipeline` from Huggingface transformers. Later, the specific pipeline we need will require `sentencepiece`. We'll demo installing `sentencepiece` via the Runtime Environment. \n",
    "\n",
    "Beyond just specifying the library, we have to be careful about the order of imports and other calls, to ensure we don't need something from the library before it's available. We ensure that by delaying imports or use of anything with a relevant import until an actual method is called on our service. We can capture variables as usual in the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd6ca85-c975-4987-bd41-64f749d7c313",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "runtime_env = {\"pip\": [\"sentencepiece==0.1.99\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dff8921-cb86-49e8-9ed3-6594ecb9095a",
   "metadata": {
    "id": "d1b79e65-6a72-4046-a551-dea32116b83d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment(ray_actor_options={\"runtime_env\" : runtime_env})\n",
    "class Translate:\n",
    "    def __init__(self, task: str, model: str):\n",
    "        self._task = task\n",
    "        self._model = model\n",
    "        self._pipeline = None\n",
    "    \n",
    "    def get_response(self, user_input: str) -> str:\n",
    "        if (self._pipeline is None):\n",
    "            self._pipeline = pipeline(task=self._task, model=self._model)\n",
    "        outputs = self._pipeline(user_input)\n",
    "        response = outputs[0]['translation_text']\n",
    "        return response\n",
    "        \n",
    "translate_en_fr = Translate.bind(task='translation_en_to_fr', model='t5-small')\n",
    "translate_fr_en = Translate.bind(task='translation_fr_to_en', model='Helsinki-NLP/opus-mt-fr-en')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c86d7e7-509f-4d57-9856-24a289cd1926",
   "metadata": {},
   "source": [
    "Notice how we have two different services but they are built on the same reusable code by calling `.bind()` with different initialization parameters.\n",
    "\n",
    "*We don’t need to define new deployments for every variation in functionality -- just as we don't need to write new Python classes for every variation in functionality.*\n",
    "\n",
    "This time we’re haven't published an application (via `serve.run()`) because these components will be invoked only by our main deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264edf69-dad8-42c9-802c-5dce1c673063",
   "metadata": {
    "id": "1e6f7e12-cfa0-41c2-abd5-4d300b14195e"
   },
   "source": [
    "## Language detection\n",
    "\n",
    "We can create the language detection service in a similar way. \n",
    "\n",
    "> This service is lighter weight because we’re using https://github.com/pemistahl/lingua-py … which leverages traditional NLP and n-grams for detection instead of a deep learning model. It can handle more traffic than, e.g., the chat model -- and it won't require a GPU. So we can benefit from Ray Serve's fine-grained resource allocation.\n",
    "    \n",
    "Lingua is optimized for strong detection on very short text snippets, like tweets, so it should be useful for our chat exchanges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7391791e-6afe-4c57-89a7-0e50f48a0992",
   "metadata": {},
   "source": [
    "In this service implementation, we'll demonstrate the custom environment feature by requiring a pip install of lingua-language-detector wherever this deployment happens to run. Ray will ensure this environment is installed as needed. But note the `import` is deferred until the `get_response(...)` method is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8636ee6a-a28f-4fa5-8a59-9fc74a8b172d",
   "metadata": {
    "id": "5f6bd63d-4958-4580-b72a-caeba1a1f578",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment(ray_actor_options={\"runtime_env\" : {\"pip\": [\"lingua-language-detector==1.3.2\"]}})\n",
    "class LangDetect:\n",
    "    def __init__(self):\n",
    "        self._detector = None\n",
    "        \n",
    "    def get_response(self, user_input: str) -> str:\n",
    "        from lingua import Language, LanguageDetectorBuilder\n",
    "        \n",
    "        if (self._detector is None):\n",
    "            languages = [Language.ENGLISH, Language.FRENCH]\n",
    "            self._detector = LanguageDetectorBuilder.from_languages(*languages).build()\n",
    "        \n",
    "        output = self._detector.detect_language_of(user_input)\n",
    "        if (output == Language.ENGLISH):\n",
    "            return 'en'\n",
    "        else:\n",
    "            return 'fr'\n",
    "        \n",
    "lang_detect = LangDetect.bind()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1347781f-13ce-468f-a3f7-06aa4262d4a6",
   "metadata": {
    "id": "778ede38-d221-4f45-8e2d-71ec0702aae4",
    "tags": []
   },
   "source": [
    "## Composing multiple models\n",
    "\n",
    "Let's bring the whole system together. We'll implement an ingress deployment which represents our external endpoint for HTTP or Python invocations.\n",
    "* This deployment will have references to the deployments we've built so far, and will implement some conditional logic to ensure the correct language is used\n",
    "* Note that even if the user is interacting in French, we need to return the English response as well so that client can use that to build the chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f717ca2e-d6c6-4ef3-975d-fe34c9919a84",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22f85caa-133e-4df7-8aab-4e3278ffc54b",
    "outputId": "2086ccc1-0d59-419f-d47f-d6144980f536",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class Endpoint:\n",
    "    def __init__(self, chat, lang_detect, translate_en_fr, translate_fr_en):\n",
    "        # assign dependent service handles to instance variables\n",
    "        self._chat = chat\n",
    "        self._lang_detect = lang_detect\n",
    "        self._translate_en_fr = translate_en_fr\n",
    "        self._translate_fr_en = translate_fr_en\n",
    "\n",
    "    async def __call__(self, request: Request) -> dict:\n",
    "        data = await request.json()\n",
    "        data = json.loads(data)\n",
    "        return {'response': await self.get_response(data['user_input'], data['history']) }\n",
    "    \n",
    "    async def get_response(self, user_input: str, history: list[str]):\n",
    "        lang_obj_ref = await self._lang_detect.get_response.remote(user_input)\n",
    "        \n",
    "        # if we didn't need the literal value of the language yet, we could pass that (future) object reference to other services\n",
    "        # here, though, we need the value in order to decide whether to call the translation services\n",
    "        # we get the Python value by awaiting the object reference\n",
    "        lang = await lang_obj_ref\n",
    "\n",
    "        if (lang == 'fr'):\n",
    "            user_input = await self._translate_fr_en.get_response.remote(user_input)\n",
    "\n",
    "        response = response_en = await self._chat.get_response.remote(user_input, history)\n",
    "        \n",
    "        if (lang == 'fr'):\n",
    "            response = await self._translate_en_fr.get_response.remote(response_en)\n",
    "            user_input = await user_input\n",
    "            \n",
    "        response = await response\n",
    "        response_en = await response_en\n",
    "        \n",
    "        return response  + '|' + user_input + '|' + response_en\n",
    "\n",
    "chat = Chat.bind(model='facebook/blenderbot-400M-distill')\n",
    "endpoint = Endpoint.bind(chat, lang_detect, translate_en_fr, translate_fr_en)\n",
    "\n",
    "endpoint_handle = serve.run(endpoint, name = 'multilingual_chat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04f16ab-1f13-4d0d-a33d-d52f6c266782",
   "metadata": {},
   "source": [
    "We've implemented control flow through our deployments and used the async/await pattern in several places so that we don't unnecessarily block.\n",
    "\n",
    "Then we set up the external endpoint (ingress deployment) and start a new application serving that deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63165f18-216a-411f-99f1-4fb55dd8aa8c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "id": "fbe6a39a-1b55-49c6-8a7e-632256588bf3",
    "outputId": "fd81afe6-5f31-4777-ecd5-c4d138ca901e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "message = 'My friends are cool but they eat too many carbs.'\n",
    "history = []\n",
    "response = ray.get(endpoint_handle.get_response.remote(message, history))\n",
    "response.split('|')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35872a30-9ed0-4ac9-b11d-1cab798a5cf3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0b852c27-20d3-4cc3-b300-5fd15e0ba124",
    "outputId": "386cf1b6-daeb-4a19-d0cd-248131bd9f52",
    "tags": []
   },
   "outputs": [],
   "source": [
    "history += response.split('|')[1:]\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c90bec-1b5e-4a1a-b67a-1e11f37169ea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234
    },
    "id": "73855166-c226-40f9-a365-babb36f751b3",
    "outputId": "17498373-04e4-4595-80af-88cb24c568f2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "message = 'Je ne suis pas sûr.'\n",
    "response = ray.get(endpoint_handle.get_response.remote(message, history))\n",
    "response.split('|')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5402694c-e4a3-4227-b438-0faab9dd9e3a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "25952d4a-43fe-49fa-a437-eb91813cde5c",
    "outputId": "29e847c0-4819-46c6-d15c-681bab01e028",
    "tags": []
   },
   "outputs": [],
   "source": [
    "history += response.split('|')[1:]\n",
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d25201-b26a-43c1-b8e7-0f6f5fc9fea8",
   "metadata": {
    "tags": []
   },
   "source": [
    "At this point we have a service which can support the many functional and operational properties we expect to need in production, including scalability, separation of concerns, and composability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac6bab4-7552-4ab5-9439-0df07d632fe4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.delete('multilingual_chat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7f8816-e905-4c91-80f2-445a9d99dfa4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "__Lab activity: build a simpler translator__\n",
    "    \n",
    "You can use Google's Flan-T5 large model like this\n",
    "    \n",
    "```python\n",
    "  from transformers import pipeline\n",
    "\n",
    "  pipe = pipeline(\"text2text-generation\", model=\"google/flan-t5-large\")\n",
    "```\n",
    "   \n",
    "<br/>\n",
    "    \n",
    "This model can translate between English and several other languages if instructed to do so in the prompt. \n",
    "    \n",
    "Build a Serve application that\n",
    "    \n",
    "* detects in the input language\n",
    "* translates (Engligh-Italian or the reverse) using Flan-T5\n",
    "* returns the result\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
