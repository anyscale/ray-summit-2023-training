{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe257dd9-7305-4647-8b1f-a8667e9364a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ray import serve\n",
    "import requests, json\n",
    "from starlette.requests import Request\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c912fb-71e0-4cb9-ba8d-5179dbf62263",
   "metadata": {},
   "source": [
    "# Architecture / under-the-hood\n",
    "\n",
    "## Ray cluster perspective: actors\n",
    "\n",
    "In Ray, user code is executed by worker processes. These workers can run tasks (stateless functions) or actors (stateful class instances).\n",
    "\n",
    "Ray Serve is built on actors, allowing deployments to collect expensive state once (such as loading a ML model) and to reuse it across many requests.\n",
    "\n",
    "Although you may never need to code any Ray tasks or actors yourself, your Ray Serve application has full access to those cluster capabilities and you may wish to use them to implement other functionality (e.g., operations that don't need to accept HTTP traffic). More information is at https://docs.ray.io/en/releases-2.6.1/ray-core/walkthrough.html\n",
    "\n",
    "## Serve design\n",
    "\n",
    "Under the hood, a few other actors are used to make up a serve instance.\n",
    "\n",
    "* Controller: A global actor unique to each Serve instance is responsible for managing other actors. Serve API calls like creating or getting a deployment make remote calls to the Controller.\n",
    "\n",
    "* HTTP Proxy: By default there is one HTTP proxy actor on the head node that accepts incoming requests, forwards them to replicas, and responds once they are completed. For scalability and high availability, you can also run a proxy on each node in the cluster via the location field of http_options.\n",
    "\n",
    "* Deployment Replicas: Actors that execute the code in response to a request. Each replica processes requests from the HTTP proxy.\n",
    "\n",
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Serve/serve-system-architecture.png\" width=\"70%\" loading=\"lazy\">\n",
    "\n",
    "Incoming requests, once resolved to a particular deployment, are queued. The requests from the queue are assigned round-robin to available replicas as long as capacity is available. This design provides load balancing and elasticity. \n",
    "\n",
    "Capacity can be managed with the `max_concurrent_queries` parameter to the deployment decorator. This value defaults to 100 and represents the maximum number of queries that will be sent to a replica of this deployment without receiving a response. Each replica has its own queue to collect and smooth incoming request traffic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aea24f-b77d-41af-af97-1ab4e767c827",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Production features: scaling, performance, and more\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "__Detail: Production features__\n",
    "    \n",
    "1. Replicas and autoscaling\n",
    "1. Request batching\n",
    "1. Fault tolerance\n",
    "1. Serve CLI, in-place upgrades, multi-application support\n",
    "\n",
    "</div>\n",
    "\n",
    "## Replicas and autoscaling\n",
    "\n",
    "Each deployment can have its own resource management and autoscaling configuration, with several options for scaling.\n",
    "\n",
    "By default -- if nothing is specified, as in our examples above -- the default is a single replica. We can specify a larger, constant number of replicas in the decorator:\n",
    "```python\n",
    "@serve.deployment(num_replicas=3)\n",
    "```\n",
    "\n",
    "For autoscaling, instead of `num_replicas`, we provide an `autoscaling_config` dictionary. With autoscaling, we can specify a minimum and maximum range for the number of replicas, the initial replica count, a load target, and more.\n",
    "\n",
    "Here is example of extended configuration -- see https://docs.ray.io/en/releases-2.6.1/serve/scaling-and-resource-allocation.html#scaling-and-resource-allocation for more details:\n",
    "\n",
    "```python\n",
    "@serve.deployment(\n",
    "    autoscaling_config={\n",
    "        'min_replicas': 1,\n",
    "        'initial_replicas': 2,\n",
    "        'max_replicas': 5,\n",
    "        'target_num_ongoing_requests_per_replica': 10,\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "`min_replicas` can also be set to zero to create a \"serverless\" style design: in exchange for potentially slower startup, no actors (or their CPU/GPU resources) need to be permanently reserved.\n",
    "\n",
    "### Autoscaling LLM chat\n",
    "\n",
    "The LLM-based chat is a good example for seeing autoscaling in action, because LLM inference is relative expensive so we can easily build up a queue of requests. The autoscaler responds to the dynamics of queue sizes and will launch additional replicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a698315-f56d-4f8f-94ff-5ffbaf620271",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment(ray_actor_options={'num_gpus': 0.5}, autoscaling_config={ 'min_replicas': 1, 'max_replicas': 4 })\n",
    "class Chat:\n",
    "    def __init__(self, model: str):\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        self._model =  AutoModelForSeq2SeqLM.from_pretrained(model).to(0)\n",
    "\n",
    "    async def __call__(self, request: Request) -> dict:\n",
    "        data = await request.json()\n",
    "        data = json.loads(data)\n",
    "        return {'response': self.get_response(data['user_input'], data['history']) }\n",
    "    \n",
    "    def get_response(self, user_input: str, history: list[str]) -> str:\n",
    "        history.append(user_input)\n",
    "        inputs = self._tokenizer('</s><s>'.join(history), return_tensors='pt').to(0)\n",
    "        reply_ids = self._model.generate(**inputs, max_new_tokens=500)\n",
    "        response = self._tokenizer.batch_decode(reply_ids.cpu(), skip_special_tokens=True)[0]\n",
    "        return response\n",
    "    \n",
    "chat = Chat.bind(model='facebook/blenderbot-400M-distill')\n",
    "\n",
    "handle = serve.run(chat, name='autoscale_chat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df62e50c-abe7-4ca8-b688-6fb7fcc41999",
   "metadata": {},
   "source": [
    "We can generate a little load and look at the Ray Dashboard\n",
    "\n",
    "What do we expect to see?\n",
    "\n",
    "* Autoscaling of the Chat deployment up to 4 replicas\n",
    "* Efficient use of fractional GPU resources\n",
    "    * If our cluster has just 2 GPUs, we can run 4 replicas there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a111763-9bf0-453c-ade9-b011d06f414f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_request(s):\n",
    "    return requests.post(\"http://localhost:8000/\", json = s).json()\n",
    "\n",
    "sample = '{ \"user_input\" : \"Hello there, chatbot!\", \"history\":[] }'\n",
    "make_request(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06c65f0-8477-40c3-871c-80d454d7e19f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "executor = ThreadPoolExecutor(max_workers=32)\n",
    "\n",
    "results = executor.map(make_request, ['{ \"user_input\" : \"Hello there, chatbot!\", \"history\":[] }'] * 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2424f00a-3c60-478a-bd54-50003bd6024a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.delete('autoscale_chat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b969d0-0657-42bb-a824-d42d73f9dc71",
   "metadata": {},
   "source": [
    "### Request batching\n",
    "\n",
    "Many components -- especially components that rely on neural net models -- can produce higher throughput on batches of data.\n",
    "\n",
    "At the same time, most component interfaces or contracts are based on a single request-response.\n",
    "\n",
    "Ray Serve enables us to meet both of those goals by automatically applying batching based on a specified batch size and batch timeout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addc680f-62b9-4558-ba2d-3fa60eca40c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment()\n",
    "class Chat:\n",
    "    def __init__(self):\n",
    "        self._message = \"Chatbot counts the batch size at \"\n",
    "\n",
    "    @serve.batch(max_batch_size=10, batch_wait_timeout_s=0.01)\n",
    "    async def handle_batch(self, request_batch):\n",
    "        num_requests = len(request_batch)\n",
    "        return [ {'response': self._message + str(num_requests) } ] * num_requests\n",
    "    \n",
    "    async def __call__(self, request: Request) -> dict:\n",
    "        data = await request.json()\n",
    "        data = json.loads(data)\n",
    "        return await self.handle_batch(data)\n",
    "    \n",
    "chat = Chat.bind()\n",
    "\n",
    "handle = serve.run(chat, name='batch_chat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b8803a-e2f7-4754-8367-377f8feafeb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = executor.map(make_request, ['{ \"user_input\" : \"Hello there, chatbot!\", \"history\":[] }'] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2a781a-14ab-436e-8fc4-bc2382ddf63d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batches = [int(resp['response'].split(' ')[-1]) for resp in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26c605a-2af9-4187-b795-58bbe977e356",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.hist(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292df94d-4895-45de-9ab5-a6d38c6a8617",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.delete('batch_chat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23f3d4f-055f-407d-ac76-4582e072cf26",
   "metadata": {},
   "source": [
    "### Fault tolerance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7054a80d-0643-4299-bc87-471791e72448",
   "metadata": {},
   "source": [
    "Serve provides some fault tolerance features out of the box\n",
    "\n",
    "* Replica health-checking: by default, the Serve controller periodically health-checks each Serve deployment replica and restarts it on failure\n",
    "  * __Built in__: does not require KubeRay\n",
    "  * Support for custom application-level health-checks, frequency, and timeout\n",
    "  * If the health-check fails, the Serve controller logs the exception, kills the unhealthy replica(s), and restarts them\n",
    "\n",
    "End-to-end fault tolerance by running Serve on top of KubeRay or Anyscale\n",
    "\n",
    "* Worker node restart\n",
    "* Head node restart\n",
    "* Head node state recovery with Redis\n",
    "\n",
    "While Ray can start/restart/scale worker processes, KubeRay and Anyscale provide the ability to recover nodes, provision additional nodes from a resource pool, cloud provider, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df085a1a-ad86-4d41-ae69-a145dec70838",
   "metadata": {},
   "source": [
    "### Additional production considerations and features\n",
    "\n",
    "#### Web application capabilities\n",
    "\n",
    "* FastAPI support\n",
    "* WebSockets\n",
    "* Streaming responsea\n",
    "\n",
    "#### Serve CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213ef3d4-9a53-4ba1-a496-d29b65efa1e4",
   "metadata": {},
   "source": [
    "For use in production, Serve includes a CLI with commands to deploy applications, check them, update them, and more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00676263-70d2-4c0d-971f-2a2a4ce5f704",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! serve status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a80285c-442f-46c5-947f-1ee3f7dec193",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "handle = serve.run(chat, name='batch_chat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31552b5d-8d0c-450d-8246-80e0cf80ae03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! ray status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56e07b4-6706-444d-be77-dd793c651dec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! serve status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f403da-b45b-46a9-8cd5-4e39fff175cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e465a9-d452-4240-99f2-b6ceff2065e9",
   "metadata": {},
   "source": [
    "#### In-place upgrades and multi-application support\n",
    "\n",
    "While deployments can be reconfigured in-place and hot-redeployed, those updates will trigger an update of all deployments within the application.\n",
    "\n",
    "In large, complex design, you may want to use a single Ray cluster (service) and make updates to individual components, but not redeploy the entire set of deployments. For those use cases, Ray Serve allows you do define multiple applications.\n",
    "\n",
    "This collection of applications\n",
    "* runs in the same Ray cluster\n",
    "* can interact with each other and lookup other services by name\n",
    "* can be upgraded independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eecd2eb-3789-4808-b2ff-8a2f1bba0f95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
