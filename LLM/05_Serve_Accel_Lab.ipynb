{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0671364a-b9ab-4deb-9c7c-95674b8f4efb",
   "metadata": {},
   "source": [
    "# vLLM accelerated deployment streamed batching lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3454168-618e-405c-a204-dbf7b96fe65d",
   "metadata": {},
   "source": [
    "This lab is an opportunity to familiarize yourself with\n",
    "* Accelerated inference with vLLM in Ray Serve\n",
    "* One streaming request/response pattern for interacting with a LLM app (there are other patterns as well -- see https://docs.ray.io/en/latest/serve/tutorials/streaming.html)\n",
    "* Performance of streamed batching\n",
    "\n",
    "If you're new to LLM applications, focus on the intro level activity. If you've worked with LLM apps before, you may have time to try the additional activities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5d1eb4-576d-4c27-a80b-c03e39df5433",
   "metadata": {},
   "source": [
    "## Intro level activity\n",
    "\n",
    "The main activity in this lab is to refactor the code below from Chen Shen and Cade Daniel's work (referenced in this blog: https://www.anyscale.com/blog/continuous-batching-llm-inference) so that you can test it out in a notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e05b25-e7b6-476b-bb15-c73611bef8a9",
   "metadata": {},
   "source": [
    "## Intermediate level activity\n",
    "\n",
    "In the initial script, only one request is sent and streamed back.\n",
    "\n",
    "vLLM really shines when we send lots of requests asynchronously -- use the \"cities\" requests from the Hosting with Ray notebook to generate 12 requests, and send them asynchronously to the model deployment.\n",
    "\n",
    "Stream the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc6f122-d004-419b-b97c-aecd8437c50f",
   "metadata": {},
   "source": [
    "## Advanced activity\n",
    "\n",
    "In the demo script, we only get back 10 or so tokens for each request. Modify the code so that we get back 200 tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea33aa0-0cf6-4b91-a1e8-b001a09ee28b",
   "metadata": {},
   "source": [
    "## Bonus!\n",
    "\n",
    "* Since we've focused on LLMs and Ray, we haven't built HTTP-based \"front ends\" to our services. This example includes such a HTTP adapter. Examine the code and try to learn how it works -- which pieces are \"typical\" HTTP processing boilerplate vs. which pieces are \"Ray specific\"\n",
    "\n",
    "* Note that the quality of responses from the simpler model in the demo script will not be great. If you have extra time, feel free to \"upgrade\" to a more powerful model. Hints: check the vLLM docs for compatible models and recall that chat-tuned models may need a more complex prompting pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669152f5-f692-4a67-a5af-3ddc6b491cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code example from https://github.com/ray-project/ray/blob/cc983fc3e64c1ba215e981a43dd0119c03c74ff1/doc/source/serve/doc_code/vllm_example.py\n",
    "\n",
    "import json\n",
    "from typing import AsyncGenerator\n",
    "\n",
    "from fastapi import BackgroundTasks\n",
    "from starlette.requests import Request\n",
    "from starlette.responses import StreamingResponse, Response\n",
    "from vllm.engine.arg_utils import AsyncEngineArgs\n",
    "from vllm.engine.async_llm_engine import AsyncLLMEngine\n",
    "from vllm.sampling_params import SamplingParams\n",
    "from vllm.utils import random_uuid\n",
    "\n",
    "from ray import serve\n",
    "\n",
    "\n",
    "@serve.deployment(ray_actor_options={\"num_gpus\": 1})\n",
    "class VLLMPredictDeployment:\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Construct a VLLM deployment.\n",
    "\n",
    "        Refer to https://github.com/vllm-project/vllm/blob/main/vllm/engine/arg_utils.py\n",
    "        for the full list of arguments.\n",
    "\n",
    "        Args:\n",
    "            model: name or path of the huggingface model to use\n",
    "            download_dir: directory to download and load the weights,\n",
    "                default to the default cache dir of huggingface.\n",
    "            use_np_weights: save a numpy copy of model weights for\n",
    "                faster loading. This can increase the disk usage by up to 2x.\n",
    "            use_dummy_weights: use dummy values for model weights.\n",
    "            dtype: data type for model weights and activations.\n",
    "                The \"auto\" option will use FP16 precision\n",
    "                for FP32 and FP16 models, and BF16 precision.\n",
    "                for BF16 models.\n",
    "            seed: random seed.\n",
    "            worker_use_ray: use Ray for distributed serving, will be\n",
    "                automatically set when using more than 1 GPU\n",
    "            pipeline_parallel_size: number of pipeline stages.\n",
    "            tensor_parallel_size: number of tensor parallel replicas.\n",
    "            block_size: token block size.\n",
    "            swap_space: CPU swap space size (GiB) per GPU.\n",
    "            gpu_memory_utilization: the percentage of GPU memory to be used for\n",
    "                the model executor\n",
    "            max_num_batched_tokens: maximum number of batched tokens per iteration\n",
    "            max_num_seqs: maximum number of sequences per iteration.\n",
    "            disable_log_stats: disable logging statistics.\n",
    "            engine_use_ray: use Ray to start the LLM engine in a separate\n",
    "                process as the server process.\n",
    "            disable_log_requests: disable logging requests.\n",
    "        \"\"\"\n",
    "        args = AsyncEngineArgs(**kwargs)\n",
    "        self.engine = AsyncLLMEngine.from_engine_args(args)\n",
    "\n",
    "    async def stream_results(self, results_generator) -> AsyncGenerator[bytes, None]:\n",
    "        num_returned = 0\n",
    "        async for request_output in results_generator:\n",
    "            text_outputs = [output.text for output in request_output.outputs]\n",
    "            assert len(text_outputs) == 1\n",
    "            text_output = text_outputs[0][num_returned:]\n",
    "            ret = {\"text\": text_output}\n",
    "            yield (json.dumps(ret) + \"\\n\").encode(\"utf-8\")\n",
    "            num_returned += len(text_output)\n",
    "\n",
    "    async def may_abort_request(self, request_id) -> None:\n",
    "        await self.engine.abort(request_id)\n",
    "\n",
    "    async def __call__(self, request: Request) -> Response:\n",
    "        \"\"\"Generate completion for the request.\n",
    "\n",
    "        The request should be a JSON object with the following fields:\n",
    "        - prompt: the prompt to use for the generation.\n",
    "        - stream: whether to stream the results or not.\n",
    "        - other fields: the sampling parameters (See `SamplingParams` for details).\n",
    "        \"\"\"\n",
    "        request_dict = await request.json()\n",
    "        prompt = request_dict.pop(\"prompt\")\n",
    "        stream = request_dict.pop(\"stream\", False)\n",
    "        sampling_params = SamplingParams(**request_dict)\n",
    "        request_id = random_uuid()\n",
    "        results_generator = self.engine.generate(prompt, sampling_params, request_id)\n",
    "        if stream:\n",
    "            background_tasks = BackgroundTasks()\n",
    "            # Using background_taks to abort the the request\n",
    "            # if the client disconnects.\n",
    "            background_tasks.add_task(self.may_abort_request, request_id)\n",
    "            return StreamingResponse(\n",
    "                self.stream_results(results_generator), background=background_tasks\n",
    "            )\n",
    "\n",
    "        # Non-streaming case\n",
    "        final_output = None\n",
    "        async for request_output in results_generator:\n",
    "            if await request.is_disconnected():\n",
    "                # Abort the request if the client disconnects.\n",
    "                await self.engine.abort(request_id)\n",
    "                return Response(status_code=499)\n",
    "            final_output = request_output\n",
    "\n",
    "        assert final_output is not None\n",
    "        prompt = final_output.prompt\n",
    "        text_outputs = [prompt + output.text for output in final_output.outputs]\n",
    "        ret = {\"text\": text_outputs}\n",
    "        return Response(content=json.dumps(ret))\n",
    "\n",
    "\n",
    "def send_sample_request():\n",
    "    import requests\n",
    "\n",
    "    prompt = \"How do I cook fried rice?\"\n",
    "    sample_input = {\"prompt\": prompt, \"stream\": True}\n",
    "    output = requests.post(\"http://localhost:8000/\", json=sample_input)\n",
    "    for line in output.iter_lines():\n",
    "        print(line.decode(\"utf-8\"))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # To run this example, you need to install vllm which requires\n",
    "    # OS: Linux\n",
    "    # Python: 3.8 or higher\n",
    "    # CUDA: 11.0 â€“ 11.8\n",
    "    # GPU: compute capability 7.0 or higher (e.g., V100, T4, RTX20xx, A100, L4, etc.)\n",
    "    # see https://vllm.readthedocs.io/en/latest/getting_started/installation.html\n",
    "    # for more details.\n",
    "    deployment = VLLMPredictDeployment.bind(model=\"facebook/opt-125m\")\n",
    "    serve.run(deployment)\n",
    "    send_sample_request()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
