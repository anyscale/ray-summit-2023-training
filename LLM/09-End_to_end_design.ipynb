{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d49464-d217-4254-b362-decce78b302a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7ee779-9a33-4720-99a5-e98b9e388f1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from InstructorEmbedding import INSTRUCTOR\n",
    "import requests\n",
    "from starlette.requests import Request\n",
    "import ray\n",
    "from ray import serve\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11870fdf-54f7-4b09-83bb-7f3dbaa8a482",
   "metadata": {},
   "source": [
    "# Large scale architecture for retrieval-augmented generation (RAG) applications\n",
    "\n",
    "In a large-scale design and deployment, we want to address several operational, performance, and cost goals:\n",
    "* the overall system should be scalable to the limit of our budget and available hardware\n",
    "* each component should be scalable separately, including \"serverless,\" manual, and autoscaling\n",
    "* our deployment mechanism should support fractional resource usage so that we can most efficiently use GPUs and other hardware\n",
    "* it should be possible to leverage tight packing or sparse spreading for component replicas, depending on their compute, I/O, and data needs\n",
    "* we would like to avoid manual placement of resources\n",
    "* the system should support high availability and load balancing throughout all calls to various components\n",
    "* it should be straightforward to code, modify, configure, and upgrade components\n",
    "\n",
    "Ray and Ray Serve -- on top of a resource manager like Anyscale or KubeRay+Kubernetes -- provides an elegant platform to meet these requirements.\n",
    "\n",
    "We can structure a basic system as a collection of Serve deployments, including at least one ingress deployment -- it might look like this:\n",
    "\n",
    "# <div style='color:red;'>DIAGRAM of SYSTEM</div>\n",
    "\n",
    "The key components are\n",
    "* Embedder for creating embedding vectors of queries and data\n",
    "* Vector database for retrieving semantically relevant information\n",
    "* Prompt builder for creating custom prompts based on queries, supporting information, and task (goal)\n",
    "* Chat or LLM wrapper for collecting and dispatching inference (sampling) calls to the language model(s)\n",
    "* Orchestrator to manage the flow of data through the various components\n",
    "* Ingress to handle HTTP calls, streaming responses, and data conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e83f2a-b56f-41b9-a192-a28dfc9f18ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EMBEDDER_MODEL = 'hkunlp/instructor-large'\n",
    "CHAT_MODEL = 'stabilityai/StableBeluga-7B'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01e66b4-b8ea-4463-87b7-438326dc945f",
   "metadata": {},
   "source": [
    "We'll code and run a skeletal implementation of this architecture.\n",
    "\n",
    "* To deploy at a larger scale, we would want to provision more resources and allow larger scaling\n",
    "* We would also want to enable capabilities like batching, since most language models generate much better throughput via batched inference\n",
    "\n",
    "For each component below, we'll also note specific additional capabilities to consider as \"next steps\" in evolving this application toward production\n",
    "\n",
    "## Embedder\n",
    "\n",
    "This embedder component encodes only one string (to one vector) at a time. \n",
    "\n",
    "We may want to extend this capability to encode batches of vectors for\n",
    "* batched inference tasks\n",
    "* generation of new knowledge bases or expansion of our existing knowledge base (see database component below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6e6e39-b16a-4b2b-9621-0619fa3de70d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment(ray_actor_options={\"num_gpus\": 0.1}, autoscaling_config={ \"min_replicas\": 1, \"max_replicas\": 2 })\n",
    "class Embedder:\n",
    "    def __init__(self, model: str):\n",
    "        self._model = INSTRUCTOR(model, cache_folder=\"/mnt/local_storage\")\n",
    "        \n",
    "    def get_response(self, message):\n",
    "        return self._model.encode(message).tolist()\n",
    "\n",
    "embedder = Embedder.bind(EMBEDDER_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a7a43d-e1b2-420e-b3cf-b099f8ece2b3",
   "metadata": {},
   "source": [
    "## Database\n",
    "\n",
    "This database component scales out effectively given a static vector dataset.\n",
    "\n",
    "Future steps include an architecture for adding and/or updating and re-indexing the dataset\n",
    "\n",
    "The pattern employed will depend on concrete decisions regarding how often and how large the updates will be\n",
    "* scaling the \"write path\" on the database typically requires some design balance between speed, consistency, and availability\n",
    "* while a small number vector databases currently support some form of scale-out on the write path, none are very simple, and the segment is evolving quickly -- so we expect to see new options soon\n",
    "* cloud vector store services may or may not be a suitable solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d430b402-2517-4057-821b-6f401ecd6f86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment(autoscaling_config={ \"min_replicas\": 4, \"max_replicas\": 8 }, \n",
    "                  ray_actor_options={ \"runtime_env\" : { \"pip\": [\"chromadb\"] }})\n",
    "class ChromaDBReader:\n",
    "    def __init__(self, collection: str):\n",
    "        self._collection_name = collection\n",
    "        self._coll = None\n",
    "    \n",
    "    def get_response(self, query_vec):\n",
    "        if self._coll is None:\n",
    "            import chromadb\n",
    "            chroma_client = chromadb.PersistentClient(path=\"/mnt/cluster_storage/vector_store\")\n",
    "            self._coll = chroma_client.get_collection(self._collection_name)\n",
    "            \n",
    "        return self._coll.query(query_embeddings=[query_vec], n_results=3,)['documents'][0]\n",
    "\n",
    "db = ChromaDBReader.bind('persistent_text_chunks')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb3ec09-31f6-4bf6-a980-50d80e923140",
   "metadata": {},
   "source": [
    "## Prompt generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23de4d66-a78a-4ef7-a445-f1c02117da43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_prompt = \"\"\"You are a helpful assistant who can answer questions about a text based on your existing knowledge and documents supplied here.\n",
    "When answering questions, use the following relevant excerpts from the text:\n",
    "{ newline.join([doc for doc in docs]) } \n",
    "If you don't have information to answer a question, please say you don't know. Don't make up an answer.### User: \"\"\"\n",
    "\n",
    "@serve.deployment(autoscaling_config={ \"min_replicas\": 1, \"max_replicas\": 4 })\n",
    "class PromptBuilder:\n",
    "    def __init__(self, base_prompt):                \n",
    "        self._base_prompt = base_prompt\n",
    "    \n",
    "    def get_response(self, query, docs):\n",
    "        newline = '\\n'\n",
    "        return eval(f'f\"\"\"{self._base_prompt}\"\"\"') + query + '\\n\\n### Assistant:\\n'\n",
    "\n",
    "prompt_builder = PromptBuilder.bind(base_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1992a723-c0a6-4041-832e-a54a62d27e60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment(ray_actor_options={\"num_gpus\": 0.9}, autoscaling_config={ \"min_replicas\": 1, \"max_replicas\": 2 })\n",
    "class Chat:\n",
    "    def __init__(self, model: str):\n",
    "        self._model = None\n",
    "        self._model_name = model\n",
    "        \n",
    "    def get_response(self, message) -> str:\n",
    "        if self._model is None:\n",
    "            import torch\n",
    "            from transformers import pipeline\n",
    "            self._model =  pipeline(\"text-generation\", model=self._model_name, model_kwargs={\n",
    "                                        'torch_dtype':torch.float16,\n",
    "                                        'device_map':'auto',\n",
    "                                        \"cache_dir\": \"/mnt/local_storage\"})\n",
    "        \n",
    "        return self._model(message, max_length=1500)[0]['generated_text'].split('### Assistant:\\n')[1]\n",
    "\n",
    "chat = Chat.bind(model=CHAT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b754277-7c86-4030-b35b-d6772bcd8f61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment(ray_actor_options={\"num_gpus\": 0.9}, autoscaling_config={ \"min_replicas\": 1, \"max_replicas\": 2 })\n",
    "class AcceleratedChat:\n",
    "    def __init__(self, model: str):\n",
    "        from vllm import LLM, SamplingParams\n",
    "        self._llm = LLM(model=model, download_dir='/mnt/local_storage')\n",
    "        self._sp = SamplingParams(max_tokens=200)\n",
    "        \n",
    "    def get_response(self, message) -> str:\n",
    "        return self._llm.generate(message, sampling_params = self._sp)[0].outputs[0].text\n",
    "\n",
    "vllm_chat = AcceleratedChat.bind(model=CHAT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fb36a1-677b-4cb8-9443-e7eddef911f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment(autoscaling_config={ \"min_replicas\": 1, \"max_replicas\": 4 })\n",
    "class Orchestrator:\n",
    "    def __init__(self, embedder, db, prompt_builder, chat):                \n",
    "        self._embedder = embedder\n",
    "        self._db = db\n",
    "        self._prompt_builder = prompt_builder\n",
    "        self._chat = chat\n",
    "    \n",
    "    async def get_response(self, query):\n",
    "        embed = self._embedder.get_response.remote(query)\n",
    "        docs = self._db.get_response.remote(await embed)\n",
    "        prompt = self._prompt_builder.get_response.remote(query, await docs)\n",
    "        resp = self._chat.get_response.remote(await prompt)\n",
    "        ref = await resp\n",
    "        result = await ref        \n",
    "        return result\n",
    "            \n",
    "orchestrator = Orchestrator.bind(embedder, db, prompt_builder, chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df358ac1-722e-48f4-9bdb-b6ca0325dd68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class Ingress:\n",
    "    def __init__(self, orchestrator):\n",
    "        self._orchestrator = orchestrator\n",
    "\n",
    "    async def __call__(self, request: Request) -> dict:\n",
    "        data = await request.json()\n",
    "        data = json.loads(data)\n",
    "        result = await self._orchestrator.get_response.remote(data['input'])\n",
    "        output = await result\n",
    "        return {\"result\": output }\n",
    "    \n",
    "ingress = Ingress.bind(orchestrator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2177eb1d-ffda-49f0-884b-edfd60aefa1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "app = serve.run(ingress, name='e2e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075ecc06-37fb-4be3-8a03-e9d94a55e056",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_json = '{ \"input\" : \"Describe the body of water in Utah\" }'\n",
    "requests.post(\"http://localhost:8000/\", json = sample_json).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4bc8af-e773-4008-99fa-a80ba6e21b07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_json = '{ \"input\" : \"Tell me about the robbery\" }'\n",
    "requests.post(\"http://localhost:8000/\", json = sample_json).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaf2cae-af1f-417e-acbf-e6e1e66425da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb03943a-68e1-4df1-9910-3ff7c77b254d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.delete('e2e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2515813-d643-4639-9f36-d7ed9e290920",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6199dd-4f31-4743-86fc-84a2c2d17f43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
