{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d49464-d217-4254-b362-decce78b302a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7ee779-9a33-4720-99a5-e98b9e388f1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from InstructorEmbedding import INSTRUCTOR\n",
    "import requests\n",
    "from starlette.requests import Request\n",
    "import ray\n",
    "from ray import serve\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11870fdf-54f7-4b09-83bb-7f3dbaa8a482",
   "metadata": {},
   "source": [
    "# Large scale architecture for retrieval-augmented generation (RAG) applications\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "__Roadmap to architecture for large-scale RAG apps__\n",
    "\n",
    "1. Goals and requirements for production systems\n",
    "1. Key components of a basic scalable app\n",
    "1. Core features and next steps for each component\n",
    "1. Running it end-to-end\n",
    "</div>\n",
    "\n",
    "In a large-scale design and deployment, we want to address several operational, performance, and cost goals:\n",
    "* the overall system should be scalable to the limit of our budget and available hardware\n",
    "* each component should be scalable separately, including \"serverless,\" manual, and autoscaling\n",
    "* our deployment mechanism should support fractional resource usage so that we can most efficiently use GPUs and other hardware\n",
    "* it should be possible to leverage tight packing or sparse spreading for component replicas, depending on their compute, I/O, and data needs\n",
    "* we would like to avoid manual placement of resources\n",
    "* the system should support high availability and load balancing throughout all calls to various components\n",
    "* it should be straightforward to code, modify, configure, and upgrade components\n",
    "\n",
    "Ray and Ray Serve -- on top of a resource manager like Anyscale or KubeRay+Kubernetes -- provides an elegant platform to meet these requirements.\n",
    "\n",
    "We can structure a basic system as a collection of Serve deployments, including at least one ingress deployment -- it might look like this:\n",
    "\n",
    "# <div style='color:red;'>DIAGRAM of SYSTEM</div>\n",
    "\n",
    "The key components are\n",
    "* Embedder for creating embedding vectors of queries and data\n",
    "* Vector database for retrieving semantically relevant information\n",
    "* Prompt builder for creating custom prompts based on queries, supporting information, and task (goal)\n",
    "* Chat or LLM wrapper for collecting and dispatching inference (sampling) calls to the language model(s)\n",
    "* Orchestrator to manage the flow of data through the various components\n",
    "* Ingress to handle HTTP calls, streaming responses, and data conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e83f2a-b56f-41b9-a192-a28dfc9f18ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EMBEDDER_MODEL = 'hkunlp/instructor-large'\n",
    "CHAT_MODEL = 'stabilityai/StableBeluga-7B'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01e66b4-b8ea-4463-87b7-438326dc945f",
   "metadata": {},
   "source": [
    "We'll code and run a skeletal implementation of this architecture.\n",
    "\n",
    "* To deploy at a larger scale, we would want to provision more resources and allow larger scaling\n",
    "* We would also want to enable capabilities like batching, since most language models generate much better throughput via batched inference\n",
    "\n",
    "For each component below, we'll also note specific additional capabilities to consider as \"next steps\" in evolving this application toward production\n",
    "\n",
    "## Embedder\n",
    "\n",
    "This embedder component encodes only one string (to one vector) at a time. \n",
    "\n",
    "We may want to extend this capability to encode batches of vectors for\n",
    "* batched inference tasks\n",
    "* generation of new knowledge bases or expansion of our existing knowledge base (see database component below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6e6e39-b16a-4b2b-9621-0619fa3de70d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment(ray_actor_options={\"num_gpus\": 0.1}, autoscaling_config={ \"min_replicas\": 1, \"max_replicas\": 2 })\n",
    "class Embedder:\n",
    "    def __init__(self, model: str):\n",
    "        self._model = INSTRUCTOR(model, cache_folder=\"/mnt/local_storage\")\n",
    "        \n",
    "    def get_response(self, message):\n",
    "        return self._model.encode(message).tolist()\n",
    "\n",
    "embedder = Embedder.bind(EMBEDDER_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a7a43d-e1b2-420e-b3cf-b099f8ece2b3",
   "metadata": {},
   "source": [
    "## Database\n",
    "\n",
    "This database component scales out effectively given a static vector dataset.\n",
    "\n",
    "We likely want to add some support for different types of indexes as well as retrieving stats. Future steps include an architecture for adding and/or updating and re-indexing the dataset.\n",
    "\n",
    "The pattern employed will depend on concrete decisions regarding how often and how large the updates will be\n",
    "* scaling the \"write path\" on the database typically requires some design balance between speed, consistency, and availability\n",
    "* while a small number vector databases currently support some form of scale-out on the write path, none are very simple, and the segment is evolving quickly -- so we expect to see new options soon\n",
    "* cloud vector store services may or may not be a suitable solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d430b402-2517-4057-821b-6f401ecd6f86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment(autoscaling_config={ \"min_replicas\": 4, \"max_replicas\": 8 }, \n",
    "                  ray_actor_options={ \"runtime_env\" : { \"pip\": [\"chromadb\"] }})\n",
    "class ChromaDBReader:\n",
    "    def __init__(self, collection: str):\n",
    "        self._collection_name = collection\n",
    "        self._coll = None\n",
    "    \n",
    "    def get_response(self, query_vec):\n",
    "        if self._coll is None:\n",
    "            import chromadb\n",
    "            chroma_client = chromadb.PersistentClient(path=\"/mnt/cluster_storage/vector_store\")\n",
    "            self._coll = chroma_client.get_collection(self._collection_name)\n",
    "            \n",
    "        return self._coll.query(query_embeddings=[query_vec], n_results=3,)['documents'][0]\n",
    "\n",
    "db = ChromaDBReader.bind('persistent_text_chunks')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb3ec09-31f6-4bf6-a980-50d80e923140",
   "metadata": {},
   "source": [
    "## Prompt generation\n",
    "\n",
    "This example prompt generation component supports a single templated prompt styled for a small number of LLMs (the ### User / ### Assistent pattern is specific to certain models).\n",
    "\n",
    "We may want to expand this service to support\n",
    "* multiple tasks/goals with specific prompt templates or patterns\n",
    "* multiple models with different mechanisms for specifying system vs. user prompt\n",
    "* use cases which require altering the system prompt\n",
    "* requiring JSON, SQL, Python, or other output types\n",
    "* constrained generation (applying a schema, regex, or other specification to guide generation; e.g., via https://github.com/normal-computing/outlines)\n",
    "\n",
    "It may become useful to back the prompt generator with a database, templating scheme, or other content-management tools.\n",
    "\n",
    "*It is also important to consider the risk of prompt injection and consider other security measures which may be useful in prompting*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23de4d66-a78a-4ef7-a445-f1c02117da43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_prompt = \"\"\"You are a helpful assistant who can answer questions about a text based on your existing knowledge and documents supplied here.\n",
    "When answering questions, use the following relevant excerpts from the text:\n",
    "{ newline.join([doc for doc in docs]) } \n",
    "If you don't have information to answer a question, please say you don't know. Don't make up an answer.### User: \"\"\"\n",
    "\n",
    "@serve.deployment(autoscaling_config={ \"min_replicas\": 1, \"max_replicas\": 4 })\n",
    "class PromptBuilder:\n",
    "    def __init__(self, base_prompt):                \n",
    "        self._base_prompt = base_prompt\n",
    "    \n",
    "    def get_response(self, query, docs):\n",
    "        newline = '\\n'\n",
    "        return eval(f'f\"\"\"{self._base_prompt}\"\"\"') + query + '\\n\\n### Assistant:\\n'\n",
    "\n",
    "prompt_builder = PromptBuilder.bind(base_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9935347b-761e-402d-ade1-d0f4356af7ab",
   "metadata": {},
   "source": [
    "## Chat component\n",
    "\n",
    "Below are simple examples of a chat service wrapping a single huggingface-hosted model as well as a similar service featuring acceleration with vLLM.\n",
    "\n",
    "Natural extensions of this pattern include\n",
    "* multiple models (e.g., different models for different tasks)\n",
    "* mixture-of-experts and/or generator-critic ensembles\n",
    "\n",
    "Also important to consider are\n",
    "* batching I/O (the below examples \"implicitly\" support batching in the `messages` param, but we may want to make that more explicit\n",
    "* streaming response \n",
    "    * and potentially streamed batching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8eecce-901f-4823-8339-2b1c5b3017bd",
   "metadata": {},
   "source": [
    "__Non-accelerated chat__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1992a723-c0a6-4041-832e-a54a62d27e60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment(ray_actor_options={\"num_gpus\": 0.9}, autoscaling_config={ \"min_replicas\": 1, \"max_replicas\": 2 })\n",
    "class Chat:\n",
    "    def __init__(self, model: str):\n",
    "        self._model = None\n",
    "        self._model_name = model\n",
    "        \n",
    "    def get_response(self, message) -> str:\n",
    "        if self._model is None:\n",
    "            import torch\n",
    "            from transformers import pipeline\n",
    "            self._model =  pipeline(\"text-generation\", model=self._model_name, model_kwargs={\n",
    "                                        'torch_dtype':torch.float16,\n",
    "                                        'device_map':'auto',\n",
    "                                        \"cache_dir\": \"/mnt/local_storage\"})\n",
    "        \n",
    "        return self._model(message, max_length=1500)[0]['generated_text'].split('### Assistant:\\n')[1]\n",
    "\n",
    "chat = Chat.bind(model=CHAT_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1969b904-9321-4774-8728-3415b3f2e831",
   "metadata": {},
   "source": [
    "__Accelerated chat with vLLM__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b754277-7c86-4030-b35b-d6772bcd8f61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment(ray_actor_options={\"num_gpus\": 0.9}, autoscaling_config={ \"min_replicas\": 1, \"max_replicas\": 2 })\n",
    "class AcceleratedChat:\n",
    "    def __init__(self, model: str):\n",
    "        from vllm import LLM, SamplingParams\n",
    "        self._llm = LLM(model=model, download_dir='/mnt/local_storage')\n",
    "        self._sp = SamplingParams(max_tokens=200)\n",
    "        \n",
    "    def get_response(self, message) -> str:\n",
    "        return self._llm.generate(message, sampling_params = self._sp)[0].outputs[0].text\n",
    "\n",
    "vllm_chat = AcceleratedChat.bind(model=CHAT_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04d2ab3-ac99-4ed5-8f99-14fc52910bd0",
   "metadata": {},
   "source": [
    "## Orchestration\n",
    "\n",
    "An orchestration service allows us to link the various components while maintaining separation of concerns.\n",
    "\n",
    "This simple orchestrator follows a \"chain\" pattern and implicitly assumes certain component interfaces.\n",
    "\n",
    "We might extend this orchestrator by\n",
    "* allowing additional control flows\n",
    "* creating a generic dict- and/or ndarry- based API for all component I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fb36a1-677b-4cb8-9443-e7eddef911f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment(autoscaling_config={ \"min_replicas\": 1, \"max_replicas\": 4 })\n",
    "class Orchestrator:\n",
    "    def __init__(self, embedder, db, prompt_builder, chat):                \n",
    "        self._embedder = embedder\n",
    "        self._db = db\n",
    "        self._prompt_builder = prompt_builder\n",
    "        self._chat = chat\n",
    "    \n",
    "    async def get_response(self, query):\n",
    "        embed = self._embedder.get_response.remote(query)\n",
    "        docs = self._db.get_response.remote(await embed)\n",
    "        prompt = self._prompt_builder.get_response.remote(query, await docs)\n",
    "        resp = self._chat.get_response.remote(await prompt)\n",
    "        ref = await resp # collecting async response (Ray ObjectRef) from chat call\n",
    "        result = await ref # collecting Python string from Ray ObjectRef\n",
    "        return result\n",
    "            \n",
    "orchestrator = Orchestrator.bind(embedder, db, prompt_builder, chat) # can swap in vllm_chat here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a18d613-15f3-40fb-a870-07e6712646aa",
   "metadata": {},
   "source": [
    "# Ingress deployment (HTTP interface)\n",
    "\n",
    "Natural extensions of this interface include\n",
    "* accommodating streaming responses\n",
    "* handling multimodal I/O; e.g., HTTP upload/download semantics for images\n",
    "* adding escaping/checking to reduce risk of injection and other content-based exploits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df358ac1-722e-48f4-9bdb-b6ca0325dd68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class Ingress:\n",
    "    def __init__(self, orchestrator):\n",
    "        self._orchestrator = orchestrator\n",
    "\n",
    "    async def __call__(self, request: Request) -> dict:\n",
    "        data = await request.json()\n",
    "        data = json.loads(data)\n",
    "        result = await self._orchestrator.get_response.remote(data['input'])\n",
    "        output = await result\n",
    "        return {\"result\": output }\n",
    "    \n",
    "ingress = Ingress.bind(orchestrator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548bd1e2-80a3-4f43-a0ef-c31cb86e4aa3",
   "metadata": {},
   "source": [
    "Despite the skeletal implementations here, we can see the full modular architecture in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2177eb1d-ffda-49f0-884b-edfd60aefa1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "app = serve.run(ingress, name='e2e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5271ec-8dd4-4cc9-8d94-a68f50f5e9bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def talk_to_LLM(query):\n",
    "    result = requests.post(\"http://localhost:8000/\", json = json.dumps({ 'input' : query})).json()\n",
    "    return result['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13729efc-c5b1-4c36-9f58-19fa80ecef8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "talk_to_LLM(\"Describe the body of water in Utah\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4bc8af-e773-4008-99fa-a80ba6e21b07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "talk_to_LLM(\"Tell me as much as you can about the robbery\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaf2cae-af1f-417e-acbf-e6e1e66425da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "talk_to_LLM(\"Did Phileas Fogg really rob the bank?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb03943a-68e1-4df1-9910-3ff7c77b254d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.delete('e2e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2515813-d643-4639-9f36-d7ed9e290920",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6199dd-4f31-4743-86fc-84a2c2d17f43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
