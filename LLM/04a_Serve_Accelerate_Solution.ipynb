{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a7d916-d3a6-499f-96a6-ccaf4c3d70b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import AsyncGenerator\n",
    "import requests\n",
    "from fastapi import BackgroundTasks\n",
    "from starlette.requests import Request\n",
    "from starlette.responses import StreamingResponse, Response\n",
    "from vllm.engine.arg_utils import AsyncEngineArgs\n",
    "from vllm.engine.async_llm_engine import AsyncLLMEngine\n",
    "from vllm.sampling_params import SamplingParams\n",
    "from vllm.utils import random_uuid\n",
    "\n",
    "from ray import serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e3d85d-81e5-4a8a-be2f-e5a53cf60b41",
   "metadata": {},
   "source": [
    "Core deployment definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49862d9d-1116-4528-967c-f11af9e9b826",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment(ray_actor_options={\"num_gpus\": 1})\n",
    "class VLLMPredictDeployment:\n",
    "    def __init__(self, **kwargs):\n",
    "        # Refer to https://github.com/vllm-project/vllm/blob/main/vllm/engine/arg_utils.py for the full list of arguments.\n",
    "        args = AsyncEngineArgs(**kwargs)\n",
    "        self.engine = AsyncLLMEngine.from_engine_args(args)\n",
    "\n",
    "    async def stream_results(self, results_generator) -> AsyncGenerator[bytes, None]:\n",
    "        num_returned = 0\n",
    "        async for request_output in results_generator:\n",
    "            text_outputs = [output.text for output in request_output.outputs]\n",
    "            assert len(text_outputs) == 1\n",
    "            text_output = text_outputs[0][num_returned:]\n",
    "            ret = {\"text\": text_output}\n",
    "            yield (json.dumps(ret) + \"\\n\").encode(\"utf-8\")\n",
    "            num_returned += len(text_output)\n",
    "\n",
    "    async def may_abort_request(self, request_id) -> None:\n",
    "        await self.engine.abort(request_id)\n",
    "\n",
    "    async def __call__(self, request: Request) -> Response:\n",
    "        # The request should be a JSON object with the following fields: prompt, stream (True/False), kwargs for vLLM `SamplingParams`\n",
    "        \n",
    "        request_dict = await request.json()\n",
    "        prompt = request_dict.pop(\"prompt\")\n",
    "        stream = request_dict.pop(\"stream\", False)\n",
    "        sampling_params = SamplingParams(**request_dict)\n",
    "        request_id = random_uuid()\n",
    "        results_generator = self.engine.generate(prompt, sampling_params, request_id)\n",
    "        if stream:\n",
    "            background_tasks = BackgroundTasks()\n",
    "            # Using background_taks to abort the the request\n",
    "            # if the client disconnects.\n",
    "            background_tasks.add_task(self.may_abort_request, request_id)\n",
    "            return StreamingResponse(\n",
    "                self.stream_results(results_generator), background=background_tasks\n",
    "            )\n",
    "\n",
    "        # Non-streaming case\n",
    "        final_output = None\n",
    "        async for request_output in results_generator:\n",
    "            if await request.is_disconnected():\n",
    "                # Abort the request if the client disconnects.\n",
    "                await self.engine.abort(request_id)\n",
    "                return Response(status_code=499)\n",
    "            final_output = request_output\n",
    "\n",
    "        assert final_output is not None\n",
    "        prompt = final_output.prompt\n",
    "        text_outputs = [prompt + output.text for output in final_output.outputs]\n",
    "        ret = {\"text\": text_outputs}\n",
    "        return Response(content=json.dumps(ret))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b0addc-d964-41ce-9ae7-ef270560ccd1",
   "metadata": {},
   "source": [
    "Our config for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6f2faa-43e3-4eb6-b5f9-bf63394b3425",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model='facebook/opt-125m'\n",
    "download_dir='/mnt/local_storage'\n",
    "\n",
    "prompt = 'What is your favorite place to visit in San Francisco?'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde6f9de-9a22-4ab8-9836-5988d07dbc4a",
   "metadata": {},
   "source": [
    "Start application on Serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0bb6ae-747e-40ba-af55-5032c365d694",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "deployment = VLLMPredictDeployment.bind(model=model, download_dir=download_dir)\n",
    "serve.run(deployment, name='vllm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfcb534-213a-444d-b966-30697d85252b",
   "metadata": {},
   "source": [
    "Test and print output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7bb244-cfb3-45c2-9abc-c1c33414c16d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_input = {\"prompt\": prompt, \"stream\": True}\n",
    "output = requests.post(\"http://localhost:8000/\", json=sample_input)\n",
    "for line in output.iter_lines():\n",
    "    print(line.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea98fe5b-0fe5-4496-8f90-06d83c319044",
   "metadata": {},
   "source": [
    "Run multiple requests asynchronously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3989c5d3-5802-4724-9c94-341b88125e1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cities = ['Atlanta', 'Boston', 'Chicago', 'Vancouver', 'Montreal', 'Toronto', 'Frankfurt', 'Rome', 'Warsaw', 'Cairo', 'Dar Es Salaam', 'Gaborone']\n",
    "prompts = [f'What is your favorite place to visit in {city}?' for city in cities]\n",
    "\n",
    "def send(m):\n",
    "    return requests.post(\"http://localhost:8000/\", json={\"prompt\": m, \"stream\": True})\n",
    "\n",
    "outputs = map(send, prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f745d553-8f22-4649-bcba-055b1ad25942",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for output in outputs:\n",
    "    for line in output.iter_lines():\n",
    "        print(line.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cfc9e3-ea1c-4acc-ae10-008195591c25",
   "metadata": {},
   "source": [
    "Change code to get 200 tokens in responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7d2a96-04b5-4ff5-bf54-b27e68eaccb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def send(m):\n",
    "    return requests.post(\"http://localhost:8000/\", json={\"prompt\": m, \"stream\": True, \"max_tokens\": 200})\n",
    "\n",
    "outputs = map(send, prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc36cf99-ace7-45ac-b7bb-7604a2f7ec3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for output in outputs:\n",
    "    for line in output.iter_lines():\n",
    "        print(line.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7494a922-598f-4f5e-9aae-988b9f495d4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
