{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee788c7-5783-4162-b1c8-00fe2a30de76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import openai\n",
    "import ray\n",
    "import requests\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from ray import serve\n",
    "from starlette.requests import Request\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392f9ef9-7f9e-446d-8876-40ca04f40b38",
   "metadata": {},
   "source": [
    "# Hosting and serving a large language model model with Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c08949-bcf2-4318-9539-b2c03a64f862",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "__Roadmap to serving LLMs with Ray__\n",
    "\n",
    "1. Review using a model via API (featuring Anyscale Endpoints)\n",
    "1. Run a model from Huggingface locally (dev/test scenario)\n",
    "1. Introducing Ray Serve\n",
    "1. Serving and scaling LLM chat models with Ray Serve\n",
    "1. Accelerating LLMs on Ray with vLLM and other technologies\n",
    "</div>\n",
    "\n",
    "## Review: Llama-2 on Anyscale Endpoints\n",
    "\n",
    "As we've seen, the easiest way to get started with LLMs is to access them remotely, via API. Top solutions include\n",
    "* OpenAI API access to proprietary GPT models\n",
    "* Anyscale Endpoints API access to open models\n",
    "\n",
    "We're going to look at a progression from this simple approach to the more challenging tasks of hosting an open model in your own infrastructure, an approach that is popular because it ...\n",
    "* allows complete control over the application including ...\n",
    "    * requirements and logic\n",
    "    * SLAs/performance\n",
    "* retains privacy of data supplied to the model\n",
    "* can be less expensive\n",
    "\n",
    "Today we'll also work with another similar LLM, and to make the progression clear, we'll start with a quick review of remote access via Anyscale Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eab17c-feab-417f-95fd-257c860b4e03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "openaikey = Path('aetoken.txt').read_text()\n",
    "openai.api_key = openaikey\n",
    "openai.api_base = 'https://api.endpoints.anyscale.com/v1'\n",
    "model=\"meta-llama/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22efdc36-4777-4c33-84bd-ec600507924a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_msg = 'You are a helpful assistant.'\n",
    "user_msg = 'What is your favorite place to visit in San Francisco?'\n",
    "response = openai.ChatCompletion.create(model=model, messages=[{\"role\": \"system\", \"content\": system_msg},\n",
    "                                         {\"role\": \"user\", \"content\": user_msg}])\n",
    "response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab2a0b6-df67-4de8-89a5-a6264f476ef6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def quick_chat(user, temp=1.0):\n",
    "    response = openai.ChatCompletion.create(model=model, temperature=temp, \n",
    "                                        messages=[{\"role\": \"system\", \"content\": 'You are a helpful assistant.'},\n",
    "                                         {\"role\": \"user\", \"content\": user}])\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dd5332-d8b0-4316-a80d-f6191e845312",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "quick_chat(user_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63008356-89cd-41e3-b9f4-003f228d739b",
   "metadata": {},
   "source": [
    "## Locally run a model via Huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13875037-d31d-4f50-aabd-eb55de69321d",
   "metadata": {},
   "source": [
    "The next step in experimenting with models is often to run them locally using the Huggingface Transformers library.\n",
    "\n",
    "HF Transformers wraps a lot of common boilerplate logic for running LLMs, including\n",
    "* downloading and locally caching weights and configs\n",
    "* standardizing the request-response interface\n",
    "* providing reasonable defaults for typical workflow of generating embeddings, passing them to the model, and decoding the response\n",
    "\n",
    "By \"local\" we mean a laptop or desktop with suitable hardware or your own cloud instance.\n",
    "\n",
    "This workflow is a great way to experiment and to build upon the base models to create more complex applications. For production, we'll want a more robust environment along several dimensions (e.g., scalability, reliability) -- but we'll come back to that.\n",
    "\n",
    "Here we'll set up a minimal script to run the model locally, test it out, and then delete it (and free the memory used on the GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84917e5b-65ab-4b91-b5f8-c63ddebd5241",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = '''You are a helpful assistant.### User: What is your favorite place to visit in San Francisco? ### Assistant:'''\n",
    "\n",
    "CHAT_MODEL = 'stabilityai/StableBeluga-7B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37467332-4761-4b55-9aba-e0cd9235e22e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", model=CHAT_MODEL, model_kwargs={\n",
    "    'torch_dtype':torch.float16,\n",
    "    'device_map':'auto',\n",
    "    \"cache_dir\": \"/mnt/local_storage\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860d6233-81c2-420d-bd93-45aa25042a3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe(prompt, max_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6974a75d-a9cf-4a79-9231-d4ee8c6abd6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del(pipe)\n",
    "accelerator = Accelerator()\n",
    "accelerator.free_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5130397-e4e5-40e3-93f6-5abc170163ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896bfb9e-185c-4069-9e2a-279672d71693",
   "metadata": {},
   "source": [
    "## Moving toward production-grade hosting with Ray Serve\n",
    "\n",
    "To host LLMs in production-quality environments, we'll want a number of additional capabilities\n",
    "* Reliability\n",
    "* Scalability -- and preferably auto-scaling since GPUs are expensive\n",
    "* Integration with other Python libraries and application code\n",
    "* Access to data at scale\n",
    "\n",
    "These capabilities are provided by Ray Serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b19ce28-590f-44df-a67c-0441dff82e96",
   "metadata": {},
   "source": [
    "### What is Ray Serve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4065e019-3e84-4868-b727-33d4dea6ec8e",
   "metadata": {},
   "source": [
    "Serve is a microservices framework for serving ML – the model serving\n",
    "component of Ray AI Libraries.\n",
    "\n",
    "<img src='https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Serve/serve_architecture.png' width=700/>\n",
    "\n",
    "### Deployments\n",
    "\n",
    "`Deployment` is the fundamental developer-facing element of serve.\n",
    "\n",
    "<img src='https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Serve/deployment.png' width=600/>\n",
    "\n",
    "### Our First Service\n",
    "\n",
    "Let’s jump right in and get something simple up and running on Ray\n",
    "Serve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98427e49-2bc9-41ce-9c76-d835b5d3c943",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class Hello:\n",
    "    \n",
    "    def get_response(self, message: str) -> str:\n",
    "        return \"Yes! \" + message\n",
    "\n",
    "handle = serve.run(Hello.bind(), name='hello_world_app')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39463167-6605-4bf1-bf1f-4331921af4aa",
   "metadata": {},
   "source": [
    "Code to create and deploy a `Deployment` (component) with Ray Serve is minimal\n",
    "\n",
    "We can invoke this service via HTTP if we add one additional method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff748a8-a249-438b-9f5f-c004876d85f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class Hello:\n",
    "\n",
    "    async def __call__(self, request: Request) -> dict:\n",
    "        data = await request.json()\n",
    "        data = json.loads(data)\n",
    "        return {\"result\": self.get_response(data['input']) }\n",
    "    \n",
    "    def get_response(self, message: str) -> str:\n",
    "        return \"Yes! \" + message\n",
    "\n",
    "handle = serve.run(Hello.bind(), name='hello_world_app')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6626e52a-e2d9-43a6-8ee9-f58a1ef363c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_json = '{ \"input\" : \"hello\" }'\n",
    "requests.post(\"http://localhost:8000/\", json = sample_json).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedb7488-7c4b-4d55-b4f3-97aa7d49413a",
   "metadata": {},
   "source": [
    "The deployment(s) that accept HTTP connections from the outside world are called \"ingress deployments\"\n",
    "\n",
    "For brevity and simplicity, as we explore LLM applications, we will create deployments without HTTP handling. In a complete system, ingress deployments would call functionality in other deployments.\n",
    "\n",
    "But we can also call into a running deployment directly, for easier dev and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ac503e-65ad-473b-8ce8-e11a4373f4e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "handle.get_response.remote('Hello San Francisco!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f421b1b-3d34-45dd-b99f-ea042e884f60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ref = handle.get_response.remote('Hello San Francisco!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04e4584-b152-4b5c-a2c8-e9bce1313099",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray.get(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf0e937-37ab-49e4-bf45-e101ed0b6f26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.delete('hello_world_app')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f642f5b-d03e-48b9-ac57-219ada987d53",
   "metadata": {},
   "source": [
    "With minimal modification, we can plug the Huggingface model into a Serve deployment\n",
    "\n",
    "Note that we guarantee access to a full GPU by annotating the `@serve.deployment` decorator with a resource requirement: in this case, 1.0 GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c7c36a-3c85-481e-bc51-21efad0b5071",
   "metadata": {
    "tags": []
   },
   "source": [
    "Resources can include\n",
    "* `num_cpus`\n",
    "* `num_gpus`\n",
    "* `resources` dictionary containing custom resources\n",
    "    * custom resources are tracked and accounted as symbols (or tags) in order to match actors to workers\n",
    "    \n",
    "Example\n",
    "```python\n",
    "@serve.deployment(ray_actor_options={'num_cpus' : 2, 'num_gpus' : 2, resources : {\"my_super_accelerator\": 1}})\n",
    "class Demo:\n",
    "    ...\n",
    "```\n",
    "\n",
    "The purpose of the declarative resource mechanism is to allow Ray to place code on suitable nodes in a heterogeneous cluster without our having know which nodes have which resources to where our code should run.\n",
    "\n",
    "> Best practice: if some nodes have a distinguising feature, request it as a resource, rather than trying to determine which nodes are present and where your code will run.\n",
    "\n",
    "For more details, see https://docs.ray.io/en/releases-2.6.1/serve/scaling-and-resource-allocation.html#resource-management-cpus-gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c2eebc-9c7b-4657-9f60-0ddbc7ddf9c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment(ray_actor_options={\"num_gpus\": 1.0})\n",
    "class Chat:\n",
    "    def __init__(self, model: str):\n",
    "        self._model =  pipeline(\"text-generation\", model=model, model_kwargs={\n",
    "                                        'torch_dtype':torch.float16,\n",
    "                                        'device_map':'auto',\n",
    "                                        \"cache_dir\": \"/mnt/local_storage\"})\n",
    "    \n",
    "    def get_response(self, message: str) -> str:\n",
    "        return self._model(message, max_length=200)\n",
    "\n",
    "handle = serve.run(Chat.bind(model=CHAT_MODEL), name='chat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80552ef-6fb5-4a2d-aa40-007ef49affd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ref = handle.get_response.remote(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba41909-07f9-4a02-b427-3ad1423cf4ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray.get(ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fffb01-8c8b-42ca-9552-0a2fc91a720d",
   "metadata": {},
   "source": [
    "That's it! We've got a basic LLM service running on our own Ray cluster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d159d5a9-ad49-41e1-9b41-19480bbf7483",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.delete('chat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc5d29c-257a-4833-a41d-d07bce077808",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Scaling and performance\n",
    "\n",
    "#### Replicas and autoscaling\n",
    "\n",
    "Each deployment can have its own resource management and autoscaling configuration, with several options for scaling.\n",
    "\n",
    "By default -- if nothing is specified, as in our examples above -- the default is a single replica. We can specify a larger, constant number of replicas in the decorator:\n",
    "```python\n",
    "@serve.deployment(num_replicas=3)\n",
    "```\n",
    "\n",
    "For autoscaling, instead of `num_replicas`, we provide an `autoscaling_config` dictionary. With autoscaling, we can specify a minimum and maximum range for the number of replicas, the initial replica count, a load target, and more.\n",
    "\n",
    "Here is example of extended configuration -- see https://docs.ray.io/en/releases-2.6.1/serve/scaling-and-resource-allocation.html#scaling-and-resource-allocation for more details:\n",
    "\n",
    "```python\n",
    "@serve.deployment(\n",
    "    autoscaling_config={\n",
    "        'min_replicas': 1,\n",
    "        'initial_replicas': 2,\n",
    "        'max_replicas': 5,\n",
    "        'target_num_ongoing_requests_per_replica': 10,\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "`min_replicas` can also be set to zero to create a \"serverless\" style design: in exchange for potentially slower startup, no actors (or their CPU/GPU resources) need to be permanently reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c582bf17-8781-4650-ae63-de98201540d7",
   "metadata": {},
   "source": [
    "## Improving performance\n",
    "\n",
    "While the qualitative capabilities of LLMs are impressive \"out-of-the-box,\" we often want to improve them along the performance and efficiency axes for\n",
    "* cost management\n",
    "* ability to run on fewer/smaller/cheaper accelerators (or even on CPUs or other devices)\n",
    "* better user experience\n",
    "\n",
    "### Acceleration techniques\n",
    "\n",
    "Techniques for accelerating computation and shrinking LLM footprint -- particularly with minimally invasive changes to the LLM itself -- are a hot area of research, with new approaches published weekly.\n",
    "\n",
    "Some popular approaches include"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea34092c-1ae4-40f1-a996-55cc3033e5fd",
   "metadata": {},
   "source": [
    "* LoRA: Low-Rank Adaptation of Large Language Models\n",
    "  * HF blog https://huggingface.co/blog/lora\n",
    "  * paper https://arxiv.org/abs/2106.09685\n",
    "* vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention\n",
    "  * modeled on virtual memory and paging\n",
    "  * [project site](https://vllm.ai/)\n",
    "* GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers\n",
    "  * quantizing (fewer bits per weight) withou re-training\n",
    "  * repo https://github.com/IST-DASLab/gptq\n",
    "  * alos see https://github.com/TimDettmers/bitsandbytes\n",
    "* Faster Transformer\n",
    "  * highly-optimized version of the transformer block; pipelining, converter for distributed inference\n",
    "  * blog https://developer.nvidia.com/blog/accelerated-inference-for-large-transformer-models-using-nvidia-fastertransformer-and-nvidia-triton-inference-server/\n",
    "  * repo https://github.com/NVIDIA/FasterTransformer/\n",
    "* FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\n",
    "  * paper https://arxiv.org/abs/2205.14135\n",
    "  * repo https://github.com/Dao-AILab/flash-attention\n",
    "\n",
    "... and more!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3bc159-f047-42fc-884d-c5f2a55316da",
   "metadata": {},
   "source": [
    "All of these mechanisms can be combined with Ray Serve to create services that are fast, cheaper, and require less hardware than our example above.\n",
    "\n",
    "We'll take a look at deploying our model using vLLM\n",
    "\n",
    "### vLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f6c523-9640-4849-aac1-6e8e43c8f53d",
   "metadata": {},
   "source": [
    "Local usage of vLLM is simple...\n",
    "\n",
    "```python\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "llm = LLM(model='NousResearch/Llama-2-7b-chat-hf', download_dir='/mnt/local_storage')\n",
    "sp = SamplingParams(max_tokens=200)\n",
    "llm.generate(prompt, sampling_params = sp)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357752d5-8cdf-4db3-8b7f-a9b3ba9d4889",
   "metadata": {},
   "source": [
    "... as is using vLLM in a Serve deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d78474-e2aa-4c12-b241-4b2364d9dd83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment(ray_actor_options={\"num_gpus\": 1.0})\n",
    "class AcceleratedChat:\n",
    "    def __init__(self, model: str):\n",
    "        from vllm import LLM, SamplingParams\n",
    "        self._llm = LLM(model=model, download_dir='/mnt/local_storage')\n",
    "        self._sp = SamplingParams(max_tokens=200)\n",
    "        \n",
    "    def get_response(self, message) -> str:\n",
    "        return self._llm.generate(message, sampling_params = self._sp)\n",
    "\n",
    "handle = serve.run(AcceleratedChat.bind(model=CHAT_MODEL), name='accelerated_chat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e7ccd9-5703-4187-a82b-56fbbe7bc977",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray.get(handle.get_response.remote(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3193efd-9dc9-4200-84d7-634e32ef8a02",
   "metadata": {},
   "source": [
    "It's straightforward to use but doesn't seem vastly faster than our earlier example.\n",
    "\n",
    "vLLM really shines when we have batches of data to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61cd317-d64f-40a3-a067-ebe75132d1b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cities = ['Atlanta', 'Boston', 'Chicago', 'Vancouver', 'Montreal', 'Toronto', 'Frankfurt', 'Rome', 'Warsaw', 'Cairo', 'Dar Es Salaam', 'Gaborone']\n",
    "prompts = [f'You are a helpful assistant.### User: What is your favorite place to visit in {city}? ### Assistant:' for city in cities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5380c546-960c-48e6-bf3b-5153ba71b9a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8c8705-1f47-41e7-bc9c-19b59930d59d",
   "metadata": {},
   "source": [
    "We'll artificially inflate this dataset a bit further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eabb5e4-47b5-429d-a5ce-dda691ac722d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = ray.get(handle.get_response.remote(prompts + prompts + prompts + prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e0dd2b-d09b-49a4-8091-06225470b9c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c217106-261d-46a3-bc0e-5d85e681fa65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results[1].outputs[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717c616c-3397-4dc3-b351-0d922bb4daf6",
   "metadata": {},
   "source": [
    "And just to verify the outputs are distinct runs of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2e696b-3667-4d8d-b424-5c5340170b34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results[13].outputs[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f6e64a-62c1-4bf3-89fd-d3f3b97f18af",
   "metadata": {},
   "source": [
    "We can see a significant speedup in per-prompt, per-token processing here. \n",
    "\n",
    "But what if we don't want to block and process a whole batch at once?\n",
    "\n",
    "vLLM supports \"streamed batching\" where we can submit multiple requests over time and still get the benefits of batching. There are more details in this Anyscale blog (https://www.anyscale.com/blog/continuous-batching-llm-inference) and we'll try it out in a lab exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e07b6a-fc23-460d-b3da-3e9c8b25a617",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.delete('accelerated_chat')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
