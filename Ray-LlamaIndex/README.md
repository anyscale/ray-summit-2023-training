# Practical Data and Evaluation Considerations for Building Production-Ready LLM Applications with LlamaIndex and Ray

Â© 2023, Anyscale Inc. All Rights Reserved

<img src="https://images.ctfassets.net/xjan103pcp94/5lr4KRl4w5Nv0MaFAf2N64/4c5a4d44cffc5cc5a64137d1867f8a30/llamaindex-img-3.png" width="70%" loading="lazy">

<a href="https://forms.gle/9TSdDYUgxYs8SA9e8"><img src="https://img.shields.io/badge/Ray-Join%20Slack-blue" alt="join-ray-slack"></a>
<a href="https://discuss.ray.io/"><img src="https://img.shields.io/badge/Discuss-Ask%20Questions-blue" alt="discuss"></a>
<a href="https://twitter.com/raydistributed"><img src="https://img.shields.io/twitter/follow/raydistributed?label=Follow" alt="twitter"></a>

## Overview

Large Language Models (LLMs) are beginning to revolutionize how users can search for, interact with, and generate new content. Some recent stacks and toolkits around Retrieval-Augmented Generation (RAG) have emerged, enabling users to build applications such as chatbots using LLMs on their private data.

However, while setting up a naive RAG stack is straightforward, addressing a long tail of quality evaluation and scalability challenges is essential for software engineers to render their applications production-ready.

This hands-on training will guide you through using LlamaIndex and Ray to implement reliable evaluation methods for LLMs. You'll learn how to design experiments to optimize key application components and utilize scalable evaluation workflows to quantitatively compare them.

Concluding the training, you'll learn how to take the best-performing configuration and transition it into production, along with discussing the path toward continuous learning.

## Learning Outcomes

* Gain an understanding of data-related design decisions and challenges when working on LLM-powered (RAG) applications.
* Acquire practical experience in using LlamaIndex and Ray to build scalable, robust, and reliable LLM software systems.
* Design reliable and evaluable workflows and run large tuning jobs with them to quantifiable decide the best version to productionize.
* Learn how to productionize an LLM application and gather the right feedback for fine-tuning
* Extend the experience to your own specific context, applications, stack and scale

## Connect with the Ray community

You can learn and get more involved with the Ray community of developers and researchers:

* [**Ray documentation**](https://docs.ray.io/en/latest)

* [**Official Ray site**](https://www.ray.io/)
Browse the ecosystem and use this site as a hub to get the information that you need to get going and building with Ray.

* [**Join the community on Slack**](https://forms.gle/9TSdDYUgxYs8SA9e8)
Find friends to discuss your new learnings in our Slack space.

* [**Use the discussion board**](https://discuss.ray.io/)
Ask questions, follow topics, and view announcements on this community forum.

<img src="https://technical-training-assets.s3.us-west-2.amazonaws.com/Generic/ray_logo.png" width="30%" loading="lazy">
