{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "685a91ef-626a-4d76-8f7f-b89bfa6d1d6f",
   "metadata": {},
   "source": [
    "# Part 1: Developing the RAG application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ab54b8-5341-42fa-8790-93e71bbc43b5",
   "metadata": {},
   "source": [
    "- GitHub repository: https://github.com/anyscale/ray-summit-2023-training/tree/main\n",
    "- Anyscale Endpoints: https://endpoints.anyscale.com/\n",
    "- Ray documentation: https://docs.ray.io/\n",
    "- LlamaIndex documentation: https://gpt-index.readthedocs.io/en/stable/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536f1270-5328-416e-90c5-9a8e087ae354",
   "metadata": {},
   "source": [
    "We will start by building our example RAG application: a Q&A app that given a question about Ray, can answer it using the Ray documentation.\n",
    "\n",
    "In this notebook we will learn how to:\n",
    "1. ðŸ’» Develop a retrieval augmented generation (RAG) based LLM application.\n",
    "2. ðŸš€ Scale the major components (embed, index, serve, etc.) in our application.\n",
    "\n",
    "We will use both [LlamaIndex](https://gpt-index.readthedocs.io/en/stable/) and [Ray](https://docs.ray.io/) for developing our LLM application, and [Anyscale Endpoints](https://endpoints.anyscale.com/) as the LLM engine. \n",
    "\n",
    "<span style=\"background: yellow; color: red; font-size: 1rem;\"><b>DIAGRAM:</b></span> overall application view."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa52945-492f-47ae-aabc-18ad43430f6d",
   "metadata": {},
   "source": [
    "## Setup Credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f4fa1b-e1a6-402e-8f8a-462b3d02c87d",
   "metadata": {},
   "source": [
    "Let's setup our credentials for Anyscale Endpoints, and optionally for Open AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e991060f-c95d-46f0-8bb9-7a310fc17ed3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"ANYSCALE_API_BASE\"] = \"https://api.endpoints.anyscale.com/v1\"\n",
    "os.environ[\"ANYSCALE_API_KEY\"] = \"secret_wu621tehcw9gikrbku4hy1xqqv\"\n",
    "\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://api.openai.com/v1\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bd9f4f-ba08-4178-a077-a31751ae91b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 1: Loading and parsing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e0d3e2-f390-4023-b24b-6904bdd361c4",
   "metadata": {},
   "source": [
    "To build our RAG application, we first need to load, parse, and embed the data that we want to use for answering our questions. \n",
    "\n",
    "This data processing pipeline has 3 steps:\n",
    "1. First, we will load the latest documentation for Ray\n",
    "2. Then we will parse the documentation to extract out chunks of text\n",
    "3. Finally, we will **embed** each chunk. This creates a vector representation of the provided text snippet. This vector representation allows us to easily determine the similarity between two different text snippets.\n",
    "\n",
    "<span style=\"background: yellow; color: red; font-size: 1rem;\"><b>DIAGRAM:</b></span> Example of the loading, parsing, and embedding process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ca95a6-d8c4-47c8-b960-c14094967e28",
   "metadata": {},
   "source": [
    "LlamaIndex provides utlities for loading our data, and also the abstractions for how we represent our data and their relationships.\n",
    "\n",
    "Ray, and in particular the Ray Data library, is used to scale out our data processing pipeline, allowing us to process data in parallel, leveraging the cores and GPUs in our Ray cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78c1823-ac58-4bc3-a0b7-b94e8a7bac52",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d310aaa2-9cfc-4a90-bf2b-76ac06f7f68b",
   "metadata": {},
   "source": [
    "The Ray documentation has already been downloaded and is stored in shared storage directory in our Anyscale workspace. We parse the html files in the downloaded documentation, and create a Ray Dataset out of the doc paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "140a2be2-aa55-4223-8c1b-20cc4d5a1f27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "RAY_DOCS_DIRECTORY = \"/efs/shared_storage/simon/docs.ray.io/en/master/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd4ae53d-f922-4240-af8b-985d943151fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-11 18:36:51,637\tINFO worker.py:1465 -- Connecting to existing Ray cluster at address: 10.0.16.199:6379...\n",
      "2023-09-11 18:36:51,644\tINFO worker.py:1640 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-avuihpw2h1plyjdln682tfxy6c.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2023-09-11 18:36:51,646\tINFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_96b55b929742ae557051cf91262d0873.zip' (0.34MiB) to Ray cluster...\n",
      "2023-09-11 18:36:51,648\tINFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_96b55b929742ae557051cf91262d0873.zip'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3266 documents\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "docs_path = Path(RAY_DOCS_DIRECTORY)\n",
    "ds = ray.data.from_items([{\"path\": path} for path in docs_path.rglob(\"*.html\") if not path.is_dir()])\n",
    "print(f\"{ds.count()} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a94e5f-aa03-4483-b3a7-0a4509769671",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now that we have a dataset of all the paths to the html files, we now need to extract text from these HTML files. We want to do this in a generalized manner so that we can perform this extraction across all of our docs pages. \n",
    "\n",
    "Therefore, we use LlamaIndex's HTMLTagReader to identify the sections in our HTML page and then extract the text in between them. For each section of text, we create a LlamaIndex Document, and also store the source url for that section as part of the metadata for the Document. After extracting all the text, we return a list of LlamaIndex documents.\n",
    "\n",
    "<span style=\"background: yellow; color: red; font-size: 1rem;\"><b>DIAGRAM:</b></span> Example of sectionization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c872e26-615b-4d91-96f5-603d3828c177",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.readers import HTMLTagReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae9a63f7-e9da-4b33-b613-e1bf902d493d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def path_to_uri(path, scheme=\"https://\", domain=\"docs.ray.io\"):\n",
    "    # Converts the file path of a Ray documentation page to the original URL for the documentation.\n",
    "    # Example: /efs/shared_storage/goku/docs.ray.io/en/master/rllib-env.html -> https://docs.ray.io/en/master/rllib/rllib-env.html#environments\n",
    "    return scheme + domain + str(path).split(domain)[-1]\n",
    "\n",
    "def extract_sections(record):\n",
    "    # Given a HTML file path, extract out text from the section tags, and return a LlamaIndex document from each one. \n",
    "    html_file_path = record[\"path\"]\n",
    "    reader = HTMLTagReader(tag=\"section\")\n",
    "    documents = reader.load_data(html_file_path)\n",
    "    \n",
    "    # For each document, store the source URL as part of the metadata.\n",
    "    for document in documents:\n",
    "        document.metadata[\"source\"] = path_to_uri(document.metadata[\"file_path\"])\n",
    "    return [{\"document\": document} for document in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b096df-94e3-48bb-8f1b-a464b8e9ef0d",
   "metadata": {},
   "source": [
    "Let's try this out on a single example HTML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef89386e-0203-446e-97dd-243197393eea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc ID: fdaca75f-76ab-44ec-aaa1-5ef51c989b24\n",
      "Text: Environments# RLlib works with several different types of\n",
      "environments, including Farama-Foundation Gymnasium, user-defined,\n",
      "multi-agent, and also batched environments. Tip Not all environments\n",
      "work with all algorithms. Check out the algorithm overview for more\n",
      "information.\n",
      "\n",
      "\n",
      "Document source:  https://docs.ray.io/en/master/rllib/rllib-env.html\n"
     ]
    }
   ],
   "source": [
    "example_path = Path(RAY_DOCS_DIRECTORY, \"rllib/rllib-env.html\")\n",
    "document = extract_sections({\"path\": example_path})[0][\"document\"]\n",
    "print(document)\n",
    "print(\"\\n\")\n",
    "print(\"Document source: \", document.metadata[\"source\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea63b9b8-1873-4595-a5e4-fb7debc78f32",
   "metadata": {},
   "source": [
    "Now, let's use Ray Data to parallelize this across all of the HTML files. We can stitch together operations on our Ray dataset to map a function over each document. \n",
    "\n",
    "Ray Data is lazy by default, so can first stitch together our entire pipeline, and then trigger execution. This allows Ray Data to fully optimize resource usage for our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a3dcf6e-a12f-423d-8141-e602798f6c42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-11 18:36:55,642\tINFO streaming_executor.py:93 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[FlatMap(extract_sections)] -> LimitOperator[limit=1]\n",
      "2023-09-11 18:36:55,643\tINFO streaming_executor.py:94 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-09-11 18:36:55,644\tINFO streaming_executor.py:96 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Column    Type\n",
       "------    ----\n",
       "document  <class 'object'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections_ds = ds.flat_map(extract_sections)\n",
    "sections_ds.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5322a66f-4209-4e29-bba3-269052329ec8",
   "metadata": {},
   "source": [
    "### Chunk data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77115cd-734a-4228-b49e-5546739b8694",
   "metadata": {},
   "source": [
    "We now have a list of Documents (with text and source of each section) but we shouldn't directly use this as context to our RAG application just yet. The text lengths of each section are all varied and many are quite large chunks. If were to use these large sections, then we'd be inserting a lot of noisy/unwanted context and because all LLMs have a maximum context length, we wouldn't be able to fit too many relevant contexts. Therefore, we're going to split the text within each section into smaller chunks. Intuitively, smaller chunks will encapsulate single/few concepts and will be less noisy compared to larger chunks. We're going to choose some typical text splitting values (ex. `chunk_size=300`) to create our chunks for now but we'll be experiments with a range of values later.\n",
    "\n",
    "<span style=\"background: yellow; color: red; font-size: 1rem;\"><b>DIAGRAM:</b></span> Sample chunking logic in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f58b811-153d-4cc0-9f04-a1df48ce82a9",
   "metadata": {},
   "source": [
    "Once again, we will use LlamaIndex's abstractions to chunk each Document into a **Node** with the provided chunk size. And we will use Ray Data to parallelize the chunking computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7b8eecd-b74a-4d8b-b6af-942917755925",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.node_parser import SimpleNodeParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52d8bb61-ada8-4f9f-a838-64b4d86614b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunk_size = 300\n",
    "chunk_overlap = 50\n",
    "\n",
    "def chunk_document(document):\n",
    "    node_parser = SimpleNodeParser.from_defaults(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    nodes = node_parser.get_nodes_from_documents([document[\"document\"]])\n",
    "    return [{\"node\": node} for node in nodes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2dfc56-d6c4-47ff-8f3a-5a5420ebe9d8",
   "metadata": {},
   "source": [
    "Let's run an example over a single document. The document wil be chunked and will result in 2 nodes, each representing 1 chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddbbc34b-8cb5-4ee4-9d53-cd8d873afb0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-11 18:37:02,205\tINFO dataset.py:2380 -- Tip: Use `take_batch()` instead of `take() / show()` to return records in pandas or numpy batch format.\n",
      "2023-09-11 18:37:02,207\tINFO streaming_executor.py:93 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[FlatMap(extract_sections)] -> LimitOperator[limit=1]\n",
      "2023-09-11 18:37:02,207\tINFO streaming_executor.py:94 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-09-11 18:37:02,208\tINFO streaming_executor.py:96 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /tmp/llama_index...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num chunks:  1\n",
      "Example text: Reference#\n",
      "Monitor and debug your Ray applications and clusters using the API and CLI documented in these references.\n",
      "The guides include:\n",
      "State API\n",
      "State CLI\n",
      "System Metrics\n",
      "\n",
      "Example metadata: {'tag': 'section', 'tag_id': 'reference', 'file_path': '/efs/shared_storage/simon/docs.ray.io/en/master/ray-observability/reference/index.html', 'source': 'https://docs.ray.io/en/master/ray-observability/reference/index.html'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_document = sections_ds.take(1)[0]\n",
    "\n",
    "# Nodes\n",
    "nodes = chunk_document(sample_document)\n",
    "\n",
    "print(\"Num chunks: \", len(nodes))\n",
    "print(f\"Example text: {nodes[0]['node'].text}\\n\")\n",
    "print(f\"Example metadata: {nodes[0]['node'].metadata}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0491bab5-bea6-4fbc-9347-0645e0df2e33",
   "metadata": {},
   "source": [
    "Now let's chunk all of our documents, stitching this operation into our Ray Dataset pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "018a79ac-d28e-474c-bbe3-da4cf8e0adaf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-11 18:37:04,169\tINFO streaming_executor.py:93 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[FlatMap(extract_sections)] -> TaskPoolMapOperator[FlatMap(chunk_document)] -> LimitOperator[limit=1]\n",
      "2023-09-11 18:37:04,171\tINFO streaming_executor.py:94 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-09-11 18:37:04,172\tINFO streaming_executor.py:96 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Column  Type\n",
       "------  ----\n",
       "node    <class 'object'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.util.scheduling_strategies import NodeAffinitySchedulingStrategy\n",
    "\n",
    "chunks_ds = sections_ds.flat_map(chunk_document, scheduling_strategy=NodeAffinitySchedulingStrategy(node_id=ray.get_runtime_context().get_node_id(), soft=False))\n",
    "chunks_ds.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0321c4d-6244-4aa7-834f-f3d117e78f76",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Embed data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5790a1-d153-4aaa-b28f-17f2c501810e",
   "metadata": {},
   "source": [
    "Now that we've created small chunks from our dataset, we need a way to identify the most relevant ones to a given query. A very effective and quick method is to embed our data using a pretrained model and use the same model to embed the query. We can then compute the distance between all of the chunk embeddings and our query embedding to determine the top k chunks. There are many different pretrained models to choose from to embed our data but the most popular ones can be discovered through [HuggingFace's Massive Text Embedding Benchmark (MTEB)](https://huggingface.co/spaces/mteb/leaderboard) leadboard. These models were pretrained on very large text corpus through tasks such as next/masked token prediction that allows them to learn to represent subtokens in N dimensions and capture semantic relationships. We can leverage this to represent our data and make decisions such as the most relevant contexts to use to answer a given query. We're using Langchain's Embedding wrappers ([HuggingFaceEmbeddings](https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.huggingface.HuggingFaceEmbeddings.html) and [OpenAIEmbeddings](https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.openai.OpenAIEmbeddings.html)) to easily load the models and embed our document chunks.\n",
    "\n",
    "**Note**: embeddings aren't the only way to determine the more relevant chunks. We could also use an LLM to decide! However, because LLMs are much larger than these embedding models and have maximum context lengths, it's better to use embeddings to retrieve the top k chunks. And then we could use LLMs on the fewer k chunks to determine the <k chunks to use as the context to answer our query. We could also use reranking (ex. [Cohere Rerank](https://txt.cohere.com/rerank/)) to further identify the most relevant chunks to use.\n",
    "\n",
    "<span style=\"background: yellow; color: red; font-size: 1rem;\"><b>DIAGRAM:</b></span> Represent a text chunk getting embedded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93b4fc25-4ac4-4913-b098-f5ea7ee35dd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "def get_embedding_model(model_name):\n",
    "    if model_name == \"text-embedding-ada-002\":\n",
    "            return OpenAIEmbeddings(\n",
    "                model=model_name,\n",
    "                openai_api_base=os.environ[\"OPENAI_API_BASE\"],\n",
    "                openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "    else:\n",
    "        model_kwargs = {\"device\": \"cuda\"}\n",
    "        encode_kwargs = {\"device\": \"cuda\", \"batch_size\": 100}\n",
    "\n",
    "        return HuggingFaceEmbeddings(\n",
    "            model_name=model_name,\n",
    "            model_kwargs=model_kwargs,\n",
    "            encode_kwargs=encode_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05da2395-d40a-41ab-806f-1e7bd0f9048f",
   "metadata": {},
   "source": [
    "Here, we will use a Python **class** instead of a function to encapsulate the embedding logic. Since loading the embedding model is not cheap, we want to load the model just once and re-use the loaded model when embedding each batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abe0ee5f-72b4-4439-938f-30b5dacda82a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EmbedChunks:\n",
    "    def __init__(self, model_name):\n",
    "        self.embedding_model = get_embedding_model(model_name)\n",
    "    \n",
    "    def __call__(self, node_batch):\n",
    "        # Get the batch of text that we want to embed.\n",
    "        nodes = node_batch[\"node\"]\n",
    "        text = [node.text for node in nodes]\n",
    "        \n",
    "        # Embed the batch of text.\n",
    "        embeddings = self.embedding_model.embed_documents(text)\n",
    "        assert len(nodes) == len(embeddings)\n",
    "\n",
    "        # Store the embedding in the LlamaIndex node.\n",
    "        for node, embedding in zip(nodes, embeddings):\n",
    "            node.embedding = embedding\n",
    "        return {\"embedded_nodes\": nodes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a5c8865-a304-4f56-82d3-42741d67c3c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the embedding model to use.\n",
    "# Specify \"text-embedding-ada-002\" for Open AI embeddings.\n",
    "embedding_model_name = \"thenlper/gte-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c900079-807a-49a7-8748-946e1cb68c28",
   "metadata": {},
   "source": [
    "Let's try this out on an example chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5961125-cd55-4e1f-800d-d9383a8ef03b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-11 18:37:10,815\tINFO streaming_executor.py:93 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[FlatMap(extract_sections)] -> TaskPoolMapOperator[FlatMap(chunk_document)] -> LimitOperator[limit=1]\n",
      "2023-09-11 18:37:10,816\tINFO streaming_executor.py:94 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-09-11 18:37:10,817\tINFO streaming_executor.py:96 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a09684987f448d3b1b4d35fa7a1ddfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)9c8a9/.gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c88bd07b617f4649bc0477b0de87b74c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a490bec6f58b45cda7b82258698f3686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)db4ec9c8a9/README.md:   0%|          | 0.00/68.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c8ec84da01e40c9a13c1aa64eb76a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)4ec9c8a9/config.json:   0%|          | 0.00/618 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2fe5f51d97e4fe1bf28dda3163085a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)8a9/onnx/config.json:   0%|          | 0.00/630 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4eacd9adf1f4986a6ef9b4fc9017e45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.onnx:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b12a4525eb2c450788969e02516b085b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eee8d7cf4c440b4acd73574b4963ce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)/onnx/tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6060f577ff0f4aa08ff84a606cc59240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0353c1854042ac8bef9289c8aa72e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)9c8a9/onnx/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "711e92d96e094d94a66a511662fce55b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/219M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef90f9d8adb74eb192da1593118eb642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)nce_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62d8b166a19428f94bea7c81445b335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "705531980f7d4f099a444ef2917b4cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)9c8a9/tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ad4b9a79f9a4ba4b3231e0d2a42c2ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5943045c99e4053aa9f4d437154971a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)db4ec9c8a9/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba0724c3aea84138a40afff95ce764d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)ec9c8a9/modules.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_chunk = chunks_ds.take_batch(1)\n",
    "embedder = EmbedChunks(model_name=embedding_model_name)\n",
    "example_node_with_embedding = embedder(example_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c707dd7-6101-4a8a-bebe-75b39f34802f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID: efbf5ce7-e020-4618-b22d-0c39b630104b\n",
      "Text:\n",
      "ray.tune.search.basic_variant.BasicVariantGenerator.CKPT_FILE_TMPL#\n",
      "BasicVariantGenerator.CKPT_FILE_TMPL = 'basic-variant-state-{}.json'#\n",
      "\n",
      "\n",
      "Embedding size:  768\n"
     ]
    }
   ],
   "source": [
    "print(example_node_with_embedding[\"embedded_nodes\"][0])\n",
    "print(\"\\n\")\n",
    "print(\"Embedding size: \", len(example_node_with_embedding[\"embedded_nodes\"][0].embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfea63ce-179a-44b0-96e7-80ff485fc9df",
   "metadata": {},
   "source": [
    "We're now able to embed our chunks at scale by using the [map_batches](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html) operation in our Ray Data pipeline.\n",
    "\n",
    "All we have to do is define the `batch_size` and the compute to use (we're using two workers, each with 1 GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98253c11-214a-43c2-a9b9-7df65611b7f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ray.data import ActorPoolStrategy\n",
    "\n",
    "embedded_chunks = chunks_ds.map_batches(\n",
    "    EmbedChunks,\n",
    "    fn_constructor_kwargs={\"model_name\": embedding_model_name},\n",
    "    batch_size=100, \n",
    "    num_gpus=1 if embedding_model_name!=\"text-embedding-ada-002\" else 0,\n",
    "    compute=ActorPoolStrategy(size=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c706d2f-bf04-4a6c-8024-c792f8ac00da",
   "metadata": {},
   "source": [
    "### Index data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c354d707-3dfb-48ec-af8a-484814deeccb",
   "metadata": {},
   "source": [
    "Now that we have our embedded chunks, we need to index (store) them somewhere so that we can retrieve them quickly for inference. While there are many popular vector database options, we're going to use [Postgres](https://www.postgresql.org/) for it's simplificty and performance. We'll create a table (`document`) and write the (`text`, `source`, `embedding`) triplets for each embedded chunk we have.\n",
    "\n",
    "<span style=\"background: yellow; color: red; font-size: 1rem;\"><b>DIAGRAM:</b></span> Show a triplet getting indexed in a vector DB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d26ef0f-14a5-423c-a429-c6d71dfe6e03",
   "metadata": {},
   "source": [
    "Let's setup a Postgres database. We have already installed Postgres for you in this workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1203cbbb-965a-4055-bd48-7e1ad8fb2f43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Starting PostgreSQL 15 database server\n",
      "   ...done.\n",
      "ALTER ROLE\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Set up pgvector\n",
    "bash ../setup-pgvector.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587030d3-4b28-4cf3-82c4-08bfcc7fa3c9",
   "metadata": {},
   "source": [
    "As the final step in our data pipeline, we will store the embeddings into our Postgres database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d2cd4e1-5d5b-4ccb-bb4e-5ef6bbc4f70c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.vector_stores import PGVectorStore\n",
    "\n",
    "# First create the table.\n",
    "def get_postgres_store():\n",
    "    return PGVectorStore.from_params(\n",
    "            database=\"postgres\", \n",
    "            user=\"postgres\", \n",
    "            password=\"postgres\", \n",
    "            host=\"localhost\", \n",
    "            table_name=\"document\",\n",
    "            port=\"5432\",\n",
    "            embed_dim=768,\n",
    "        )\n",
    "\n",
    "store = get_postgres_store()\n",
    "del store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "139ff2a5-fc04-456d-8f6a-f2b408ab80ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StoreResults:\n",
    "    def __init__(self):\n",
    "        self.vector_store = get_postgres_store()\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        embedded_nodes = batch[\"embedded_nodes\"]\n",
    "        self.vector_store.add(list(embedded_nodes))\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "295c75b2-3d1d-4dd0-af1f-046016a3f2ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-11 18:37:23,190\tINFO streaming_executor.py:93 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[FlatMap(extract_sections)] -> TaskPoolMapOperator[FlatMap(chunk_document)] -> ActorPoolMapOperator[MapBatches(EmbedChunks)] -> ActorPoolMapOperator[MapBatches(StoreResults)]\n",
      "2023-09-11 18:37:23,191\tINFO streaming_executor.py:94 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-09-11 18:37:23,192\tINFO streaming_executor.py:96 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "2023-09-11 18:37:23,212\tINFO actor_pool_map_operator.py:106 -- MapBatches(EmbedChunks): Waiting for 2 pool actors to start...\n",
      "Downloading (â€¦)9c8a9/.gitattributes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.52k/1.52k [00:00<00:00, 12.5MB/s]\n",
      "Downloading (â€¦)_Pooling/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 190/190 [00:00<00:00, 2.58MB/s]\n",
      "Downloading (â€¦)db4ec9c8a9/README.md: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68.1k/68.1k [00:00<00:00, 86.5MB/s]\n",
      "Downloading (â€¦)4ec9c8a9/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 618/618 [00:00<00:00, 8.42MB/s]\n",
      "Downloading (â€¦)8a9/onnx/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 630/630 [00:00<00:00, 9.02MB/s]\n",
      "Downloading model.onnx:   0%|          | 0.00/436M [00:00<?, ?B/s]\n",
      "Downloading model.onnx:   5%|â–         | 21.0M/436M [00:00<00:02, 176MB/s]\n",
      "Downloading model.onnx:  12%|â–ˆâ–        | 52.4M/436M [00:00<00:01, 229MB/s]\n",
      "Downloading model.onnx:  19%|â–ˆâ–‰        | 83.9M/436M [00:00<00:01, 251MB/s]\n",
      "Downloading model.onnx:  26%|â–ˆâ–ˆâ–‹       | 115M/436M [00:00<00:01, 249MB/s] \n",
      "Downloading model.onnx:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 147M/436M [00:00<00:01, 241MB/s]\n",
      "Downloading model.onnx:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178M/436M [00:00<00:01, 241MB/s]\n",
      "Downloading model.onnx:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 210M/436M [00:00<00:00, 237MB/s]\n",
      "Downloading model.onnx:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 241M/436M [00:01<00:00, 235MB/s]\n",
      "Downloading model.onnx:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273M/436M [00:01<00:00, 234MB/s]\n",
      "Downloading model.onnx:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 304M/436M [00:01<00:00, 234MB/s]\n",
      "Downloading model.onnx:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 336M/436M [00:01<00:00, 233MB/s]\n",
      "Downloading model.onnx:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 367M/436M [00:01<00:00, 232MB/s]\n",
      "Downloading model.onnx:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398M/436M [00:01<00:00, 236MB/s]\n",
      "Downloading model.onnx:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 430M/436M [00:01<00:00, 243MB/s]\n",
      "Downloading model.onnx: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436M/436M [00:01<00:00, 236MB/s]\n",
      "Downloading (â€¦)cial_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:00<00:00, 1.64MB/s]\n",
      "Downloading (â€¦)/onnx/tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]\n",
      "Downloading (â€¦)/onnx/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 712k/712k [00:00<00:00, 5.23MB/s]\n",
      "Downloading (â€¦)okenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:00<00:00, 4.33MB/s]\n",
      "Downloading (â€¦)9c8a9/onnx/vocab.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232k/232k [00:00<00:00, 46.3MB/s]\n",
      "Downloading pytorch_model.bin:   0%|          | 0.00/219M [00:00<?, ?B/s]\n",
      "Downloading pytorch_model.bin:  14%|â–ˆâ–        | 31.5M/219M [00:00<00:00, 247MB/s]\n",
      "Downloading pytorch_model.bin:  29%|â–ˆâ–ˆâ–Š       | 62.9M/219M [00:00<00:00, 243MB/s]\n",
      "Downloading pytorch_model.bin:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 94.4M/219M [00:00<00:00, 244MB/s]\n",
      "Downloading pytorch_model.bin:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 126M/219M [00:00<00:00, 249MB/s] \n",
      "Downloading pytorch_model.bin:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 157M/219M [00:00<00:00, 253MB/s]\n",
      "Downloading pytorch_model.bin:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 189M/219M [00:00<00:00, 255MB/s]\n",
      "Downloading pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 219M/219M [00:00<00:00, 251MB/s]\n",
      "Downloading (â€¦)nce_bert_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57.0/57.0 [00:00<00:00, 659kB/s]\n",
      "Downloading (â€¦)cial_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:00<00:00, 1.68MB/s]\n",
      "Downloading (â€¦)9c8a9/tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]\n",
      "Downloading (â€¦)9c8a9/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 712k/712k [00:00<00:00, 2.63MB/s]\n",
      "Downloading (â€¦)okenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:00<00:00, 3.88MB/s]\n",
      "Downloading (â€¦)db4ec9c8a9/vocab.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232k/232k [00:00<00:00, 52.3MB/s]\n",
      "Downloading (â€¦)ec9c8a9/modules.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 385/385 [00:00<00:00, 4.52MB/s]\n",
      "2023-09-11 18:37:36,338\tINFO actor_pool_map_operator.py:106 -- MapBatches(StoreResults): Waiting for 8 pool actors to start...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-11 18:38:15,302\tWARNING actor_pool_map_operator.py:265 -- Your batch size is too large. Currently, your batch size is 128. Your dataset contains 0, and Ray Data tried to parallelize it across 8 actors. To parallelize this fully across all 8 actors, set batch size to not exceed `0 / 8 = 0`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store all the embeddings in Postgres, and trigger exection of the Ray Data pipeline.\n",
    "from ray.util.scheduling_strategies import NodeAffinitySchedulingStrategy\n",
    "\n",
    "embedded_chunks.map_batches(\n",
    "    StoreResults,\n",
    "    batch_size=128,\n",
    "    num_cpus=1,\n",
    "    compute=ActorPoolStrategy(size=8),\n",
    "    # Since our database is only created on the head node, we need to force the Ray tasks to only executed on the head node.\n",
    "    scheduling_strategy=NodeAffinitySchedulingStrategy(node_id=ray.get_runtime_context().get_node_id(), soft=False)\n",
    "    \n",
    ").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac01c84b-019b-4005-9bb8-4d4366003ef2",
   "metadata": {},
   "source": [
    "Let's check our table to see how many chunks that we have stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24e45b87-f44a-4cbc-8a75-78a8db2fe20e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " count \n",
      "-------\n",
      " 16353\n",
      "(1 row)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "sudo -u postgres psql -c \"SELECT count(*) FROM data_document;\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b0a6c5-2963-43c4-b002-7b2c48c69842",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d7b49e-5cda-4542-8286-1e004c59db9f",
   "metadata": {},
   "source": [
    "Now that we have processed, embedded, and stored all of our chunks from the Ray documentation, we can test out the retrieval portion of the application.\n",
    "\n",
    "In the retrieval portion, we want to pull the relevant context for a given query. We do this by embedding the query using the same embedding model we used to embed the chunks, and then check for similarity between the embedded query and all the embedded chunks to pull the most relevant context.\n",
    "\n",
    "<span style=\"background: yellow; color: red; font-size: 1rem;\"><b>DIAGRAM:</b></span> Show the query getting embedded and show the retrieval process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6749178b-f02a-4c2a-8c62-3b0b8dcce1f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, ServiceContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a3d0823-721f-4813-8619-b5bc7eb15333",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a connection to our Postgres vector store\n",
    "vector_store = get_postgres_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d54980d-2749-44f4-a19d-c6dedf05638a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the same embedding model that we used to embed our documents.\n",
    "embedding_model = get_embedding_model(embedding_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d2f3071-c7b5-407f-a344-093449235a14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create our retriever.\n",
    "service_context = ServiceContext.from_defaults(embed_model=embedding_model)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store, service_context=service_context)\n",
    "\n",
    "# Fetch the top 5 most relevant chunks.\n",
    "retriever = index.as_retriever(similarity_top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dceebd-9c40-4315-b940-3026d3190533",
   "metadata": {},
   "source": [
    "Now, let's try a sample query and pull the most relevant context. Looks like the retrieval is working great! From the eye-test, it looks like the chunks are all relevant to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ba1ce48-ba88-456c-8f42-262bb0a3ce40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID: 48a4d8af-b766-40e0-ad98-d9b6ee12adf5\n",
      "Text: Configuring batch size# Increasing batch_size improves the\n",
      "performance of vectorized transformations like NumPy functions and\n",
      "model inference. However, if your batch size is too large, your\n",
      "program might run out of memory. If you encounter an out-of-memory\n",
      "error, decrease your batch_size. Note The default batch size depends\n",
      "on your resource type...\n",
      "Score:  0.908\n",
      "\n",
      "Source:  https://docs.ray.io/en/master/data/transforming-data.html\n",
      "Node ID: 2a9832b9-8f1b-4bb4-9f62-7cf40a3c96d6\n",
      "Text: batch_size=720,  # Use the largest batch size that can fit on\n",
      "our GPUs )\n",
      "Score:  0.903\n",
      "\n",
      "Source:  https://docs.ray.io/en/master/data/examples/pytorch_resnet_batch_prediction.html\n",
      "Node ID: 0c91c99c-8e9d-47a8-991f-3c95a122ef42\n",
      "Text: Configuring Batch Size# Configure the size of the input batch\n",
      "thatâ€™s passed to __call__ by setting the batch_size argument for\n",
      "ds.map_batches() Increasing batch size results in faster execution\n",
      "because inference is a vectorized operation.For GPU inference,\n",
      "increasing batch size increases GPU utilization.Set the batch size to\n",
      "as large possible wi...\n",
      "Score:  0.898\n",
      "\n",
      "Source:  https://docs.ray.io/en/master/data/batch_inference.html\n",
      "Node ID: 30d5273e-0c92-4bff-8ef6-9e8065ff66e6\n",
      "Text: Defaults to 1. batch_size â€“ The number of rows in each batch, or\n",
      "None to use entire blocks as batches (blocks may contain different\n",
      "numbers of rows).The final batch may include fewer than batch_size\n",
      "rows if drop_last is False.Defaults to 256. batch_format â€“ If\n",
      "\"default\" or \"numpy\", batches are Dict[str, numpy.ndarray].If\n",
      "\"pandas\", batches are pa...\n",
      "Score:  0.894\n",
      "\n",
      "Source:  https://docs.ray.io/en/master/data/api/doc/ray.data.Dataset.iter_batches.html\n",
      "Node ID: 9b7b62ba-1156-4222-9003-2c905e4c3b87\n",
      "Text: Defaults to 1. batch_size â€“ The number of rows in each batch, or\n",
      "None to use entire blocks as batches (blocks may contain different\n",
      "numbers of rows).The final batch may include fewer than batch_size\n",
      "rows if drop_last is False.Defaults to 256. dtypes â€“ The TensorFlow\n",
      "dtype(s) for the created tensor(s); if None, the dtype is inferred\n",
      "from the tens...\n",
      "Score:  0.892\n",
      "\n",
      "Source:  https://docs.ray.io/en/master/data/api/doc/ray.data.Dataset.iter_tf_batches.html\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the default batch size for map_batches?\"\n",
    "nodes = retriever.retrieve(query)\n",
    "\n",
    "for node in nodes:\n",
    "    print(node)\n",
    "    print(\"Source: \", node.metadata[\"source\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06ee9e3-9482-41c3-9edb-852d2376a276",
   "metadata": {},
   "source": [
    "## Response generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87f064e-50bd-46f4-b8ed-7a367b5dcb8b",
   "metadata": {},
   "source": [
    "With our retrieval working, we can now build the next portion of our LLM application, which is the actual response generation.\n",
    "\n",
    "In this step, we pass in both the query and the relevant contex to an LLM. The LLM synthesizes a response to the query given the context. Without this relevant context that we retreived, the LLM may not have been able to accurately answer our question. And as our data grows, we can just as easily embed and index any new data and be able to retrieve it to answer questions.\n",
    "\n",
    "<span style=\"background: yellow; color: red; font-size: 1rem;\"><b>DIAGRAM:</b></span> Show how retrieved context + query texts are fed into API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db024edc-1922-4ec4-b90d-31def67a5f5e",
   "metadata": {},
   "source": [
    "Creating an end-to-end query engine becomes very easy with LlamaIndex and Anyscale Endpoints. With Anyscale endpoints, we can use open source LLMs, like Llama2 models, just as easy as Open AI, but more cost effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b4539eb1-b429-4a63-b96c-f8867953e518",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +12m8s)\u001b[0m [workspace snapshot] New snapshot created successfully (size: 1.44 MB).\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms import Anyscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97e1041f-ba18-48ac-b4e6-4527973f0159",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use Anyscale endpoints as the LLM to LlamaIndex.\n",
    "llm = Anyscale(model=\"meta-llama/Llama-2-70b-chat-hf\", temperature=0.1)\n",
    "\n",
    "# Use the same embedding model that we used to embed our documents.\n",
    "embedding_model = get_embedding_model(embedding_model_name)\n",
    "\n",
    "service_context = ServiceContext.from_defaults(embed_model=embedding_model, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e117dab2-b483-417f-af63-d65e4fe51231",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create our query engine.\n",
    "vector_store = get_postgres_store()\n",
    "index = VectorStoreIndex.from_vector_store(vector_store, service_context=service_context)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f923c14d-acb3-4598-84c8-c715a36d131d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get a response to our query.\n",
    "\n",
    "query = \"What is the default batch size for map_batches?\"\n",
    "response = query_engine.query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfa881e-a6ad-4d03-97a6-59a70827e4cf",
   "metadata": {},
   "source": [
    "Let's see the response to our query, as well as the retrieved context that we passed to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f07b97eb-ef73-4fd2-9d80-49802048fe84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  The default batch size for map_batches is 4096.\n",
      "\n",
      "\n",
      "Text:  Configuring batch size#\n",
      "Increasing batch_size improves the performance of vectorized transformations like\n",
      "NumPy functions and model inference. However, if your batch size is too large, your\n",
      "program might run out of memory. If you encounter an out-of-memory error, decrease your\n",
      "batch_size.\n",
      "Note\n",
      "The default batch size depends on your resource type. If youâ€™re using CPUs,\n",
      "the default batch size is 4096. If youâ€™re using GPUs, you must specify an explicit\n",
      "batch size.\n",
      "Score:  0.907563624880965\n",
      "Source:  https://docs.ray.io/en/master/data/transforming-data.html\n",
      "\n",
      "\n",
      "Text:  batch_size=720,  # Use the largest batch size that can fit on our GPUs\n",
      ")\n",
      "Score:  0.9032071311925921\n",
      "Source:  https://docs.ray.io/en/master/data/examples/pytorch_resnet_batch_prediction.html\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Response: \", response.response)\n",
    "print(\"\\n\")\n",
    "source_nodes = response.source_nodes\n",
    "\n",
    "for node in source_nodes:\n",
    "    print(\"Text: \", node.node.text)\n",
    "    print(\"Score: \", node.score)\n",
    "    print(\"Source: \", node.node.metadata[\"source\"])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1531b7a-9a38-41ef-bd33-e20af27b670e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
