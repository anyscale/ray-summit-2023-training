{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c044803a-3fe2-4297-ad71-c93ae2e078f5",
   "metadata": {},
   "source": [
    "# Part 2: Evaluating our LLM application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc13cfb-5e8a-401d-bb04-24a5793e69be",
   "metadata": {},
   "source": [
    "So far, we've chosen typical/arbitrary values for the various parts of our RAG application. But if we were to change something, such as our chunking logic, embedding model, LLM, etc. how can we know that we have a better configuration than before. A generative task like this is very difficult to quantitatively assess and so we need to develop creative ways to do so. \n",
    "\n",
    "Because we have many moving parts in our application, we need to perform unit/component and end-to-end evaluation. Component-wise evaluation can involve evaluating our retrieval in isolation (is the best source in our set of retrieved chunks) and evaluating our LLMs response (given the best source, is the LLM able to produce a quality answer). As for end-to-end evaluation, we can assess the quality of the entire system (given all data, what is the quality of the response).\n",
    "\n",
    "<span style=\"background: yellow; color: red; font-size: 1rem;\"><b>DIAGRAM:</b></span> Component and end-to-end evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b67c46c-7650-484b-a792-7eaf62c0e82e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "119575d6-b0cc-49f3-a118-46bc3adf8189",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"ANYSCALE_API_BASE\"] = \"https://api.endpoints.anyscale.com/v1\"\n",
    "os.environ[\"ANYSCALE_API_KEY\"] = \"secret_wu621tehcw9gikrbku4hy1xqqv\"\n",
    "\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://api.openai.com/v1\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cedb0a-a7b3-4194-88a9-355122cd8a00",
   "metadata": {},
   "source": [
    "## Golden Context Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31583b8a-bd08-4054-95f6-2ae5256e6a21",
   "metadata": {},
   "source": [
    "In an ideal world, we would have a golden validation dataset: given a set of queries, we would have the correct sources that answer those queries, and optionally the correct answer that should be returned by the LLM.\n",
    "\n",
    "For this example, we have manually collected 177 representative user queries and identified the correct source in the documentation that answer those user queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8495831b-c4f9-4e84-a211-bc37ecf369e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "golden_dataset_path = Path(\"../datasets/eval-dataset-v1.jsonl\")\n",
    "\n",
    "with open(golden_dataset_path, \"r\") as f:\n",
    "    data = [json.loads(item) for item in list(f)]\n",
    "    \n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b187c2d5-5c4d-4b9a-9e5b-5f37c9e92b36",
   "metadata": {},
   "source": [
    "Our dataset contains 'question' and 'source' pairs. If we have a golden context dataset, it is the best option for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2300e0fc-3e65-4f43-98b6-3564bc1ccb3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'Iâ€™m struggling a bit with Ray Data type conversions when I do map_batches. Any advice?',\n",
       "  'source': 'https://docs.ray.io/en/master/data/transforming-data.html#configuring-batch-format'},\n",
       " {'question': 'How does autoscaling work in a Ray Serve application?',\n",
       "  'source': 'https://docs.ray.io/en/master/serve/scaling-and-resource-allocation.html#autoscaling'},\n",
       " {'question': 'how do I get the address of a ray node',\n",
       "  'source': 'https://docs.ray.io/en/master/ray-core/miscellaneous.html#node-information'},\n",
       " {'question': 'Does Ray support NCCL?',\n",
       "  'source': 'https://docs.ray.io/en/master/ray-more-libs/ray-collective.html'},\n",
       " {'question': 'Is Ray integrated with DeepSpeed?',\n",
       "  'source': 'https://docs.ray.io/en/master/ray-air/examples/gptj_deepspeed_fine_tuning.html#fine-tuning-the-model-with-ray-air-a-name-train-a'}]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6739202-90fa-46e8-bc7d-3c0ed33c8673",
   "metadata": {},
   "source": [
    "## Cold Start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20636b3e-5211-41c9-a390-0cfa23654c44",
   "metadata": {
    "tags": []
   },
   "source": [
    "We may not always have a prepared dataset of questions and the best source to answer that question readily available. To address this cold start problem, we could use an LLM to look at our documents and generate questions that the specific chunk would answer. This provides us with quality questions and the exact source the answer is in. However, this dataset generation method could be a bit noisy. The generate questions may not always be resembling of what your users may ask and the specific chunk we say is the best source may also have that exact information in other chunks. Nonetheless, this is a great way to start our development process while we collect + manually label a high quality dataset.\n",
    "\n",
    "<span style=\"background: yellow; color: red; font-size: 1rem;\"><b>DIAGRAM:</b></span> Show the synthetic data generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d718383c-74a2-4518-a51b-69a7857c5cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.evaluation import DatasetGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba0b1e6-4bc6-440b-a4e6-3f9915196f97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64373866-d0f7-4704-b03b-5c466fc59afb",
   "metadata": {},
   "source": [
    "Since we already have a dataset with representative user queries and ground truth labels, we will use that for evaluation instead of a synthetically generated dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330a7e10-18fc-495e-ab4a-da2f3ca00a97",
   "metadata": {},
   "source": [
    "## Evaluating Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6e7e79-5a2a-4247-bc3a-d093d0416761",
   "metadata": {},
   "source": [
    "The first component to evaluate in our RAG application is retrieval. Given a query, is our retriever pulling in the correct context to answer that query? Regardless of how good our LLM is, if it does not have the right context to answer the question, it cannot provide the right answer.\n",
    "\n",
    "We can use our golden context dataset to evaluate retrieval. The simplest approach is that for each query in our dataset, we can test to see if the correct source is included in any of the chunks that are retrieved by our retriever. This measures \"hit rate\".\n",
    "\n",
    "However, simply checking for existence can be misleading if we increase the number of chunks that we retrieve. Therefore, we also want to check the score that our retriever gives for the correct source. A higher score means our retriever is accurately determining the correct context. \n",
    "\n",
    "To summarize, for each query in our evaluation dataset, we will measure the following:\n",
    "1. Is the correct source included in any of the retrived chunks?\n",
    "2. What is the score our retriever gives to the correct source?\n",
    "\n",
    "<span style=\"background: yellow; color: red; font-size: 1rem;\"><b>DIAGRAM:</b></span> Show retrieval evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a5958c-74d8-4a11-b46c-a12a8fd5ad99",
   "metadata": {},
   "source": [
    "First, let's a get a retriever over the vector database. We have packaged this as a utility. It is the same as we did in notebook 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "b88e0d5f-a0ed-47e9-afcf-0ac882201e57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import get_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "dad0afdc-2f44-4ca7-b5c0-2c6499f4f35e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever = get_retriever(similarity_top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3b5438-9930-4857-a9a5-165cbd15c79b",
   "metadata": {},
   "source": [
    "Now let's evaluate our retriever. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "bb4cd2ae-17ff-4e75-904f-a577499bc8ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "\n",
    "for entry in data:\n",
    "    query = entry[\"question\"]\n",
    "    expected_source = entry['source']\n",
    "    \n",
    "    retrieved_nodes = retriever.retrieve(query)\n",
    "    retrieved_sources = [node.metadata['source'] for node in retrieved_nodes]\n",
    "    \n",
    "    # If our label does not include a section, then any sections on the page should be considered a hit.\n",
    "    if \"#\" not in expected_source:\n",
    "        retrieved_sources = [source.split(\"#\")[0] for source in retrieved_sources]\n",
    "    \n",
    "    if expected_source in retrieved_sources:\n",
    "        is_hit = True\n",
    "        score = retrieved_nodes[retrieved_sources.index(expected_source)].score\n",
    "    else:\n",
    "        is_hit = False\n",
    "        score = 0.0\n",
    "    \n",
    "    result = {\n",
    "        \"is_hit\": is_hit,\n",
    "        \"score\": score,\n",
    "        \"retrieved\": retrieved_sources,\n",
    "        \"expected\": expected_source,\n",
    "        \"query\": query,\n",
    "    }\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "a12ecc64-efae-4a67-86de-3f3fbb36820b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'is_hit': True,\n",
       "  'score': 0.9110969673181731,\n",
       "  'retrieved': ['https://docs.ray.io/en/master/data/transforming-data.html#configuring-batch-format',\n",
       "   'https://docs.ray.io/en/master/data/working-with-tensors.html#transforming-tensor-data',\n",
       "   'https://docs.ray.io/en/master/data/working-with-pytorch.html#transformations-with-torch-tensors',\n",
       "   'https://docs.ray.io/en/master/data/examples/pytorch_resnet_batch_prediction.html#preprocessing',\n",
       "   'https://docs.ray.io/en/master/data/transforming-data.html#transforming-batches-with-tasks'],\n",
       "  'expected': 'https://docs.ray.io/en/master/data/transforming-data.html#configuring-batch-format',\n",
       "  'query': 'Iâ€™m struggling a bit with Ray Data type conversions when I do map_batches. Any advice?'},\n",
       " {'is_hit': True,\n",
       "  'score': 0.9194783238454012,\n",
       "  'retrieved': ['https://docs.ray.io/en/master/serve/architecture.html#ray-serve-autoscaling',\n",
       "   'https://docs.ray.io/en/master/cluster/key-concepts.html#autoscaling',\n",
       "   'https://docs.ray.io/en/master/cluster/vms/user-guides/configuring-autoscaling.html#configuring-autoscaling',\n",
       "   'https://docs.ray.io/en/master/serve/scaling-and-resource-allocation.html#autoscaling',\n",
       "   'https://docs.ray.io/en/master/cluster/kubernetes/user-guides/configuring-autoscaling.html#understanding-the-ray-autoscaler-in-the-context-of-kubernetes'],\n",
       "  'expected': 'https://docs.ray.io/en/master/serve/scaling-and-resource-allocation.html#autoscaling',\n",
       "  'query': 'How does autoscaling work in a Ray Serve application?'}]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1ddd39-f141-4585-93a4-593daa146f60",
   "metadata": {},
   "source": [
    "Let's see how well our retriever does. It's not great right now, but we now have a solid metric to evaluate our retriever for future optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "df7d03f6-76bf-414d-a072-2862bdc2c1a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4406779661016949"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_hits = sum(result[\"is_hit\"] for result in results)\n",
    "hit_percentage = total_hits / len(results)\n",
    "hit_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "069c754b-30d9-432a-98f0-1bcec9d0fe99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39427708148150287"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_score = sum(result[\"score\"] for result in results) / len(results)\n",
    "average_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e852dd79-247f-47d3-8ffa-97f79781a68a",
   "metadata": {},
   "source": [
    "## End-to-end evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6193ec2a-3a68-412b-bbb9-997e57b27edf",
   "metadata": {},
   "source": [
    "While we can evaluate our retriever in isolation, ultimately we want to evaluate our RAG application end-to-end, which includes the final response generated from our LLM.\n",
    "\n",
    "To effectively evaluate our generated responses, we need \"ground truth\" responses. These ground truth responses can be generated by feeding the correct context to a \"golden\" LLM. Then, we can use an LLM to evaluate our generated responses compared to the ground truth responses.\n",
    "\n",
    "<span style=\"background: yellow; color: red; font-size: 1rem;\"><b>DIAGRAM:</b></span> Show e2e evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da62c16b-605f-41cd-8894-b7f2e6ae460c",
   "metadata": {},
   "source": [
    "### Choosing a Golden LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0266f2-8a34-411e-a661-4c2fc8cbb5ef",
   "metadata": {},
   "source": [
    "To generate ground truth responses, and then to evaluate the generated responses vs. the ground truth, we need a \"golden\" LLM. But which LLM should we use? We now run into a problem: we need to determine the quality of different LLMs to choose as a \"golden\" LLM, but doing so requires a \"golden\" LLM. Leaderboards on general benchmarks provide a rough indication on which LLMs perform better, but in this case, we will go with the eye-test.\n",
    "\n",
    "Let's get responses from both GPT-4 and Llama2-70B and see for ourselves which one is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "75423140-d93e-495a-8a3a-f1cb3cbb72db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def fetch_text_from_source(source: str):\n",
    "    url, anchor = source.split(\"#\") if \"#\" in source else (source, None)\n",
    "    file_path = Path(\"/efs/shared_storage/amog/\", url.split(\"https://\")[-1])\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        html_content = file.read()\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    if anchor:\n",
    "        target_element = soup.find(id=anchor)\n",
    "        if target_element:\n",
    "            text = target_element.get_text()\n",
    "        else:\n",
    "            return fetch_text_from_source(source=url)\n",
    "    else:\n",
    "        text = soup.get_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "257763e2-d016-4ddb-8e5b-f06f7f520eba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://docs.ray.io/en/master/data/transforming-data.html#configuring-batch-format\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nConfiguring batch format#\\nRay Data represents batches as dicts of NumPy ndarrays or pandas DataFrames. By\\ndefault, Ray Data represents batches as dicts of NumPy ndarrays.\\nTo configure the batch type, specify batch_format in\\nmap_batches(). You can return either format from your function.\\n\\n\\n\\nNumPy\\nfrom typing import Dict\\nimport numpy as np\\nimport ray\\n\\ndef increase_brightness(batch: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\\n    batch[\"image\"] = np.clip(batch[\"image\"] + 4, 0, 255)\\n    return batch\\n\\nds = (\\n    ray.data.read_images(\"s3://[email\\xa0protected]/image-datasets/simple\")\\n    .map_batches(increase_brightness, batch_format=\"numpy\")\\n)\\n\\n\\n\\n\\n\\npandas\\nimport pandas as pd\\nimport ray\\n\\ndef drop_nas(batch: pd.DataFrame) -> pd.DataFrame:\\n    return batch.dropna()\\n\\nds = (\\n    ray.data.read_csv(\"s3://[email\\xa0protected]/iris.csv\")\\n    .map_batches(drop_nas, batch_format=\"pandas\")\\n)\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_source = data[0][\"source\"]\n",
    "print(example_source)\n",
    "\n",
    "text = fetch_text_from_source(example_source)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "1b223954-2d09-4013-a9e9-7473073c66c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Content for inference\n",
    "system_content = \"Answer the query using the context provided.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "2e8ffa65-6e0d-49d0-92c3-4ef73d1573f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def generate_response(llm_name, \n",
    "                      temperature, \n",
    "                      system_content, \n",
    "                      user_content):\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=llm_name,\n",
    "        temperature=temperature,\n",
    "        messages=[\n",
    "                {\"role\": \"system\", \"content\": system_content},\n",
    "                {\"role\": \"assistant\", \"content\": \"\"},\n",
    "                {\"role\": \"user\", \"content\": user_content},\n",
    "            ],)\n",
    "    return response[\"choices\"][-1][\"message\"][\"content\"]\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4747619-5678-4e8a-b59e-0576322ad727",
   "metadata": {},
   "source": [
    "Let's get responses from gpt-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "e4e97c5a-2ed5-4e91-964d-80d2f7663107",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt4_responses = []\n",
    "llm_name = \"gpt-4\"\n",
    "max_context_length = 8192\n",
    "\n",
    "openai.api_base = os.environ[\"OPENAI_API_BASE\"]\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "for entry in data[:5]:\n",
    "    query = entry[\"question\"]\n",
    "    source = entry[\"source\"]\n",
    "    context = fetch_text_from_source(source)\n",
    "    \n",
    "    context_length = max_context_length - len(system_content)\n",
    "    user_content = f\"The query is {query} and the additional context is {context}\"[:context_length]\n",
    "    \n",
    "    response = generate_response(llm_name, temperature=0.0, system_content=system_content, user_content=user_content)\n",
    "    gpt4_responses.append(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "0230ccc5-a2d2-475a-9ca7-cb594d99c9f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If you\\'re struggling with data type conversions in Ray Data when using map_batches, it\\'s important to remember that Ray Data represents batches as dicts of NumPy ndarrays or pandas DataFrames by default. \\n\\nIf you want to change the batch type, you need to specify the batch_format in map_batches(). The function you use with map_batches() should return the same format you specified. \\n\\nFor example, if you\\'re working with images and want to use NumPy ndarrays, your function should take a dict of ndarrays as input and return a dict of ndarrays. Here\\'s an example function that increases the brightness of an image:\\n\\n```python\\nfrom typing import Dict\\nimport numpy as np\\nimport ray\\n\\ndef increase_brightness(batch: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\\n    batch[\"image\"] = np.clip(batch[\"image\"] + 4, 0, 255)\\n    return batch\\n\\nds = (\\n    ray.data.read_images(\"s3://[email\\xa0protected]/image-datasets/simple\")\\n    .map_batches(increase_brightness, batch_format=\"numpy\")\\n)\\n```\\n\\nIf you\\'re working with CSV data and want to use pandas DataFrames, your function should take a DataFrame as input and return a DataFrame. Here\\'s an example function that drops NA values from a DataFrame:\\n\\n```python\\nimport pandas as pd\\nimport ray\\n\\ndef drop_nas(batch: pd.DataFrame) -> pd.DataFrame:\\n    return batch.dropna()\\n\\nds = (\\n    ray.data.read_csv(\"s3://[email\\xa0protected]/iris.csv\")\\n    .map_batches(drop_nas, batch_format=\"pandas\")\\n)\\n```\\n\\nRemember to match the batch_format with the type of data you\\'re working with and the format your function expects and returns.',\n",
       " 'Autoscaling in a Ray Serve application works by adjusting to traffic spikes through observing queue sizes and making scaling decisions to add or remove replicas. This is configured by setting the autoscaling_config field in deployment options. The configuration includes parameters like min_replicas, max_replicas, initial_replicas, and target_num_ongoing_requests_per_replica. \\n\\nThe min_replicas and max_replicas fields set the range of replicas that the Serve autoscaler can choose from. The initial_replicas field sets the number of replicas at the start of the deployment. The target_num_ongoing_requests_per_replica configuration determines how aggressively the autoscaler should react to traffic. \\n\\nThe Ray Serve Autoscaler is an application-level autoscaler that operates on top of the Ray Autoscaler. It requests Ray to start a number of replica actors based on the request demand. If there arenâ€™t enough available CPUs to place these actors, the Ray Autoscaler requests more Ray nodes from the underlying cloud provider. \\n\\nThe autoscaling_config parameters also include downscale_delay_s and upscale_delay_s which control the frequency of autoscaling work, smoothing_factor to speed up or slow down each autoscaling step, and metrics_interval_s which controls how often each replica sends metrics to the autoscaler.',\n",
       " \"To get the address of a Ray node, you can use the `ray.nodes()` function in Python after initializing Ray with `ray.init()`. This function returns a list of dictionaries, each representing a node in the cluster. The 'NodeManagerAddress' key in each dictionary provides the private IP address of the node. Here is an example:\\n\\n```python\\nimport ray\\n\\nray.init()\\nnodes = ray.nodes()\\n\\nfor node in nodes:\\n    print(node['NodeManagerAddress'])\\n```\\n\\nThis will print the addresses of all nodes in your Ray cluster.\",\n",
       " 'Yes, Ray does support NCCL. The Ray collective communication library uses NCCL and GLOO as the optional high-performance communication backends. It enables more efficient out-of-band collective communication between Ray actor and task processes, operating on both distributed CPUs and GPUs.',\n",
       " \"Yes, Ray is integrated with DeepSpeed. The integration of Ray AIR's TransformersTrainer allows for the configuration and use of Distributed Data Parallelism (DDP) and DeepSpeed. This is particularly useful for training large models like GPT-J, as DeepSpeed optimizes the training process and allows for offloading and partitioning of optimizer and parameter states, reducing GPU RAM usage. The DeepSpeed configuration can be specified in the TrainingArguments object.\"]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt4_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dee70c-4b84-435b-9dc9-463ab4ce5d5e",
   "metadata": {},
   "source": [
    "Now let's get responses from LLama2-70b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "c3433a5b-cbcb-44bc-8049-36c847bba3c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_responses = []\n",
    "llm_name = \"meta-llama/Llama-2-70b-chat-hf\"\n",
    "max_context_length = 4096\n",
    "\n",
    "openai.api_base = os.environ[\"ANYSCALE_API_BASE\"]\n",
    "openai.api_key = os.environ[\"ANYSCALE_API_KEY\"]\n",
    "\n",
    "for entry in data[:5]:\n",
    "    query = entry[\"question\"]\n",
    "    source = entry[\"source\"]\n",
    "    context = fetch_text_from_source(source)\n",
    "    \n",
    "    context_length = max_context_length - len(system_content)\n",
    "    user_content = f\"The query is {query} and the additional context is {context}\"[:context_length]\n",
    "    \n",
    "    response = generate_response(llm_name, temperature=0.0, system_content=system_content, user_content=user_content)\n",
    "    llama_responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "b39e4fd2-c6e7-4ba8-837f-543b1087d00d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sure, I can help you with that! It looks like you\\'re struggling with converting data types when using Ray\\'s `map_batches` function. Specifically, you\\'re having trouble with the `batch_format` parameter, which can be set to either \"numpy\" or \"pandas\" depending on the type of data you\\'re working with.\\n\\nHere\\'s a breakdown of the issue and some potential solutions:\\n\\n1. When working with NumPy arrays, you don\\'t need to specify the `batch_format` parameter. Ray will automatically infer the type of data and use the appropriate format.\\n2. When working with pandas DataFrames, you need to specify `batch_format=\"pandas\"` in the `map_batches` function. This tells Ray to expect pandas DataFrames as input and convert them to the appropriate format for processing.\\n3. If you\\'re working with a mix of data types (e.g., some batches are NumPy arrays, while others are pandas DataFrames), you can use the `batch_format` parameter to specify the format of each batch individually. For example:\\n```python\\nds = (\\n    ray.data.read_images(\"s3://[email protected]/image-datasets/simple\")\\n    .map_batches(increase_brightness, batch_format=\"numpy\")\\n    .concat(\\n        ray.data.read_csv(\"s3://[email protected]/iris.csv\")\\n        .map_batches(drop_nas, batch_format=\"pandas\")\\n    )\\n)\\n```\\nIn this example, the first batch is a NumPy array, so we set `batch_format=\"numpy\"`. The second batch is a pandas DataFrame, so we set `batch_format=\"pandas\"`. The `concat` function will automatically handle the type conversion and concatenate the two datasets correctly.\\n\\nI hope this helps! Let me know if you have any further questions or concerns.',\n",
       " \"Sure! Here's an answer to the query based on the provided context:\\n\\nAutoscaling in Ray Serve works by adjusting the number of replicas of a deployment based on the demand of incoming requests. The autoscaler observes the queue sizes and makes scaling decisions to add or remove replicas accordingly. The autoscaling configuration can be set using the `autoscaling_config` field in the deployment options.\\n\\nThe `min_replicas` field specifies the minimum number of replicas for the deployment, and it will also be the initial number of replicas when the deployment is deployed. The `max_replicas` field specifies the maximum number of replicas for the deployment, and Ray Serve Autoscaling will rely on the Ray Autoscaler to scale up more nodes when the currently available cluster resources are not enough to support more replicas.\\n\\nThe `target_num_ongoing_requests_per_replica` field specifies how many ongoing requests are expected to run concurrently per replica. The autoscaler scales up if the value is lower than the current number of ongoing requests per replica, and it scales down if itâ€™s higher than the current number of ongoing requests. Scaling happens quicker if thereâ€™s a high disparity between this value and the current number of ongoing requests.\\n\\nInternally, the autoscaler decides to scale up or down by comparing `target_num_ongoing_requests_per_replica` to the number of RUNNING and PENDING tasks on each replica. The autoscaler will try to make sure that each replica has roughly the target number of requests being processed and waiting in the queue.\\n\\nIt's recommended to load test your workloads to determine the appropriate value for `target_num_ongoing_requests_per_replica` based on your end-to-end latency objective. Lowering this value can maintain high performance for latency-sensitive use cases.\",\n",
       " \"To get the address of a ray node, you can use the `ray.nodes()` method and access the `NodeManagerAddress` field of the node object.\\n\\nHere's an example:\\n```\\nray.init()\\nnodes = ray.nodes()\\nfor node in nodes:\\n    print(node['NodeManagerAddress'])\\n```\\nThis will print the private IP address of each ray node in the cluster.\\n\\nAlternatively, you can also access the `NodeManagerHostname` field to get the hostname of the node, and then use a library like `socket` to resolve the hostname to an IP address.\\n```\\nray.init()\\nnodes = ray.nodes()\\nfor node in nodes:\\n    hostname = node['NodeManagerHostname']\\n    address = socket.gethostbyname(hostname)\\n    print(address)\\n```\\nKeep in mind that the `NodeManagerAddress` field is only available if the ray node is running on a private IP address. If the ray node is running on a public IP address, the `NodeManagerAddress` field will be empty, and you'll need to use the `NodeManagerHostname` field to resolve the hostname to an IP address.\",\n",
       " 'Yes, Ray supports NCCL. The Ray Collective Communication Lib documentation states that NCCL is used as an optional high-performance communication backend for collective communication between Ray actor and task processes. The documentation also provides a support matrix that shows which collective primitives are supported by NCCL and which are not. Additionally, the documentation explains how to install and import the necessary packages to use NCCL with Ray.',\n",
       " \"Yes, Ray is integrated with DeepSpeed. The integration allows you to easily configure and use Distributed Data Parallelism (DDP) and DeepSpeed's optimization features, such as offloading and partitioning optimizer and parameter states, to reduce GPU memory usage.\\n\\nIn the provided code snippet, the `TrainingArguments` object is used to specify the DeepSpeed configuration. The `Trainer` class from Transformers is then used to create a distributed trainer that uses DDP and DeepSpeed to train the model. The `trainer_init_per_worker` function is used to create a separate trainer instance for each worker, which will operate on a different shard of the data.\\n\\nThe effective batch size is calculated as the per-device batch size multiplied by the number of workers and the number of gradient accumulation steps. By increasing the number of workers, the effective batch size increases, which can lead to faster training times. However, the speedup is not always linear due to extra communication overheads.\\n\\nIn the example provided, the effective batch size was 256 with 16 g4dn.4xlarge nodes, which took approximately 2440 seconds (including initialization time) to complete one epoch. With 32 g4dn.4xlarge nodes, the effective batch size was 512, which took approximately 1280 seconds (including initialization time) to complete one epoch.\"]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df65c767-a3f5-4987-a7e6-58067b16c351",
   "metadata": {},
   "source": [
    "Now let's compare the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "1d441a97-f5c6-451c-9852-4cf13328a466",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mQuery:\u001b[0m Iâ€™m struggling a bit with Ray Data type conversions when I do map_batches. Any advice?\n",
      "\u001b[1mGPT4 answer:\u001b[0m If you're struggling with data type conversions in Ray Data when using map_batches, it's important to remember that Ray Data represents batches as dicts of NumPy ndarrays or pandas DataFrames by default. \n",
      "\n",
      "If you want to change the batch type, you need to specify the batch_format in map_batches(). The function you use with map_batches() should return the same format you specified. \n",
      "\n",
      "For example, if you're working with images and want to use NumPy ndarrays, your function should take a dict of ndarrays as input and return a dict of ndarrays. Here's an example function that increases the brightness of an image:\n",
      "\n",
      "```python\n",
      "from typing import Dict\n",
      "import numpy as np\n",
      "import ray\n",
      "\n",
      "def increase_brightness(batch: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
      "    batch[\"image\"] = np.clip(batch[\"image\"] + 4, 0, 255)\n",
      "    return batch\n",
      "\n",
      "ds = (\n",
      "    ray.data.read_images(\"s3://[emailÂ protected]/image-datasets/simple\")\n",
      "    .map_batches(increase_brightness, batch_format=\"numpy\")\n",
      ")\n",
      "```\n",
      "\n",
      "If you're working with CSV data and want to use pandas DataFrames, your function should take a DataFrame as input and return a DataFrame. Here's an example function that drops NA values from a DataFrame:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import ray\n",
      "\n",
      "def drop_nas(batch: pd.DataFrame) -> pd.DataFrame:\n",
      "    return batch.dropna()\n",
      "\n",
      "ds = (\n",
      "    ray.data.read_csv(\"s3://[emailÂ protected]/iris.csv\")\n",
      "    .map_batches(drop_nas, batch_format=\"pandas\")\n",
      ")\n",
      "```\n",
      "\n",
      "Remember to match the batch_format with the type of data you're working with and the format your function expects and returns.\n",
      "\u001b[1mLlama2-70B answer:\u001b[0m Sure, I can help you with that! It looks like you're struggling with converting data types when using Ray's `map_batches` function. Specifically, you're having trouble with the `batch_format` parameter, which can be set to either \"numpy\" or \"pandas\" depending on the type of data you're working with.\n",
      "\n",
      "Here's a breakdown of the issue and some potential solutions:\n",
      "\n",
      "1. When working with NumPy arrays, you don't need to specify the `batch_format` parameter. Ray will automatically infer the type of data and use the appropriate format.\n",
      "2. When working with pandas DataFrames, you need to specify `batch_format=\"pandas\"` in the `map_batches` function. This tells Ray to expect pandas DataFrames as input and convert them to the appropriate format for processing.\n",
      "3. If you're working with a mix of data types (e.g., some batches are NumPy arrays, while others are pandas DataFrames), you can use the `batch_format` parameter to specify the format of each batch individually. For example:\n",
      "```python\n",
      "ds = (\n",
      "    ray.data.read_images(\"s3://[email protected]/image-datasets/simple\")\n",
      "    .map_batches(increase_brightness, batch_format=\"numpy\")\n",
      "    .concat(\n",
      "        ray.data.read_csv(\"s3://[email protected]/iris.csv\")\n",
      "        .map_batches(drop_nas, batch_format=\"pandas\")\n",
      "    )\n",
      ")\n",
      "```\n",
      "In this example, the first batch is a NumPy array, so we set `batch_format=\"numpy\"`. The second batch is a pandas DataFrame, so we set `batch_format=\"pandas\"`. The `concat` function will automatically handle the type conversion and concatenate the two datasets correctly.\n",
      "\n",
      "I hope this helps! Let me know if you have any further questions or concerns.\n",
      "\n",
      "\n",
      "\u001b[1mQuery:\u001b[0m How does autoscaling work in a Ray Serve application?\n",
      "\u001b[1mGPT4 answer:\u001b[0m Autoscaling in a Ray Serve application works by adjusting to traffic spikes through observing queue sizes and making scaling decisions to add or remove replicas. This is configured by setting the autoscaling_config field in deployment options. The configuration includes parameters like min_replicas, max_replicas, initial_replicas, and target_num_ongoing_requests_per_replica. \n",
      "\n",
      "The min_replicas and max_replicas fields set the range of replicas that the Serve autoscaler can choose from. The initial_replicas field sets the number of replicas at the start of the deployment. The target_num_ongoing_requests_per_replica configuration determines how aggressively the autoscaler should react to traffic. \n",
      "\n",
      "The Ray Serve Autoscaler is an application-level autoscaler that operates on top of the Ray Autoscaler. It requests Ray to start a number of replica actors based on the request demand. If there arenâ€™t enough available CPUs to place these actors, the Ray Autoscaler requests more Ray nodes from the underlying cloud provider. \n",
      "\n",
      "The autoscaling_config parameters also include downscale_delay_s and upscale_delay_s which control the frequency of autoscaling work, smoothing_factor to speed up or slow down each autoscaling step, and metrics_interval_s which controls how often each replica sends metrics to the autoscaler.\n",
      "\u001b[1mLlama2-70B answer:\u001b[0m Sure! Here's an answer to the query based on the provided context:\n",
      "\n",
      "Autoscaling in Ray Serve works by adjusting the number of replicas of a deployment based on the demand of incoming requests. The autoscaler observes the queue sizes and makes scaling decisions to add or remove replicas accordingly. The autoscaling configuration can be set using the `autoscaling_config` field in the deployment options.\n",
      "\n",
      "The `min_replicas` field specifies the minimum number of replicas for the deployment, and it will also be the initial number of replicas when the deployment is deployed. The `max_replicas` field specifies the maximum number of replicas for the deployment, and Ray Serve Autoscaling will rely on the Ray Autoscaler to scale up more nodes when the currently available cluster resources are not enough to support more replicas.\n",
      "\n",
      "The `target_num_ongoing_requests_per_replica` field specifies how many ongoing requests are expected to run concurrently per replica. The autoscaler scales up if the value is lower than the current number of ongoing requests per replica, and it scales down if itâ€™s higher than the current number of ongoing requests. Scaling happens quicker if thereâ€™s a high disparity between this value and the current number of ongoing requests.\n",
      "\n",
      "Internally, the autoscaler decides to scale up or down by comparing `target_num_ongoing_requests_per_replica` to the number of RUNNING and PENDING tasks on each replica. The autoscaler will try to make sure that each replica has roughly the target number of requests being processed and waiting in the queue.\n",
      "\n",
      "It's recommended to load test your workloads to determine the appropriate value for `target_num_ongoing_requests_per_replica` based on your end-to-end latency objective. Lowering this value can maintain high performance for latency-sensitive use cases.\n",
      "\n",
      "\n",
      "\u001b[1mQuery:\u001b[0m how do I get the address of a ray node\n",
      "\u001b[1mGPT4 answer:\u001b[0m To get the address of a Ray node, you can use the `ray.nodes()` function in Python after initializing Ray with `ray.init()`. This function returns a list of dictionaries, each representing a node in the cluster. The 'NodeManagerAddress' key in each dictionary provides the private IP address of the node. Here is an example:\n",
      "\n",
      "```python\n",
      "import ray\n",
      "\n",
      "ray.init()\n",
      "nodes = ray.nodes()\n",
      "\n",
      "for node in nodes:\n",
      "    print(node['NodeManagerAddress'])\n",
      "```\n",
      "\n",
      "This will print the addresses of all nodes in your Ray cluster.\n",
      "\u001b[1mLlama2-70B answer:\u001b[0m To get the address of a ray node, you can use the `ray.nodes()` method and access the `NodeManagerAddress` field of the node object.\n",
      "\n",
      "Here's an example:\n",
      "```\n",
      "ray.init()\n",
      "nodes = ray.nodes()\n",
      "for node in nodes:\n",
      "    print(node['NodeManagerAddress'])\n",
      "```\n",
      "This will print the private IP address of each ray node in the cluster.\n",
      "\n",
      "Alternatively, you can also access the `NodeManagerHostname` field to get the hostname of the node, and then use a library like `socket` to resolve the hostname to an IP address.\n",
      "```\n",
      "ray.init()\n",
      "nodes = ray.nodes()\n",
      "for node in nodes:\n",
      "    hostname = node['NodeManagerHostname']\n",
      "    address = socket.gethostbyname(hostname)\n",
      "    print(address)\n",
      "```\n",
      "Keep in mind that the `NodeManagerAddress` field is only available if the ray node is running on a private IP address. If the ray node is running on a public IP address, the `NodeManagerAddress` field will be empty, and you'll need to use the `NodeManagerHostname` field to resolve the hostname to an IP address.\n",
      "\n",
      "\n",
      "\u001b[1mQuery:\u001b[0m Does Ray support NCCL?\n",
      "\u001b[1mGPT4 answer:\u001b[0m Yes, Ray does support NCCL. The Ray collective communication library uses NCCL and GLOO as the optional high-performance communication backends. It enables more efficient out-of-band collective communication between Ray actor and task processes, operating on both distributed CPUs and GPUs.\n",
      "\u001b[1mLlama2-70B answer:\u001b[0m Yes, Ray supports NCCL. The Ray Collective Communication Lib documentation states that NCCL is used as an optional high-performance communication backend for collective communication between Ray actor and task processes. The documentation also provides a support matrix that shows which collective primitives are supported by NCCL and which are not. Additionally, the documentation explains how to install and import the necessary packages to use NCCL with Ray.\n",
      "\n",
      "\n",
      "\u001b[1mQuery:\u001b[0m Is Ray integrated with DeepSpeed?\n",
      "\u001b[1mGPT4 answer:\u001b[0m Yes, Ray is integrated with DeepSpeed. The integration of Ray AIR's TransformersTrainer allows for the configuration and use of Distributed Data Parallelism (DDP) and DeepSpeed. This is particularly useful for training large models like GPT-J, as DeepSpeed optimizes the training process and allows for offloading and partitioning of optimizer and parameter states, reducing GPU RAM usage. The DeepSpeed configuration can be specified in the TrainingArguments object.\n",
      "\u001b[1mLlama2-70B answer:\u001b[0m Yes, Ray is integrated with DeepSpeed. The integration allows you to easily configure and use Distributed Data Parallelism (DDP) and DeepSpeed's optimization features, such as offloading and partitioning optimizer and parameter states, to reduce GPU memory usage.\n",
      "\n",
      "In the provided code snippet, the `TrainingArguments` object is used to specify the DeepSpeed configuration. The `Trainer` class from Transformers is then used to create a distributed trainer that uses DDP and DeepSpeed to train the model. The `trainer_init_per_worker` function is used to create a separate trainer instance for each worker, which will operate on a different shard of the data.\n",
      "\n",
      "The effective batch size is calculated as the per-device batch size multiplied by the number of workers and the number of gradient accumulation steps. By increasing the number of workers, the effective batch size increases, which can lead to faster training times. However, the speedup is not always linear due to extra communication overheads.\n",
      "\n",
      "In the example provided, the effective batch size was 256 with 16 g4dn.4xlarge nodes, which took approximately 2440 seconds (including initialization time) to complete one epoch. With 32 g4dn.4xlarge nodes, the effective batch size was 512, which took approximately 1280 seconds (including initialization time) to complete one epoch.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BOLD = '\\033[1m'\n",
    "END = '\\033[0m'\n",
    "    \n",
    "for query, gpt_response, llama_response in zip(data[:5], gpt4_responses, llama_responses):\n",
    "    print(f\"{BOLD}Query:{END} {query['question']}\")\n",
    "    print(f\"{BOLD}GPT4 answer:{END} {gpt_response}\")\n",
    "    print(f\"{BOLD}Llama2-70B answer:{END} {llama_response}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e3fc5d-13bb-4314-b148-ab4be5b98dcb",
   "metadata": {},
   "source": [
    "Based on these answers, we go with GPT-4 as our \"golden\" LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a6cfa4-8f59-43e7-a694-843b9613b1b3",
   "metadata": {},
   "source": [
    "### Generating our Golden Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f5c2a6-a5ac-4b9c-ab45-61d444b97b9b",
   "metadata": {},
   "source": [
    "Now that we have chosen which LLM to use, we can generate our reference responses. Let's generate 10 reference responses and save them to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "5981d038-4b6c-4a97-8e05-ccf9121ade7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "golden_responses = []\n",
    "llm_name = \"gpt-4\"\n",
    "max_context_length = 8192\n",
    "\n",
    "openai.api_base = os.environ[\"OPENAI_API_BASE\"]\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "ten_samples = data[:10]\n",
    "\n",
    "for entry in ten_samples:\n",
    "    query = entry[\"question\"]\n",
    "    source = entry[\"source\"]\n",
    "    context = fetch_text_from_source(source)\n",
    "    \n",
    "    context_length = max_context_length - len(system_content)\n",
    "    user_content = f\"The query is {query} and the additional context is {context}\"[:context_length]\n",
    "    \n",
    "    response = generate_response(llm_name, temperature=0.0, system_content=system_content, user_content=user_content)\n",
    "    golden_responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "5fc8506c-0deb-43ae-9936-cd323635cc44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reference_dataset = [{\"question\": entry[\"question\"], \"source\": entry[\"source\"], \"response\": response} for entry, response in zip(ten_samples, golden_responses)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "23632aab-2be3-49cb-905b-1b26d7bde46a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"golden-responses.json\", \"w\") as file:\n",
    "    json.dump(reference_dataset, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dbf6c6-74ea-4802-be16-f96f2982a642",
   "metadata": {},
   "source": [
    "## Evaluating our Query Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef0af94-f983-464f-8a4b-a91ff72d7f87",
   "metadata": {},
   "source": [
    "Once we have reference responses, we can get our generated responses from our query engine. Then pass both responses to our golden LLM to evaluate the responses from our application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "cd15459b-a679-4f1b-832c-bb38cd467d0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'Iâ€™m struggling a bit with Ray Data type conversions when I do map_batches. Any advice?',\n",
       "  'source': 'https://docs.ray.io/en/master/data/transforming-data.html#configuring-batch-format',\n",
       "  'response': \"If you're struggling with data type conversions in Ray Data when using map_batches, it's important to remember that Ray Data represents batches as either dicts of NumPy ndarrays or pandas DataFrames. The type of batch format you want to use can be specified in the map_batches() function using the batch_format parameter.\\n\\nIf you're working with NumPy, your function should take in and return a Dict[str, np.ndarray]. For example, in the increase_brightness function, the batch of images is increased in brightness and clipped to ensure values stay within the valid range for images.\\n\\nIf you're working with pandas, your function should take in and return a pd.DataFrame. For example, in the drop_nas function, any rows with NA values are dropped from the DataFrame.\\n\\nMake sure your function is correctly handling the data type you've specified in batch_format. If you're still having issues, the problem might be with the function you're passing to map_batches, not with the data type conversion itself.\"},\n",
       " {'question': 'How does autoscaling work in a Ray Serve application?',\n",
       "  'source': 'https://docs.ray.io/en/master/serve/scaling-and-resource-allocation.html#autoscaling',\n",
       "  'response': 'Autoscaling in a Ray Serve application works by adjusting to traffic spikes through observing queue sizes and making scaling decisions to add or remove replicas. This is configured by setting the autoscaling_config field in deployment options. The configuration includes parameters like min_replicas, max_replicas, initial_replicas, and target_num_ongoing_requests_per_replica. \\n\\nThe min_replicas and max_replicas fields set the range of replicas that the Serve autoscaler can choose from. The initial_replicas field sets the number of replicas at the start of the deployment. The target_num_ongoing_requests_per_replica configuration specifies how aggressively the autoscaler should react to traffic. \\n\\nThe Ray Serve Autoscaler is an application-level autoscaler that sits on top of the Ray Autoscaler. It asks Ray to start a number of replica actors based on the request demand. If the Ray Autoscaler determines there arenâ€™t enough available CPUs to place these actors, it requests more Ray nodes from the underlying cloud provider. \\n\\nThe autoscaling algorithm takes into consideration several user-specified parameters when deciding the target number of replicas for your deployment. These include min_replicas, max_replicas, target_num_ongoing_requests_per_replica, downscale_delay_s, upscale_delay_s, smoothing_factor, and metrics_interval_s.'},\n",
       " {'question': 'how do I get the address of a ray node',\n",
       "  'source': 'https://docs.ray.io/en/master/ray-core/miscellaneous.html#node-information',\n",
       "  'response': \"To get the address of a Ray node, you can use the `ray.nodes()` function which returns a list of the nodes in the cluster. This function provides a dictionary for each node, which includes the 'NodeManagerAddress' key. The value of this key is the private IP address of the node that the raylet is on. Here is an example of how to use it:\\n\\n```python\\nimport ray\\n\\nray.init()\\nnodes = ray.nodes()\\n\\nfor node in nodes:\\n    print(node['NodeManagerAddress'])\\n```\\n\\nThis will print out the addresses of all the nodes in your Ray cluster.\"},\n",
       " {'question': 'Does Ray support NCCL?',\n",
       "  'source': 'https://docs.ray.io/en/master/ray-more-libs/ray-collective.html',\n",
       "  'response': 'Yes, Ray does support NCCL. The Ray collective communication library uses NCCL and GLOO as the optional high-performance communication backends. It enables more efficient out-of-band collective communication between Ray actor and task processes, operating on both distributed CPUs and GPUs.'},\n",
       " {'question': 'Is Ray integrated with DeepSpeed?',\n",
       "  'source': 'https://docs.ray.io/en/master/ray-air/examples/gptj_deepspeed_fine_tuning.html#fine-tuning-the-model-with-ray-air-a-name-train-a',\n",
       "  'response': \"Yes, Ray is integrated with DeepSpeed. The integration of Ray AIR's TransformersTrainer allows for the configuration and use of Distributed Data Parallelism (DDP) and DeepSpeed. This is particularly useful for training large models like GPT-J, as DeepSpeed optimizes the training process and allows for offloading and partitioning of optimizer and parameter states, reducing GPU RAM usage.\"},\n",
       " {'question': \"what will happen if I use AsyncIO's await to wait for a Ray future like `await x.remote()`\",\n",
       "  'source': 'https://docs.ray.io/en/master/ray-core/actors/async_api.html#objectrefs-as-asyncio-futures',\n",
       "  'response': \"If you use AsyncIO's await to wait for a Ray future like `await x.remote()`, it will allow you to asynchronously wait for the result of the remote task. This is possible because Ray's ObjectRefs can be translated to asyncio.Futures. This feature is useful in concurrent applications where you want to wait for multiple tasks to complete. You can also directly access the future object by calling `asyncio.wrap_future(ref.future())`.\"},\n",
       " {'question': 'How would you compare Spark, Ray, Dask?',\n",
       "  'source': 'https://docs.ray.io/en/master/data/overview.html#how-does-ray-data-compare-to-x-for-offline-inference',\n",
       "  'response': \"Spark, Ray, and Dask are all distributed data processing frameworks, but they have different strengths and use cases.\\n\\nSpark is a mature and widely-used framework that excels in batch processing workloads. It has a rich ecosystem and a variety of built-in libraries for machine learning, graph processing, and stream processing. However, it may not be as well-suited for GPU workloads for deep learning inference.\\n\\nRay, specifically Ray Data, is designed for both infrastructure management and data processing. It abstracts away complexities such as sharding your dataset, parallelizing the inference over these shards, and transferring data from storage to CPU to GPU. It is built for offline batch jobs and handles many of the same batch processing workloads as Spark, but with a streaming paradigm that is better suited for GPU workloads for deep learning inference.\\n\\nDask is not mentioned in the context provided, so a comparison cannot be made based on the given information. However, in general, Dask is known for its ability to handle larger-than-memory computations and its integration with Python's scientific software stack. It is flexible and can be used for a variety of workloads, including machine learning, and it can scale from single machines to large clusters.\"},\n",
       " {'question': 'why would ray overload a node w/ more task that the resources allow ?',\n",
       "  'source': 'https://docs.ray.io/en/master/ray-core/scheduling/resources.html#physical-resources-and-logical-resources',\n",
       "  'response': \"Ray might overload a node with more tasks than the resources allow because Ray's resources are logical and do not impose limits on actual physical resource usage. This means that a task with a num_cpus=1 requirement can still launch multiple threads and use multiple physical CPUs. Ray relies on the user to ensure that tasks or actors do not use more resources than specified. It does not provide CPU isolation for tasks or actors, and instead lets the operating system schedule and run the task. However, Ray does provide GPU isolation by setting the CUDA_VISIBLE_DEVICES environment variable.\"},\n",
       " {'question': 'when should I use Ray Client?',\n",
       "  'source': 'https://docs.ray.io/en/master/cluster/running-applications/job-submission/ray-client.html#when-to-use-ray-client',\n",
       "  'response': 'You should use Ray Client when you want to connect an interactive Python shell to a remote cluster. It is useful for developing interactively in a local Python shell. However, it requires a stable connection to the remote cluster and will terminate the workload if the connection is lost for more than 30 seconds. For long running workloads that you want to run on your cluster, it is recommended to use Ray Jobs instead. Note that Ray Client may not work as expected for ML workloads like Ray Tune or Ray Train, in which case you should use Ray Jobs API for interactive development on ML projects.'},\n",
       " {'question': 'how to scatter actors across the cluster?',\n",
       "  'source': 'https://docs.ray.io/en/master/ray-core/scheduling/index.html#spread',\n",
       "  'response': 'To scatter actors across the cluster, you can use the \"SPREAD\" scheduling strategy in Ray. This strategy will distribute the tasks or actors among the available nodes. Here is how you can do it:\\n\\n1. Define a function or an actor with the \"@ray.remote\" decorator and specify the scheduling strategy as \"SPREAD\". For example:\\n\\n```python\\n@ray.remote(scheduling_strategy=\"SPREAD\")\\ndef spread_func():\\n    return 2\\n```\\n\\nor \\n\\n```python\\n@ray.remote(num_cpus=1)\\nclass SpreadActor:\\n    pass\\n```\\n\\n2. Then, you can spread tasks or actors across the cluster by calling the remote function or creating instances of the remote actor. For example:\\n\\n```python\\n# Spread tasks across the cluster.\\n[spread_func.remote() for _ in range(10)]\\n```\\n\\nor \\n\\n```python\\n# Spread actors across the cluster.\\nactors = [SpreadActor.options(scheduling_strategy=\"SPREAD\").remote() for _ in range(10)]\\n```\\n\\nThis will create 10 tasks or actors and distribute them across the available nodes in the cluster.'}]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"golden-responses.json\", \"r\") as file:\n",
    "    golden_responses = json.load(file)\n",
    "golden_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "b8a53891-cd44-4efb-b9f9-9e3cc7de3a6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import get_query_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "3adebada-bce2-4600-ac2f-4ba541ce32f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "openai.api_base = os.environ[\"ANYSCALE_API_BASE\"]\n",
    "openai.api_key = os.environ[\"ANYSCALE_API_KEY\"]\n",
    "query_engine = get_query_engine(similarity_top_k=5)\n",
    "\n",
    "# Store both the original response object and the response string.\n",
    "rag_responses = []\n",
    "rag_response_str = []\n",
    "\n",
    "for entry in golden_responses:\n",
    "    query = entry[\"question\"]\n",
    "    response = query_engine.query(query)\n",
    "    rag_responses.append(response)\n",
    "    rag_response_str.append(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "a17e76f6-7333-4ea7-9357-3bc125c59cf8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It seems like you\\'re encountering issues with type conversions when using `map_batches` with Ray Data. Here are some tips that may help:\\n\\n1. Use the `batch_format` argument: When calling `map_batches`, you can specify the `batch_format` argument to indicate the format of the batches. If you\\'re working with NumPy arrays, set `batch_format=\"numpy\"`. If you\\'re working with PyTorch tensors, set `batch_format=\"torch\"`. This can help Ray Data handle the type conversions correctly.\\n2. Return the correct type: When writing a transformation function for `map_batches`, make sure to return a batch in the correct format. If you\\'re working with NumPy arrays, return a dictionary of NumPy arrays. If you\\'re working with PyTorch tensors, return a dictionary of PyTorch tensors.\\n3. Avoid returning lists: When using `map_batches`, avoid returning lists of arrays or tensors. Instead, return a dictionary of arrays or tensors, where each key corresponds to a batch dimension. This can help Ray Data handle the type conversions correctly.\\n4',\n",
       " 'Autoscaling in a Ray Serve application works by periodically checking ServeHandle queues and in-flight queries on replicas to decide whether or not to scale the number of replicas. Each ServeHandle continuously polls the controller to check for new deployment replicas. Whenever new replicas are discovered, it will send any buffered or new queries to the replica until max_concurrent_queries is reached. Queries are sent to replicas in round-robin fashion, subject to the constraint that no replica is handling more than max_concurrent_queries requests at a time.\\n\\nThe Ray Serve autoscaler asks Ray to start a number of replica actors based on the request demand. If the Ray Autoscaler determines there arenâ€™t enough available CPUs to place these actors, it responds by requesting more Ray nodes. The underlying cloud provider will then respond by adding more nodes. Similarly, when Ray Serve scales down and terminates some replica actors, it will try to do so in a way that results in the most nodes having no Ray actors or tasks running on them, at which point the Ray autoscaler will remove those',\n",
       " \"You can get the address of a Ray node by using the `ray.nodes` API to fetch all nodes and map the node ID to the corresponding IP. Alternatively, you can use the `ray.init` method to specify the address of the Ray cluster, or you can use the `RAY_ADDRESS` environment variable to set the address of the Ray dashboard.\\n\\nHere's an example of how you can use the `ray.nodes` API to get the address of a Ray node:\\n```\\nimport ray\\n\\n# Initialize the Ray cluster\\nray.init()\\n\\n# Get the list of nodes in the Ray cluster\\nnodes = ray.nodes()\\n\\n# Print the address of the first node\\nprint(nodes[0].address)\\n```\\nThis will print the address of the first node in the Ray cluster. You can also use the `ray.nodes` API to filter the list of nodes based on certain criteria, such as the node's IP address or the node's role in the cluster.\\n\\nAlternatively, you can use the `ray.init` method to specify the address of the Ray cluster. For example:\\n```\\nimport ray\\n\\n# Initialize the Ray cluster\",\n",
       " 'Yes, Ray supports NCCL as an optional high-performance communication backend for its collective communication library.',\n",
       " \"No, Ray doesn't provide native integration with DeepSpeed. However, PyTorch Lightning offers a DeepSpeed integration that provides a simple interface to configure the knobs for DeepSpeed and automatically trigger the training process with the DeepSpeed Engine. Ray LightningTrainer allows scaling PyTorch Lightning jobs across multiple nodes in a Ray cluster, without worrying about the underlying cluster management, autoscaling, and distributed process group settings.\",\n",
       " \"Based on the provided context information, it seems that you are asking about using AsyncIO's `await` keyword to wait for a Ray future object, specifically `x.remote()`.\\n\\nAccording to the context, Ray provides an async API that allows you to use asyncio to write concurrent code. However, the context also mentions that Ray does not support asyncio for remote tasks, which means that using `await` with a Ray future object like `x.remote()` will not work as expected.\\n\\nInstead, you can wrap the async function with a wrapper to run the task synchronously, as shown in the example provided in the context. This will allow you to use `await` with the wrapped function, which will in turn allow you to wait for the task to complete.\\n\\nAlternatively, you can use Ray's `wait` method to wait for the future object to complete, as shown in the first code snippet provided in the context. This method will block the execution of the code until the future object is resolved, allowing you to write synchronous code that waits for the result of the remote task.\\n\\nIn summary, using `await` with a Ray future object like `x.remote()` will\",\n",
       " 'Spark, Ray, and Dask are all powerful tools for data processing and machine learning. While they share some similarities, they also have some key differences.\\n\\nSpark is a widely-used open-source data processing engine that can handle large-scale data processing tasks. It has a wide range of libraries, including Spark SQL, Spark DataFrame, and Spark MLlib, which make it easy to perform data processing, data transformation, and machine learning tasks. Spark is designed to be fast, scalable, and fault-tolerant, making it a popular choice for big data applications.\\n\\nRay is a distributed computing framework that is designed to handle complex, large-scale data processing tasks. It provides a simple, Python-based API that allows developers to write code that can be executed on a distributed cluster. Ray is built on top of Apache Spark and provides additional features such as automatic task retries, stateful streaming, and support for distributed machine learning.\\n\\nDask is a parallel computing library that provides big data collections that mimic the APIs of familiar libraries like NumPy and Pandas. It allows developers to write code that can be executed on a multi-machine cluster, providing automatic data parallelism, smart scheduling,',\n",
       " 'Ray would overload a node with more tasks than the resources allow because the scheduler limits task concurrency to the available CPUs and actor concurrency to infinite. Tasks that use more than 1 CPU (e.g., via multithreading) may experience slowdown due to interference from concurrent ones, but otherwise are safe to run. However, tasks or actors that use more than their proportionate share of memory may overload a node and cause issues like OOM.',\n",
       " 'Ray Client should be used when you want to connect an interactive Python shell to a remote cluster. It is useful for developing interactively in a local Python shell, but it requires a stable connection to the remote cluster and will terminate the workload if the connection is lost for more than 30 seconds. If you have a long-running workload that you want to run on your cluster, it is recommended to use Ray Jobs instead. Additionally, Ray Client has architectural limitations and may not work as expected when using Ray for ML workloads (like Ray Tune or Ray Train). In such cases, it is recommended to use Ray Jobs API for interactive development on ML projects.',\n",
       " 'You can use the \"SPREAD\" scheduling strategy in Ray to scatter actors across the cluster. The \"SPREAD\" strategy will try to spread the tasks or actors among available nodes. You can specify the scheduling strategy when creating a remote function or actor, like this:\\n\\n@ray.remote(scheduling_strategy=\"SPREAD\")\\ndef my_actor():\\n    pass\\n\\nOr\\n\\n@ray.remote(num_cpus=1, scheduling_strategy=\"SPREAD\")\\nclass MyActor:\\n    pass\\n\\nThis will tell Ray to schedule the actor on a random node in the cluster. You can also use the \"SPREAD\" strategy in combination with the \"max_concurrency\" parameter to control the maximum number of actors that can run concurrently on a single node. For example:\\n\\n@ray.remote(num_cpus=1, max_concurrency=4, scheduling_strategy=\"SPREAD\")\\nclass MyActor:\\n    pass\\n\\nThis will tell Ray to schedule the actor on a random node in the cluster, but limit the number of actors that can run concurrently on a single node to 4.\\n\\nAdditionally']"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_response_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "38d8690c-d80a-4921-a182-f6c809792e7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluation prompt\n",
    "system_content = \"\"\"\n",
    "    \"You are given a query, a reference answer, and a candidate answer.\n",
    "    You must {score} the candidate answer between 1 and 5 on how well it answers the query, \n",
    "    using the reference answer as the golden truth.\n",
    "    You must return your response in a line with only the score.\n",
    "    Do not add any more details.\n",
    "    On a separate line provide your {reasoning} for the score as well.\n",
    "    Return your response following the exact format outlined below.\n",
    "    All of this must be in a valid JSON format.\n",
    "    \n",
    "    {\"score\": score,\n",
    "     \"reasoning\": reasoning}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "5ccde762-18cb-4912-af78-ccf2697164fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluation_scores = []\n",
    "llm_name = \"gpt-4\"\n",
    "max_context_length = 8192\n",
    "\n",
    "openai.api_base = os.environ[\"OPENAI_API_BASE\"]\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "for rag_response, golden_response in zip(rag_response_str, golden_responses):\n",
    "    query = golden_response[\"question\"]\n",
    "    golden_answer = golden_response[\"response\"]\n",
    "    generated_answer = rag_response\n",
    "    \n",
    "    context_length = max_context_length - len(system_content)\n",
    "    user_content = f\"The query is {query}, the reference answer is {golden_answer}, and the candidate answer is {generated_answer}\"[:context_length]\n",
    "    \n",
    "    response = generate_response(llm_name, temperature=0.0, system_content=system_content, user_content=user_content)\n",
    "    evaluation_scores.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "cf863f46-a74e-4005-a844-eaa764163686",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"score\": 4,\\n \"reasoning\": \"The candidate answer provides a good response to the query, offering practical advice on how to handle data type conversions when using map_batches in Ray Data. However, it introduces PyTorch tensors, which were not mentioned in the reference answer. The reference answer specifically talks about handling NumPy ndarrays and pandas DataFrames, not PyTorch tensors. Therefore, while the candidate answer is helpful, it is not entirely aligned with the reference answer.\"}',\n",
       " '{\"score\": 4,\\n \"reasoning\": \"The candidate answer provides a detailed explanation of how autoscaling works in a Ray Serve application, including how it checks ServeHandle queues and in-flight queries, how it distributes queries to replicas, and how it interacts with the Ray Autoscaler. However, it does not mention the specific configuration parameters that can be set in the autoscaling_config field, such as min_replicas, max_replicas, initial_replicas, and target_num_ongoing_requests_per_replica, which are mentioned in the reference answer. Therefore, it is not a perfect match to the reference answer, but it is still a good answer to the query.\"}',\n",
       " '{\"score\": 4,\\n \"reasoning\": \"The candidate answer is mostly correct and provides a detailed explanation on how to get the address of a Ray node. However, it incorrectly suggests that the \\'address\\' attribute can be accessed directly from the node dictionary returned by \\'ray.nodes()\\', which is not the case. The correct key to use is \\'NodeManagerAddress\\', as stated in the reference answer. The candidate answer also provides additional information on how to initialize a Ray cluster with a specific address, which is not directly related to the query but could be useful in a broader context.\"}',\n",
       " '{\"score\": 5,\\n \"reasoning\": \"The candidate answer correctly states that Ray supports NCCL, which is in line with the reference answer. It also provides additional information about how Ray uses NCCL, which is accurate and relevant to the query.\"}',\n",
       " '{\"score\": 1,\\n \"reasoning\": \"The candidate answer contradicts the reference answer. The reference answer states that Ray is integrated with DeepSpeed, while the candidate answer states that Ray doesn\\'t provide native integration with DeepSpeed. Therefore, the candidate answer does not correctly answer the query.\"}',\n",
       " '{\"score\": 2,\\n \"reasoning\": \"The candidate answer is partially correct but it contains some inaccuracies. It correctly mentions that Ray provides an async API that allows you to use asyncio to write concurrent code. However, it incorrectly states that Ray does not support asyncio for remote tasks. The reference answer clearly states that Ray\\'s ObjectRefs can be translated to asyncio.Futures, which means that you can use `await` with a Ray future object like `x.remote()`. The candidate answer also suggests using Ray\\'s `wait` method, which is not mentioned in the reference answer.\"}',\n",
       " '{\"score\": 4,\\n \"reasoning\": \"The candidate answer provides a detailed comparison of Spark, Ray, and Dask, similar to the reference answer. However, it incorrectly states that Ray is built on top of Apache Spark, which is not mentioned or implied in the reference answer. This misinformation slightly reduces the score.\"}',\n",
       " '{\"score\": 4,\\n \"reasoning\": \"The candidate answer correctly identifies that Ray can overload a node with more tasks than the resources allow, and provides a valid explanation for this. However, it does not mention that Ray\\'s resources are logical and do not impose limits on actual physical resource usage, which is a key point in the reference answer. The candidate answer also does not mention that Ray relies on the user to ensure that tasks or actors do not use more resources than specified, which is another important point in the reference answer.\"}',\n",
       " '{\"score\": 5,\\n \"reasoning\": \"The candidate answer accurately and completely answers the query. It includes all the information from the reference answer, and even adds a bit more detail about the limitations of Ray Client for ML workloads. The candidate answer is well-structured and easy to understand.\"}',\n",
       " '{\"score\": 4,\\n \"reasoning\": \"The candidate answer provides a correct and detailed explanation of how to scatter actors across a cluster using the \\'SPREAD\\' scheduling strategy in Ray, similar to the reference answer. However, it does not provide examples of how to call the remote function or create instances of the remote actor, which is included in the reference answer.\"}']"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c958b4c-7be8-4f95-ad1c-3c8141b445e1",
   "metadata": {},
   "source": [
    "Now let's parse the score and the response from the LLM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "3a2c690c-b7e5-4a96-8342-5050ec6c5b06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_evaluation_score(evaluation_result):\n",
    "    # Define regular expressions for extracting values\n",
    "    score_pattern = r'\"score\"\\s*:\\s*([0-9]+)'\n",
    "    reasoning_pattern = r'\"reasoning\"\\s*:\\s*\"([^\"]*)\"'\n",
    "\n",
    "    # Extract values using regular expressions\n",
    "    score_match = re.search(score_pattern, evaluation_result)\n",
    "    reasoning_match = re.search(reasoning_pattern, evaluation_result)\n",
    "\n",
    "    # Convert\n",
    "    if score_match and reasoning_match:\n",
    "        score = float(score_match.group(1))\n",
    "        reasoning = reasoning_match.group(1)\n",
    "        return {\"score\": score, \"reasoning\": reasoning}\n",
    "\n",
    "    return {\"score\": \"\", \"reasoning\": \"\"}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "f5feeaac-bb4c-4130-8213-16c0bd3641bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parsed_evaluation_scores = [parse_evaluation_score(result) for result in evaluation_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "9118c3b8-c42f-4b54-8eef-9c96908cd9cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 4.0,\n",
       "  'reasoning': 'The candidate answer provides a good response to the query, offering practical advice on how to handle data type conversions when using map_batches in Ray Data. However, it introduces PyTorch tensors, which were not mentioned in the reference answer. The reference answer specifically talks about handling NumPy ndarrays and pandas DataFrames, not PyTorch tensors. Therefore, while the candidate answer is helpful, it is not entirely aligned with the reference answer.'},\n",
       " {'score': 4.0,\n",
       "  'reasoning': 'The candidate answer provides a detailed explanation of how autoscaling works in a Ray Serve application, including how it checks ServeHandle queues and in-flight queries, how it distributes queries to replicas, and how it interacts with the Ray Autoscaler. However, it does not mention the specific configuration parameters that can be set in the autoscaling_config field, such as min_replicas, max_replicas, initial_replicas, and target_num_ongoing_requests_per_replica, which are mentioned in the reference answer. Therefore, it is not a perfect match to the reference answer, but it is still a good answer to the query.'},\n",
       " {'score': 4.0,\n",
       "  'reasoning': \"The candidate answer is mostly correct and provides a detailed explanation on how to get the address of a Ray node. However, it incorrectly suggests that the 'address' attribute can be accessed directly from the node dictionary returned by 'ray.nodes()', which is not the case. The correct key to use is 'NodeManagerAddress', as stated in the reference answer. The candidate answer also provides additional information on how to initialize a Ray cluster with a specific address, which is not directly related to the query but could be useful in a broader context.\"},\n",
       " {'score': 5.0,\n",
       "  'reasoning': 'The candidate answer correctly states that Ray supports NCCL, which is in line with the reference answer. It also provides additional information about how Ray uses NCCL, which is accurate and relevant to the query.'},\n",
       " {'score': 1.0,\n",
       "  'reasoning': \"The candidate answer contradicts the reference answer. The reference answer states that Ray is integrated with DeepSpeed, while the candidate answer states that Ray doesn't provide native integration with DeepSpeed. Therefore, the candidate answer does not correctly answer the query.\"},\n",
       " {'score': 2.0,\n",
       "  'reasoning': \"The candidate answer is partially correct but it contains some inaccuracies. It correctly mentions that Ray provides an async API that allows you to use asyncio to write concurrent code. However, it incorrectly states that Ray does not support asyncio for remote tasks. The reference answer clearly states that Ray's ObjectRefs can be translated to asyncio.Futures, which means that you can use `await` with a Ray future object like `x.remote()`. The candidate answer also suggests using Ray's `wait` method, which is not mentioned in the reference answer.\"},\n",
       " {'score': 4.0,\n",
       "  'reasoning': 'The candidate answer provides a detailed comparison of Spark, Ray, and Dask, similar to the reference answer. However, it incorrectly states that Ray is built on top of Apache Spark, which is not mentioned or implied in the reference answer. This misinformation slightly reduces the score.'},\n",
       " {'score': 4.0,\n",
       "  'reasoning': \"The candidate answer correctly identifies that Ray can overload a node with more tasks than the resources allow, and provides a valid explanation for this. However, it does not mention that Ray's resources are logical and do not impose limits on actual physical resource usage, which is a key point in the reference answer. The candidate answer also does not mention that Ray relies on the user to ensure that tasks or actors do not use more resources than specified, which is another important point in the reference answer.\"},\n",
       " {'score': 5.0,\n",
       "  'reasoning': 'The candidate answer accurately and completely answers the query. It includes all the information from the reference answer, and even adds a bit more detail about the limitations of Ray Client for ML workloads. The candidate answer is well-structured and easy to understand.'},\n",
       " {'score': 4.0,\n",
       "  'reasoning': \"The candidate answer provides a correct and detailed explanation of how to scatter actors across a cluster using the 'SPREAD' scheduling strategy in Ray, similar to the reference answer. However, it does not provide examples of how to call the remote function or create instances of the remote actor, which is included in the reference answer.\"}]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_evaluation_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3190240d-7bb1-4079-8bfa-0d6d0ed9166d",
   "metadata": {},
   "source": [
    "Let's save the query, both responses, and the score to a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "8fc98a6d-a27a-4a42-a5c5-4574a336e679",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores = [\n",
    "    {\"question\": golden_response[\"question\"],\n",
    "     \"golden_response\": golden_response[\"response\"],\n",
    "     \"generated_response\": rag_response,\n",
    "     \"score\": parsed_score[\"score\"],\n",
    "     \"reasoning\": parsed_score[\"reasoning\"],\n",
    "    }\n",
    "    for parsed_score, rag_response, golden_response in zip(parsed_evaluation_scores, rag_response_str, golden_responses)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "1268855c-351f-4eef-9c4b-807c923c038a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'Iâ€™m struggling a bit with Ray Data type conversions when I do map_batches. Any advice?',\n",
       "  'golden_response': \"If you're struggling with data type conversions in Ray Data when using map_batches, it's important to remember that Ray Data represents batches as either dicts of NumPy ndarrays or pandas DataFrames. The type of batch format you want to use can be specified in the map_batches() function using the batch_format parameter.\\n\\nIf you're working with NumPy, your function should take in and return a Dict[str, np.ndarray]. For example, in the increase_brightness function, the batch of images is increased in brightness and clipped to ensure values stay within the valid range for images.\\n\\nIf you're working with pandas, your function should take in and return a pd.DataFrame. For example, in the drop_nas function, any rows with NA values are dropped from the DataFrame.\\n\\nMake sure your function is correctly handling the data type you've specified in batch_format. If you're still having issues, the problem might be with the function you're passing to map_batches, not with the data type conversion itself.\",\n",
       "  'generated_response': 'It seems like you\\'re encountering issues with type conversions when using `map_batches` with Ray Data. Here are some tips that may help:\\n\\n1. Use the `batch_format` argument: When calling `map_batches`, you can specify the `batch_format` argument to indicate the format of the batches. If you\\'re working with NumPy arrays, set `batch_format=\"numpy\"`. If you\\'re working with PyTorch tensors, set `batch_format=\"torch\"`. This can help Ray Data handle the type conversions correctly.\\n2. Return the correct type: When writing a transformation function for `map_batches`, make sure to return a batch in the correct format. If you\\'re working with NumPy arrays, return a dictionary of NumPy arrays. If you\\'re working with PyTorch tensors, return a dictionary of PyTorch tensors.\\n3. Avoid returning lists: When using `map_batches`, avoid returning lists of arrays or tensors. Instead, return a dictionary of arrays or tensors, where each key corresponds to a batch dimension. This can help Ray Data handle the type conversions correctly.\\n4',\n",
       "  'score': 4.0,\n",
       "  'reasoning': 'The candidate answer provides a good response to the query, offering practical advice on how to handle data type conversions when using map_batches in Ray Data. However, it introduces PyTorch tensors, which were not mentioned in the reference answer. The reference answer specifically talks about handling NumPy ndarrays and pandas DataFrames, not PyTorch tensors. Therefore, while the candidate answer is helpful, it is not entirely aligned with the reference answer.'},\n",
       " {'question': 'How does autoscaling work in a Ray Serve application?',\n",
       "  'golden_response': 'Autoscaling in a Ray Serve application works by adjusting to traffic spikes through observing queue sizes and making scaling decisions to add or remove replicas. This is configured by setting the autoscaling_config field in deployment options. The configuration includes parameters like min_replicas, max_replicas, initial_replicas, and target_num_ongoing_requests_per_replica. \\n\\nThe min_replicas and max_replicas fields set the range of replicas that the Serve autoscaler can choose from. The initial_replicas field sets the number of replicas at the start of the deployment. The target_num_ongoing_requests_per_replica configuration specifies how aggressively the autoscaler should react to traffic. \\n\\nThe Ray Serve Autoscaler is an application-level autoscaler that sits on top of the Ray Autoscaler. It asks Ray to start a number of replica actors based on the request demand. If the Ray Autoscaler determines there arenâ€™t enough available CPUs to place these actors, it requests more Ray nodes from the underlying cloud provider. \\n\\nThe autoscaling algorithm takes into consideration several user-specified parameters when deciding the target number of replicas for your deployment. These include min_replicas, max_replicas, target_num_ongoing_requests_per_replica, downscale_delay_s, upscale_delay_s, smoothing_factor, and metrics_interval_s.',\n",
       "  'generated_response': 'Autoscaling in a Ray Serve application works by periodically checking ServeHandle queues and in-flight queries on replicas to decide whether or not to scale the number of replicas. Each ServeHandle continuously polls the controller to check for new deployment replicas. Whenever new replicas are discovered, it will send any buffered or new queries to the replica until max_concurrent_queries is reached. Queries are sent to replicas in round-robin fashion, subject to the constraint that no replica is handling more than max_concurrent_queries requests at a time.\\n\\nThe Ray Serve autoscaler asks Ray to start a number of replica actors based on the request demand. If the Ray Autoscaler determines there arenâ€™t enough available CPUs to place these actors, it responds by requesting more Ray nodes. The underlying cloud provider will then respond by adding more nodes. Similarly, when Ray Serve scales down and terminates some replica actors, it will try to do so in a way that results in the most nodes having no Ray actors or tasks running on them, at which point the Ray autoscaler will remove those',\n",
       "  'score': 4.0,\n",
       "  'reasoning': 'The candidate answer provides a detailed explanation of how autoscaling works in a Ray Serve application, including how it checks ServeHandle queues and in-flight queries, how it distributes queries to replicas, and how it interacts with the Ray Autoscaler. However, it does not mention the specific configuration parameters that can be set in the autoscaling_config field, such as min_replicas, max_replicas, initial_replicas, and target_num_ongoing_requests_per_replica, which are mentioned in the reference answer. Therefore, it is not a perfect match to the reference answer, but it is still a good answer to the query.'},\n",
       " {'question': 'how do I get the address of a ray node',\n",
       "  'golden_response': \"To get the address of a Ray node, you can use the `ray.nodes()` function which returns a list of the nodes in the cluster. This function provides a dictionary for each node, which includes the 'NodeManagerAddress' key. The value of this key is the private IP address of the node that the raylet is on. Here is an example of how to use it:\\n\\n```python\\nimport ray\\n\\nray.init()\\nnodes = ray.nodes()\\n\\nfor node in nodes:\\n    print(node['NodeManagerAddress'])\\n```\\n\\nThis will print out the addresses of all the nodes in your Ray cluster.\",\n",
       "  'generated_response': \"You can get the address of a Ray node by using the `ray.nodes` API to fetch all nodes and map the node ID to the corresponding IP. Alternatively, you can use the `ray.init` method to specify the address of the Ray cluster, or you can use the `RAY_ADDRESS` environment variable to set the address of the Ray dashboard.\\n\\nHere's an example of how you can use the `ray.nodes` API to get the address of a Ray node:\\n```\\nimport ray\\n\\n# Initialize the Ray cluster\\nray.init()\\n\\n# Get the list of nodes in the Ray cluster\\nnodes = ray.nodes()\\n\\n# Print the address of the first node\\nprint(nodes[0].address)\\n```\\nThis will print the address of the first node in the Ray cluster. You can also use the `ray.nodes` API to filter the list of nodes based on certain criteria, such as the node's IP address or the node's role in the cluster.\\n\\nAlternatively, you can use the `ray.init` method to specify the address of the Ray cluster. For example:\\n```\\nimport ray\\n\\n# Initialize the Ray cluster\",\n",
       "  'score': 4.0,\n",
       "  'reasoning': \"The candidate answer is mostly correct and provides a detailed explanation on how to get the address of a Ray node. However, it incorrectly suggests that the 'address' attribute can be accessed directly from the node dictionary returned by 'ray.nodes()', which is not the case. The correct key to use is 'NodeManagerAddress', as stated in the reference answer. The candidate answer also provides additional information on how to initialize a Ray cluster with a specific address, which is not directly related to the query but could be useful in a broader context.\"},\n",
       " {'question': 'Does Ray support NCCL?',\n",
       "  'golden_response': 'Yes, Ray does support NCCL. The Ray collective communication library uses NCCL and GLOO as the optional high-performance communication backends. It enables more efficient out-of-band collective communication between Ray actor and task processes, operating on both distributed CPUs and GPUs.',\n",
       "  'generated_response': 'Yes, Ray supports NCCL as an optional high-performance communication backend for its collective communication library.',\n",
       "  'score': 5.0,\n",
       "  'reasoning': 'The candidate answer correctly states that Ray supports NCCL, which is in line with the reference answer. It also provides additional information about how Ray uses NCCL, which is accurate and relevant to the query.'},\n",
       " {'question': 'Is Ray integrated with DeepSpeed?',\n",
       "  'golden_response': \"Yes, Ray is integrated with DeepSpeed. The integration of Ray AIR's TransformersTrainer allows for the configuration and use of Distributed Data Parallelism (DDP) and DeepSpeed. This is particularly useful for training large models like GPT-J, as DeepSpeed optimizes the training process and allows for offloading and partitioning of optimizer and parameter states, reducing GPU RAM usage.\",\n",
       "  'generated_response': \"No, Ray doesn't provide native integration with DeepSpeed. However, PyTorch Lightning offers a DeepSpeed integration that provides a simple interface to configure the knobs for DeepSpeed and automatically trigger the training process with the DeepSpeed Engine. Ray LightningTrainer allows scaling PyTorch Lightning jobs across multiple nodes in a Ray cluster, without worrying about the underlying cluster management, autoscaling, and distributed process group settings.\",\n",
       "  'score': 1.0,\n",
       "  'reasoning': \"The candidate answer contradicts the reference answer. The reference answer states that Ray is integrated with DeepSpeed, while the candidate answer states that Ray doesn't provide native integration with DeepSpeed. Therefore, the candidate answer does not correctly answer the query.\"},\n",
       " {'question': \"what will happen if I use AsyncIO's await to wait for a Ray future like `await x.remote()`\",\n",
       "  'golden_response': \"If you use AsyncIO's await to wait for a Ray future like `await x.remote()`, it will allow you to asynchronously wait for the result of the remote task. This is possible because Ray's ObjectRefs can be translated to asyncio.Futures. This feature is useful in concurrent applications where you want to wait for multiple tasks to complete. You can also directly access the future object by calling `asyncio.wrap_future(ref.future())`.\",\n",
       "  'generated_response': \"Based on the provided context information, it seems that you are asking about using AsyncIO's `await` keyword to wait for a Ray future object, specifically `x.remote()`.\\n\\nAccording to the context, Ray provides an async API that allows you to use asyncio to write concurrent code. However, the context also mentions that Ray does not support asyncio for remote tasks, which means that using `await` with a Ray future object like `x.remote()` will not work as expected.\\n\\nInstead, you can wrap the async function with a wrapper to run the task synchronously, as shown in the example provided in the context. This will allow you to use `await` with the wrapped function, which will in turn allow you to wait for the task to complete.\\n\\nAlternatively, you can use Ray's `wait` method to wait for the future object to complete, as shown in the first code snippet provided in the context. This method will block the execution of the code until the future object is resolved, allowing you to write synchronous code that waits for the result of the remote task.\\n\\nIn summary, using `await` with a Ray future object like `x.remote()` will\",\n",
       "  'score': 2.0,\n",
       "  'reasoning': \"The candidate answer is partially correct but it contains some inaccuracies. It correctly mentions that Ray provides an async API that allows you to use asyncio to write concurrent code. However, it incorrectly states that Ray does not support asyncio for remote tasks. The reference answer clearly states that Ray's ObjectRefs can be translated to asyncio.Futures, which means that you can use `await` with a Ray future object like `x.remote()`. The candidate answer also suggests using Ray's `wait` method, which is not mentioned in the reference answer.\"},\n",
       " {'question': 'How would you compare Spark, Ray, Dask?',\n",
       "  'golden_response': \"Spark, Ray, and Dask are all distributed data processing frameworks, but they have different strengths and use cases.\\n\\nSpark is a mature and widely-used framework that excels in batch processing workloads. It has a rich ecosystem and a variety of built-in libraries for machine learning, graph processing, and stream processing. However, it may not be as well-suited for GPU workloads for deep learning inference.\\n\\nRay, specifically Ray Data, is designed for both infrastructure management and data processing. It abstracts away complexities such as sharding your dataset, parallelizing the inference over these shards, and transferring data from storage to CPU to GPU. It is built for offline batch jobs and handles many of the same batch processing workloads as Spark, but with a streaming paradigm that is better suited for GPU workloads for deep learning inference.\\n\\nDask is not mentioned in the context provided, so a comparison cannot be made based on the given information. However, in general, Dask is known for its ability to handle larger-than-memory computations and its integration with Python's scientific software stack. It is flexible and can be used for a variety of workloads, including machine learning, and it can scale from single machines to large clusters.\",\n",
       "  'generated_response': 'Spark, Ray, and Dask are all powerful tools for data processing and machine learning. While they share some similarities, they also have some key differences.\\n\\nSpark is a widely-used open-source data processing engine that can handle large-scale data processing tasks. It has a wide range of libraries, including Spark SQL, Spark DataFrame, and Spark MLlib, which make it easy to perform data processing, data transformation, and machine learning tasks. Spark is designed to be fast, scalable, and fault-tolerant, making it a popular choice for big data applications.\\n\\nRay is a distributed computing framework that is designed to handle complex, large-scale data processing tasks. It provides a simple, Python-based API that allows developers to write code that can be executed on a distributed cluster. Ray is built on top of Apache Spark and provides additional features such as automatic task retries, stateful streaming, and support for distributed machine learning.\\n\\nDask is a parallel computing library that provides big data collections that mimic the APIs of familiar libraries like NumPy and Pandas. It allows developers to write code that can be executed on a multi-machine cluster, providing automatic data parallelism, smart scheduling,',\n",
       "  'score': 4.0,\n",
       "  'reasoning': 'The candidate answer provides a detailed comparison of Spark, Ray, and Dask, similar to the reference answer. However, it incorrectly states that Ray is built on top of Apache Spark, which is not mentioned or implied in the reference answer. This misinformation slightly reduces the score.'},\n",
       " {'question': 'why would ray overload a node w/ more task that the resources allow ?',\n",
       "  'golden_response': \"Ray might overload a node with more tasks than the resources allow because Ray's resources are logical and do not impose limits on actual physical resource usage. This means that a task with a num_cpus=1 requirement can still launch multiple threads and use multiple physical CPUs. Ray relies on the user to ensure that tasks or actors do not use more resources than specified. It does not provide CPU isolation for tasks or actors, and instead lets the operating system schedule and run the task. However, Ray does provide GPU isolation by setting the CUDA_VISIBLE_DEVICES environment variable.\",\n",
       "  'generated_response': 'Ray would overload a node with more tasks than the resources allow because the scheduler limits task concurrency to the available CPUs and actor concurrency to infinite. Tasks that use more than 1 CPU (e.g., via multithreading) may experience slowdown due to interference from concurrent ones, but otherwise are safe to run. However, tasks or actors that use more than their proportionate share of memory may overload a node and cause issues like OOM.',\n",
       "  'score': 4.0,\n",
       "  'reasoning': \"The candidate answer correctly identifies that Ray can overload a node with more tasks than the resources allow, and provides a valid explanation for this. However, it does not mention that Ray's resources are logical and do not impose limits on actual physical resource usage, which is a key point in the reference answer. The candidate answer also does not mention that Ray relies on the user to ensure that tasks or actors do not use more resources than specified, which is another important point in the reference answer.\"},\n",
       " {'question': 'when should I use Ray Client?',\n",
       "  'golden_response': 'You should use Ray Client when you want to connect an interactive Python shell to a remote cluster. It is useful for developing interactively in a local Python shell. However, it requires a stable connection to the remote cluster and will terminate the workload if the connection is lost for more than 30 seconds. For long running workloads that you want to run on your cluster, it is recommended to use Ray Jobs instead. Note that Ray Client may not work as expected for ML workloads like Ray Tune or Ray Train, in which case you should use Ray Jobs API for interactive development on ML projects.',\n",
       "  'generated_response': 'Ray Client should be used when you want to connect an interactive Python shell to a remote cluster. It is useful for developing interactively in a local Python shell, but it requires a stable connection to the remote cluster and will terminate the workload if the connection is lost for more than 30 seconds. If you have a long-running workload that you want to run on your cluster, it is recommended to use Ray Jobs instead. Additionally, Ray Client has architectural limitations and may not work as expected when using Ray for ML workloads (like Ray Tune or Ray Train). In such cases, it is recommended to use Ray Jobs API for interactive development on ML projects.',\n",
       "  'score': 5.0,\n",
       "  'reasoning': 'The candidate answer accurately and completely answers the query. It includes all the information from the reference answer, and even adds a bit more detail about the limitations of Ray Client for ML workloads. The candidate answer is well-structured and easy to understand.'},\n",
       " {'question': 'how to scatter actors across the cluster?',\n",
       "  'golden_response': 'To scatter actors across the cluster, you can use the \"SPREAD\" scheduling strategy in Ray. This strategy will distribute the tasks or actors among the available nodes. Here is how you can do it:\\n\\n1. Define a function or an actor with the \"@ray.remote\" decorator and specify the scheduling strategy as \"SPREAD\". For example:\\n\\n```python\\n@ray.remote(scheduling_strategy=\"SPREAD\")\\ndef spread_func():\\n    return 2\\n```\\n\\nor \\n\\n```python\\n@ray.remote(num_cpus=1)\\nclass SpreadActor:\\n    pass\\n```\\n\\n2. Then, you can spread tasks or actors across the cluster by calling the remote function or creating instances of the remote actor. For example:\\n\\n```python\\n# Spread tasks across the cluster.\\n[spread_func.remote() for _ in range(10)]\\n```\\n\\nor \\n\\n```python\\n# Spread actors across the cluster.\\nactors = [SpreadActor.options(scheduling_strategy=\"SPREAD\").remote() for _ in range(10)]\\n```\\n\\nThis will create 10 tasks or actors and distribute them across the available nodes in the cluster.',\n",
       "  'generated_response': 'You can use the \"SPREAD\" scheduling strategy in Ray to scatter actors across the cluster. The \"SPREAD\" strategy will try to spread the tasks or actors among available nodes. You can specify the scheduling strategy when creating a remote function or actor, like this:\\n\\n@ray.remote(scheduling_strategy=\"SPREAD\")\\ndef my_actor():\\n    pass\\n\\nOr\\n\\n@ray.remote(num_cpus=1, scheduling_strategy=\"SPREAD\")\\nclass MyActor:\\n    pass\\n\\nThis will tell Ray to schedule the actor on a random node in the cluster. You can also use the \"SPREAD\" strategy in combination with the \"max_concurrency\" parameter to control the maximum number of actors that can run concurrently on a single node. For example:\\n\\n@ray.remote(num_cpus=1, max_concurrency=4, scheduling_strategy=\"SPREAD\")\\nclass MyActor:\\n    pass\\n\\nThis will tell Ray to schedule the actor on a random node in the cluster, but limit the number of actors that can run concurrently on a single node to 4.\\n\\nAdditionally',\n",
       "  'score': 4.0,\n",
       "  'reasoning': \"The candidate answer provides a correct and detailed explanation of how to scatter actors across a cluster using the 'SPREAD' scheduling strategy in Ray, similar to the reference answer. However, it does not provide examples of how to call the remote function or create instances of the remote actor, which is included in the reference answer.\"}]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "e00e6239-5c44-4c1a-89d5-7549fb0ad98a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"eval-scores.json\", \"w\") as file:\n",
    "    json.dump(scores, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095ef29b-b968-4449-9a50-4234a5b294ab",
   "metadata": {},
   "source": [
    "We can also calculate the average scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "d6b7d3a9-d2c5-4381-b2d4-9c1aa1d21df1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_scores = sum(score[\"score\"] for score in scores) / len(scores)\n",
    "average_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0229cb-2256-4762-a772-55740e617951",
   "metadata": {},
   "source": [
    "## Alternative forms of evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43b1970-ded0-4ee4-b49e-49447fad1533",
   "metadata": {},
   "source": [
    "Generating reference responses and then using them for evaluation can give us a more accurate assesment on how our query engine is performing. However, this approach can be expensive. We have to make an initial pass through GPT4 to generate the reference response, and then we have to make another pass through GPT4 to evaluate our application's responses against the reference response.\n",
    "\n",
    "We can explore other evaluation metrics to get a better sense on how our query engine is performing, without needing to make multiple passes to GPT4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1905688d-9a0d-4e2a-81ea-40242f50905a",
   "metadata": {},
   "source": [
    "### Evaluating for hallucination/relevancy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6662a23a-8388-42cc-b986-0062afee3b48",
   "metadata": {},
   "source": [
    "One metric we can test is relevancy, which does not require generating reference responses. With this approach, we check to see if the generated response is relevant to at least one of the retrieved sources and to the query. This ensures that our LLM is not making up a response, but rather that it is relevant to the question that is being asked, and also that is relevant to at least one of the retrieved context.\n",
    "\n",
    "This does NOT check whether the response is a correct response.\n",
    "\n",
    "This capability is built into LlamaIndex, via the QueryResponseEvaluator. We use gpt-4 as the evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "c767913f-fc0a-4437-932a-4a5106a5768c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.evaluation import QueryResponseEvaluator\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index import ServiceContext\n",
    "\n",
    "def evaluate_response_relevance(queries: list, responses: list):\n",
    "    openai.api_base = os.environ[\"OPENAI_API_BASE\"]\n",
    "    openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "    llm = OpenAI(model=\"gpt-4\", temperature=0.0)\n",
    "    service_context = ServiceContext.from_defaults(llm=llm)\n",
    "    evaluator = QueryResponseEvaluator(service_context=service_context)\n",
    "\n",
    "    evals = []\n",
    "    for query, response in zip(queries, responses):\n",
    "        evals.append(str(evaluator.evaluate(query, response)))\n",
    "    \n",
    "    return len([val == \"YES\" for val in evals]) / len(evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "6ecd11db-ced6-433b-844d-03c6d0adc30a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "relevance_results = evaluate_response_relevance(queries=[sample[\"question\"] for sample in ten_samples], responses=rag_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "ed4555fa-5901-49df-8f66-afbe00d27079",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevance_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
