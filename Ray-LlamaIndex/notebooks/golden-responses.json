[
    {
        "question": "I\u2019m struggling a bit with Ray Data type conversions when I do map_batches. Any advice?",
        "source": "https://docs.ray.io/en/master/data/transforming-data.html#configuring-batch-format",
        "response": "Sure, when using the `map_batches()` function in Ray Data, you can specify the batch format by using the `batch_format` parameter. If you want to work with NumPy ndarrays, you can set `batch_format=\"numpy\"`. For example, if you have a function that increases the brightness of an image, you can use it like this:\n\n```python\nfrom typing import Dict\nimport numpy as np\nimport ray\n\ndef increase_brightness(batch: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n    batch[\"image\"] = np.clip(batch[\"image\"] + 4, 0, 255)\n    return batch\n\nds = (\n    ray.data.read_images(\"s3://[email\u00a0protected]/image-datasets/simple\")\n    .map_batches(increase_brightness, batch_format=\"numpy\")\n)\n```\n\nOn the other hand, if you prefer to work with pandas DataFrames, you can set `batch_format=\"pandas\"`. For instance, if you have a function that drops NA values from a DataFrame, you can use it like this:\n\n```python\nimport pandas as pd\nimport ray\n\ndef drop_nas(batch: pd.DataFrame) -> pd.DataFrame:\n    return batch.dropna()\n\nds = (\n    ray.data.read_csv(\"s3://[email\u00a0protected]/iris.csv\")\n    .map_batches(drop_nas, batch_format=\"pandas\")\n)\n```\n\nRemember, the function you pass to `map_batches()` should be able to handle the data type you specify in `batch_format`."
    },
    {
        "question": "How does autoscaling work in a Ray Serve application?",
        "source": "https://docs.ray.io/en/master/serve/scaling-and-resource-allocation.html#autoscaling",
        "response": "In a Ray Serve application, autoscaling works by adjusting to traffic spikes through observing queue sizes and making scaling decisions to add or remove replicas. This is achieved by setting the autoscaling_config field in deployment options. The autoscaling_config parameters include min_replicas, initial_replicas, max_replicas, and target_num_ongoing_requests_per_replica. \n\nThe min_replicas and max_replicas fields configure the range of replicas which the Serve autoscaler chooses from. Deployments will start with initial_replicas, which is optional and set to min_replicas by default. The target_num_ongoing_requests_per_replica configuration specifies how aggressively the autoscaler should react to traffic. \n\nThe Ray Serve Autoscaler is an application-level autoscaler that sits on top of the Ray Autoscaler. It asks Ray to start a number of replica actors based on the request demand. If the Ray Autoscaler determines there aren\u2019t enough available CPUs to place these actors, it responds by requesting more Ray nodes. Similarly, when Ray Serve scales down and terminates some replica actors, it will try to do so in a way that results in the most nodes having no Ray actors or tasks running on them, at which point the Ray autoscaler will remove those nodes. \n\nOther parameters that influence autoscaling decisions include downscale_delay_s, upscale_delay_s, smoothing_factor, and metrics_interval_s. These parameters control the frequency of autoscaling work, the speed of each autoscaling step, and how often each replica sends metrics to the autoscaler, respectively."
    },
    {
        "question": "how do I get the address of a ray node",
        "source": "https://docs.ray.io/en/master/ray-core/miscellaneous.html#node-information",
        "response": "You can get the address of a Ray node by accessing the 'NodeManagerAddress' field of the node's information. This field contains the PrivateIP of the node that the raylet is on."
    },
    {
        "question": "Does Ray support NCCL?",
        "source": "https://docs.ray.io/en/master/ray-more-libs/ray-collective.html",
        "response": "Yes, Ray does support NCCL. It uses NCCL as one of the optional high-performance communication backends in its collective communication library."
    },
    {
        "question": "Is Ray integrated with DeepSpeed?",
        "source": "https://docs.ray.io/en/master/ray-air/examples/gptj_deepspeed_fine_tuning.html#fine-tuning-the-model-with-ray-air-a-name-train-a",
        "response": "Yes, Ray is integrated with DeepSpeed. This integration allows for the optimization of the training process, including the ability to offload and partition optimizer and parameter states, which can reduce GPU memory usage. This is particularly useful when working with large models that may not fit on smaller GPU types."
    },
    {
        "question": "what will happen if I use AsyncIO's await to wait for a Ray future like `await x.remote()`",
        "source": "https://docs.ray.io/en/master/ray-core/actors/async_api.html#objectrefs-as-asyncio-futures",
        "response": "If you use AsyncIO's await to wait for a Ray future like `await x.remote()`, it will allow you to wait for the completion of the remote task. This is possible because ObjectRefs can be translated to asyncio.Futures. This feature enables you to integrate Ray futures into existing concurrent applications."
    },
    {
        "question": "How would you compare Spark, Ray, Dask?",
        "source": "https://docs.ray.io/en/master/data/overview.html#how-does-ray-data-compare-to-x-for-offline-inference",
        "response": "The context does not provide information on how to compare Spark, Ray, and Dask."
    },
    {
        "question": "why would ray overload a node w/ more task that the resources allow ?",
        "source": "https://docs.ray.io/en/master/ray-core/scheduling/resources.html#physical-resources-and-logical-resources",
        "response": "Ray allows for overloading a node with more tasks than the resources allow because it operates on logical resources, not physical ones. This means that the resource requirements of tasks or actors do not impose limits on actual physical resource usage. Ray doesn't prevent a task from using more resources than specified. It's the user's responsibility to ensure tasks or actors do not exceed the specified resource requirements. Ray doesn't provide CPU isolation for tasks or actors, instead, it lets the operating system schedule and run the task."
    },
    {
        "question": "when should I use Ray Client?",
        "source": "https://docs.ray.io/en/master/cluster/running-applications/job-submission/ray-client.html#when-to-use-ray-client",
        "response": "Ray Client should be used when you want to connect an interactive Python shell to a remote cluster. It is particularly useful for developing interactively in a local Python shell. However, it requires a stable connection to the remote cluster and will terminate the workload if the connection is lost for more than 30 seconds. If you have a long running workload that you want to run on your cluster, it is recommended to use Ray Jobs instead. Please note that Ray Client may not work as expected for ML workloads like Ray Tune or Ray Train."
    },
    {
        "question": "how to scatter actors across the cluster?",
        "source": "https://docs.ray.io/en/master/ray-core/scheduling/index.html#spread",
        "response": "To scatter actors across the cluster, you can use the \"SPREAD\" scheduling strategy. This can be done by creating instances of the actor with the \"SPREAD\" option. Here is an example:\n\n```python\nactors = [SpreadActor.options(scheduling_strategy=\"SPREAD\").remote() for _ in range(10)]\n```\n\nIn this example, 10 instances of the SpreadActor class are created and spread across the cluster."
    }
]