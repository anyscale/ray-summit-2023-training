{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c044803a-3fe2-4297-ad71-c93ae2e078f5",
   "metadata": {},
   "source": [
    "# Part 2: Evaluating our LLM application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc13cfb-5e8a-401d-bb04-24a5793e69be",
   "metadata": {},
   "source": [
    "So far, we've chosen typical/arbitrary values for the various parts of our RAG application. But if we were to change something, such as our chunking logic, embedding model, LLM, etc. how can we know that we have a better configuration than before. A generative task like this is very difficult to quantitatively assess and so we need to develop creative ways to do so. \n",
    "\n",
    "Because we have many moving parts in our application, we need to perform unit/component and end-to-end evaluation. Component-wise evaluation can involve evaluating our retrieval in isolation (is the best source in our set of retrieved chunks) and evaluating our LLMs response (given the best source, is the LLM able to produce a quality answer). As for end-to-end evaluation, we can assess the quality of the entire system (given all data, what is the quality of the response).\n",
    "\n",
    "<span style=\"background: yellow; color: red; font-size: 1rem;\"><b>DIAGRAM:</b></span> Component and end-to-end evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b67c46c-7650-484b-a792-7eaf62c0e82e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119575d6-b0cc-49f3-a118-46bc3adf8189",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"ANYSCALE_API_BASE\"] = \"https://ray-summit-training-jrvwy.cld-kvedzwag2qa8i5bj.s.anyscaleuserdata-staging.com/v1/chat/completions\"\n",
    "os.environ[\"ANYSCALE_API_KEY\"] = \"tZLmCV1WtQAtnDx93MM5w3xaNhYV3whhcFRTKoH1GYQ\"\n",
    "\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://api.openai.com/v1\"\n",
    "# os.environ[\"OPENAI_API_KEY\"] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cedb0a-a7b3-4194-88a9-355122cd8a00",
   "metadata": {},
   "source": [
    "## Golden Context Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31583b8a-bd08-4054-95f6-2ae5256e6a21",
   "metadata": {},
   "source": [
    "In an ideal world, we would have a golden validation dataset: given a set of queries, we would have the correct sources that answer those queries, and optionally the correct answer that should be returned by the LLM.\n",
    "\n",
    "For this example, we have manually collected 177 representative user queries and identified the correct source in the documentation that answer those user queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8495831b-c4f9-4e84-a211-bc37ecf369e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "golden_dataset_path = Path(\"../datasets/eval-dataset-v1.jsonl\")\n",
    "\n",
    "with open(golden_dataset_path, \"r\") as f:\n",
    "    data = [json.loads(item) for item in list(f)]\n",
    "    \n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b187c2d5-5c4d-4b9a-9e5b-5f37c9e92b36",
   "metadata": {},
   "source": [
    "Our dataset contains 'question' and 'source' pairs. If we have a golden context dataset, it is the best option for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2300e0fc-3e65-4f43-98b6-3564bc1ccb3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6739202-90fa-46e8-bc7d-3c0ed33c8673",
   "metadata": {},
   "source": [
    "## Cold Start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20636b3e-5211-41c9-a390-0cfa23654c44",
   "metadata": {
    "tags": []
   },
   "source": [
    "We may not always have a prepared dataset of questions and the best source to answer that question readily available. To address this cold start problem, we could use an LLM to look at our documents and generate questions that the specific chunk would answer. This provides us with quality questions and the exact source the answer is in. However, this dataset generation method could be a bit noisy. The generate questions may not always be resembling of what your users may ask and the specific chunk we say is the best source may also have that exact information in other chunks. Nonetheless, this is a great way to start our development process while we collect + manually label a high quality dataset.\n",
    "\n",
    "<span style=\"background: yellow; color: red; font-size: 1rem;\"><b>DIAGRAM:</b></span> Show the synthetic data generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d718383c-74a2-4518-a51b-69a7857c5cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.evaluation import DatasetGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba0b1e6-4bc6-440b-a4e6-3f9915196f97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64373866-d0f7-4704-b03b-5c466fc59afb",
   "metadata": {},
   "source": [
    "Since we already have a dataset with representative user queries and ground truth labels, we will use that for evaluation instead of a synthetically generated dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330a7e10-18fc-495e-ab4a-da2f3ca00a97",
   "metadata": {},
   "source": [
    "## Evaluating Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6e7e79-5a2a-4247-bc3a-d093d0416761",
   "metadata": {},
   "source": [
    "The first component to evaluate in our RAG application is retrieval. Given a query, is our retriever pulling in the correct context to answer that query? Regardless of how good our LLM is, if it does not have the right context to answer the question, it cannot provide the right answer.\n",
    "\n",
    "We can use our golden context dataset to evaluate retrieval. The simplest approach is that for each query in our dataset, we can test to see if the correct source is included in any of the chunks that are retrieved by our retriever. This measures \"hit rate\".\n",
    "\n",
    "However, simply checking for existence can be misleading if we increase the number of chunks that we retrieve. Therefore, we also want to check the score that our retriever gives for the correct source. A higher score means our retriever is accurately determining the correct context. \n",
    "\n",
    "To summarize, for each query in our evaluation dataset, we will measure the following:\n",
    "1. Is the correct source included in any of the retrived chunks?\n",
    "2. What is the score our retriever gives to the correct source?\n",
    "\n",
    "<span style=\"background: yellow; color: red; font-size: 1rem;\"><b>DIAGRAM:</b></span> Show retrieval evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a5958c-74d8-4a11-b46c-a12a8fd5ad99",
   "metadata": {},
   "source": [
    "First, let's a get a retriever over the vector database. We have packaged this as a utility. It is the same as we did in notebook 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88e0d5f-a0ed-47e9-afcf-0ac882201e57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import get_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad0afdc-2f44-4ca7-b5c0-2c6499f4f35e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever = get_retriever(similarity_top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3b5438-9930-4857-a9a5-165cbd15c79b",
   "metadata": {},
   "source": [
    "Now let's evaluate our retriever. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4cd2ae-17ff-4e75-904f-a577499bc8ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "\n",
    "for entry in data:\n",
    "    query = entry[\"question\"]\n",
    "    expected_source = entry['source']\n",
    "    \n",
    "    retrieved_nodes = retriever.retrieve(query)\n",
    "    retrieved_sources = [node.metadata['source'] for node in retrieved_nodes]\n",
    "    \n",
    "    # If our label does not include a section, then any sections on the page should be considered a hit.\n",
    "    if \"#\" not in expected_source:\n",
    "        retrieved_sources = [source.split(\"#\")[0] for source in retrieved_sources]\n",
    "    \n",
    "    if expected_source in retrieved_sources:\n",
    "        is_hit = True\n",
    "        score = retrieved_nodes[retrieved_sources.index(expected_source)].score\n",
    "    else:\n",
    "        is_hit = False\n",
    "        score = 0.0\n",
    "    \n",
    "    result = {\n",
    "        \"is_hit\": is_hit,\n",
    "        \"score\": score,\n",
    "        \"retrieved\": retrieved_sources,\n",
    "        \"expected\": expected_source,\n",
    "        \"query\": query,\n",
    "    }\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12ecc64-efae-4a67-86de-3f3fbb36820b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1ddd39-f141-4585-93a4-593daa146f60",
   "metadata": {},
   "source": [
    "Let's see how well our retriever does. It's not great right now, but we now have a solid metric to evaluate our retriever for future optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7d03f6-76bf-414d-a072-2862bdc2c1a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_hits = sum(result[\"is_hit\"] for result in results)\n",
    "hit_percentage = total_hits / len(results)\n",
    "hit_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069c754b-30d9-432a-98f0-1bcec9d0fe99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "average_score = sum(result[\"score\"] for result in results) / len(results)\n",
    "average_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e852dd79-247f-47d3-8ffa-97f79781a68a",
   "metadata": {},
   "source": [
    "## End-to-end evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6193ec2a-3a68-412b-bbb9-997e57b27edf",
   "metadata": {},
   "source": [
    "While we can evaluate our retriever in isolation, ultimately we want to evaluate our RAG application end-to-end, which includes the final response generated from our LLM.\n",
    "\n",
    "To effectively evaluate our generated responses, we need \"ground truth\" responses. These ground truth responses can be generated by feeding the correct context to a \"golden\" LLM. Then, we can use an LLM to evaluate our generated responses compared to the ground truth responses.\n",
    "\n",
    "<span style=\"background: yellow; color: red; font-size: 1rem;\"><b>DIAGRAM:</b></span> Show e2e evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da62c16b-605f-41cd-8894-b7f2e6ae460c",
   "metadata": {},
   "source": [
    "### Choosing a Golden LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0266f2-8a34-411e-a661-4c2fc8cbb5ef",
   "metadata": {},
   "source": [
    "To generate ground truth responses, and then to evaluate the generated responses vs. the ground truth, we need a \"golden\" LLM. But which LLM should we use? We now run into a problem: we need to determine the quality of different LLMs to choose as a \"golden\" LLM, but doing so requires a \"golden\" LLM. Leaderboards on general benchmarks provide a rough indication on which LLMs perform better, but in this case, we will go with the eye-test.\n",
    "\n",
    "Let's get responses from both GPT-4 and Llama2-70B and see for ourselves which one is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75423140-d93e-495a-8a3a-f1cb3cbb72db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def fetch_text_from_source(source: str):\n",
    "    url, anchor = source.split(\"#\") if \"#\" in source else (source, None)\n",
    "    file_path = Path(\"/efs/shared_storage/amog/\", url.split(\"https://\")[-1])\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        html_content = file.read()\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    if anchor:\n",
    "        target_element = soup.find(id=anchor)\n",
    "        if target_element:\n",
    "            text = target_element.get_text()\n",
    "        else:\n",
    "            return fetch_text_from_source(source=url)\n",
    "    else:\n",
    "        text = soup.get_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257763e2-d016-4ddb-8e5b-f06f7f520eba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_source = data[0][\"source\"]\n",
    "print(example_source)\n",
    "\n",
    "text = fetch_text_from_source(example_source)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b223954-2d09-4013-a9e9-7473073c66c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Content for inference\n",
    "system_content = \"Answer the query using the context provided.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8ffa65-6e0d-49d0-92c3-4ef73d1573f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def generate_response(llm_name, \n",
    "                      temperature, \n",
    "                      system_content, \n",
    "                      user_content):\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=llm_name,\n",
    "        temperature=temperature,\n",
    "        messages=[\n",
    "                {\"role\": \"system\", \"content\": system_content},\n",
    "                {\"role\": \"assistant\", \"content\": \"\"},\n",
    "                {\"role\": \"user\", \"content\": user_content},\n",
    "            ],)\n",
    "    return response[\"choices\"][-1][\"message\"][\"content\"]\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4747619-5678-4e8a-b59e-0576322ad727",
   "metadata": {},
   "source": [
    "Let's get responses from gpt-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e97c5a-2ed5-4e91-964d-80d2f7663107",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt4_responses = []\n",
    "llm_name = \"gpt-4\"\n",
    "max_context_length = 8192\n",
    "\n",
    "openai.api_base = os.environ[\"OPENAI_API_BASE\"]\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "for entry in data[:5]:\n",
    "    query = entry[\"question\"]\n",
    "    source = entry[\"source\"]\n",
    "    context = fetch_text_from_source(source)\n",
    "    \n",
    "    context_length = max_context_length - len(system_content)\n",
    "    user_content = f\"The query is {query} and the additional context is {context}\"[:context_length]\n",
    "    \n",
    "    response = generate_response(llm_name, temperature=0.0, system_content=system_content, user_content=user_content)\n",
    "    gpt4_responses.append(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0230ccc5-a2d2-475a-9ca7-cb594d99c9f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt4_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dee70c-4b84-435b-9dc9-463ab4ce5d5e",
   "metadata": {},
   "source": [
    "Now let's get responses from LLama2-70b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3433a5b-cbcb-44bc-8049-36c847bba3c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_responses = []\n",
    "llm_name = \"meta-llama/Llama-2-70b-chat-hf\"\n",
    "max_context_length = 4096\n",
    "\n",
    "openai.api_base = os.environ[\"ANYSCALE_API_BASE\"]\n",
    "openai.api_key = os.environ[\"ANYSCALE_API_KEY\"]\n",
    "\n",
    "for entry in data[:5]:\n",
    "    query = entry[\"question\"]\n",
    "    source = entry[\"source\"]\n",
    "    context = fetch_text_from_source(source)\n",
    "    \n",
    "    context_length = max_context_length - len(system_content)\n",
    "    user_content = f\"The query is {query} and the additional context is {context}\"[:context_length]\n",
    "    \n",
    "    response = generate_response(llm_name, temperature=0.0, system_content=system_content, user_content=user_content)\n",
    "    llama_responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39e4fd2-c6e7-4ba8-837f-543b1087d00d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df65c767-a3f5-4987-a7e6-58067b16c351",
   "metadata": {},
   "source": [
    "Now let's compare the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d441a97-f5c6-451c-9852-4cf13328a466",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BOLD = '\\033[1m'\n",
    "END = '\\033[0m'\n",
    "    \n",
    "for query, gpt_response, llama_response in zip(data[:5], gpt4_responses, llama_responses):\n",
    "    print(f\"{BOLD}Query:{END} {query['question']}\")\n",
    "    print(f\"{BOLD}GPT4 answer:{END} {gpt_response}\")\n",
    "    print(f\"{BOLD}Llama2-70B answer:{END} {llama_response}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e3fc5d-13bb-4314-b148-ab4be5b98dcb",
   "metadata": {},
   "source": [
    "Based on these answers, we go with GPT-4 as our \"golden\" LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a6cfa4-8f59-43e7-a694-843b9613b1b3",
   "metadata": {},
   "source": [
    "### Generating our Golden Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f5c2a6-a5ac-4b9c-ab45-61d444b97b9b",
   "metadata": {},
   "source": [
    "Now that we have chosen which LLM to use, we can generate our reference responses. Let's generate 10 reference responses and save them to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5981d038-4b6c-4a97-8e05-ccf9121ade7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "golden_responses = []\n",
    "llm_name = \"gpt-4\"\n",
    "max_context_length = 8192\n",
    "\n",
    "openai.api_base = os.environ[\"OPENAI_API_BASE\"]\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "ten_samples = data[:10]\n",
    "\n",
    "for entry in ten_samples:\n",
    "    query = entry[\"question\"]\n",
    "    source = entry[\"source\"]\n",
    "    context = fetch_text_from_source(source)\n",
    "    \n",
    "    context_length = max_context_length - len(system_content)\n",
    "    user_content = f\"The query is {query} and the additional context is {context}\"[:context_length]\n",
    "    \n",
    "    response = generate_response(llm_name, temperature=0.0, system_content=system_content, user_content=user_content)\n",
    "    golden_responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc8506c-0deb-43ae-9936-cd323635cc44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reference_dataset = [{\"question\": entry[\"question\"], \"source\": entry[\"source\"], \"response\": response} for entry, response in zip(ten_samples, golden_responses)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23632aab-2be3-49cb-905b-1b26d7bde46a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"golden-responses.json\", \"w\") as file:\n",
    "    json.dump(reference_dataset, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dbf6c6-74ea-4802-be16-f96f2982a642",
   "metadata": {},
   "source": [
    "## Evaluating our Query Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef0af94-f983-464f-8a4b-a91ff72d7f87",
   "metadata": {},
   "source": [
    "Once we have reference responses, we can get our generated responses from our query engine. Then pass both responses to our golden LLM to evaluate the responses from our application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd15459b-a679-4f1b-832c-bb38cd467d0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"golden-responses.json\", \"r\") as file:\n",
    "    golden_responses = json.load(file)\n",
    "golden_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a53891-cd44-4efb-b9f9-9e3cc7de3a6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import get_query_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adebada-bce2-4600-ac2f-4ba541ce32f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "openai.api_base = os.environ[\"ANYSCALE_API_BASE\"]\n",
    "openai.api_key = os.environ[\"ANYSCALE_API_KEY\"]\n",
    "query_engine = get_query_engine(similarity_top_k=5)\n",
    "\n",
    "# Store both the original response object and the response string.\n",
    "rag_responses = []\n",
    "rag_response_str = []\n",
    "\n",
    "for entry in golden_responses:\n",
    "    query = entry[\"question\"]\n",
    "    response = query_engine.query(query)\n",
    "    rag_responses.append(response)\n",
    "    rag_response_str.append(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17e76f6-7333-4ea7-9357-3bc125c59cf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rag_response_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d8690c-d80a-4921-a182-f6c809792e7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluation prompt\n",
    "system_content = \"\"\"\n",
    "    \"You are given a query, a reference answer, and a candidate answer.\n",
    "    You must {score} the candidate answer between 1 and 5 on how well it answers the query, \n",
    "    using the reference answer as the golden truth.\n",
    "    You must return your response in a line with only the score.\n",
    "    Do not add any more details.\n",
    "    On a separate line provide your {reasoning} for the score as well.\n",
    "    Return your response following the exact format outlined below.\n",
    "    All of this must be in a valid JSON format.\n",
    "    \n",
    "    {\"score\": score,\n",
    "     \"reasoning\": reasoning}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccde762-18cb-4912-af78-ccf2697164fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluation_scores = []\n",
    "llm_name = \"gpt-4\"\n",
    "max_context_length = 8192\n",
    "\n",
    "openai.api_base = os.environ[\"OPENAI_API_BASE\"]\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "for rag_response, golden_response in zip(rag_response_str, golden_responses):\n",
    "    query = golden_response[\"question\"]\n",
    "    golden_answer = golden_response[\"response\"]\n",
    "    generated_answer = rag_response\n",
    "    \n",
    "    context_length = max_context_length - len(system_content)\n",
    "    user_content = f\"The query is {query}, the reference answer is {golden_answer}, and the candidate answer is {generated_answer}\"[:context_length]\n",
    "    \n",
    "    response = generate_response(llm_name, temperature=0.0, system_content=system_content, user_content=user_content)\n",
    "    evaluation_scores.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf863f46-a74e-4005-a844-eaa764163686",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluation_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c958b4c-7be8-4f95-ad1c-3c8141b445e1",
   "metadata": {},
   "source": [
    "Now let's parse the score and the response from the LLM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2c690c-b7e5-4a96-8342-5050ec6c5b06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_evaluation_score(evaluation_result):\n",
    "    # Define regular expressions for extracting values\n",
    "    score_pattern = r'\"score\"\\s*:\\s*([0-9]+)'\n",
    "    reasoning_pattern = r'\"reasoning\"\\s*:\\s*\"([^\"]*)\"'\n",
    "\n",
    "    # Extract values using regular expressions\n",
    "    score_match = re.search(score_pattern, evaluation_result)\n",
    "    reasoning_match = re.search(reasoning_pattern, evaluation_result)\n",
    "\n",
    "    # Convert\n",
    "    if score_match and reasoning_match:\n",
    "        score = float(score_match.group(1))\n",
    "        reasoning = reasoning_match.group(1)\n",
    "        return {\"score\": score, \"reasoning\": reasoning}\n",
    "\n",
    "    return {\"score\": \"\", \"reasoning\": \"\"}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5feeaac-bb4c-4130-8213-16c0bd3641bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parsed_evaluation_scores = [parse_evaluation_score(result) for result in evaluation_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9118c3b8-c42f-4b54-8eef-9c96908cd9cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parsed_evaluation_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3190240d-7bb1-4079-8bfa-0d6d0ed9166d",
   "metadata": {},
   "source": [
    "Let's save the query, both responses, and the score to a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc98a6d-a27a-4a42-a5c5-4574a336e679",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores = [\n",
    "    {\"question\": golden_response[\"question\"],\n",
    "     \"golden_response\": golden_response[\"response\"],\n",
    "     \"generated_response\": rag_response,\n",
    "     \"score\": parsed_score[\"score\"],\n",
    "     \"reasoning\": parsed_score[\"reasoning\"],\n",
    "    }\n",
    "    for parsed_score, rag_response, golden_response in zip(parsed_evaluation_scores, rag_response_str, golden_responses)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1268855c-351f-4eef-9c4b-807c923c038a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00e6239-5c44-4c1a-89d5-7549fb0ad98a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"eval-scores.json\", \"w\") as file:\n",
    "    json.dump(scores, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095ef29b-b968-4449-9a50-4234a5b294ab",
   "metadata": {},
   "source": [
    "We can also calculate the average scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b7d3a9-d2c5-4381-b2d4-9c1aa1d21df1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "average_scores = sum(score[\"score\"] for score in scores) / len(scores)\n",
    "average_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0229cb-2256-4762-a772-55740e617951",
   "metadata": {},
   "source": [
    "## Alternative forms of evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43b1970-ded0-4ee4-b49e-49447fad1533",
   "metadata": {},
   "source": [
    "Generating reference responses and then using them for evaluation can give us a more accurate assesment on how our query engine is performing. However, this approach can be expensive. We have to make an initial pass through GPT4 to generate the reference response, and then we have to make another pass through GPT4 to evaluate our application's responses against the reference response.\n",
    "\n",
    "We can explore other evaluation metrics to get a better sense on how our query engine is performing, without needing to make multiple passes to GPT4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1905688d-9a0d-4e2a-81ea-40242f50905a",
   "metadata": {},
   "source": [
    "### Evaluating for hallucination/relevancy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6662a23a-8388-42cc-b986-0062afee3b48",
   "metadata": {},
   "source": [
    "One metric we can test is relevancy, which does not require generating reference responses. With this approach, we check to see if the generated response is relevant to at least one of the retrieved sources and to the query. This ensures that our LLM is not making up a response, but rather that it is relevant to the question that is being asked, and also that is relevant to at least one of the retrieved context.\n",
    "\n",
    "This does NOT check whether the response is a correct response.\n",
    "\n",
    "This capability is built into LlamaIndex, via the QueryResponseEvaluator. We use gpt-4 as the evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c767913f-fc0a-4437-932a-4a5106a5768c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.evaluation import QueryResponseEvaluator\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index import ServiceContext\n",
    "\n",
    "def evaluate_response_relevance(queries: list, responses: list):\n",
    "    openai.api_base = os.environ[\"OPENAI_API_BASE\"]\n",
    "    openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "    llm = OpenAI(model=\"gpt-4\", temperature=0.0)\n",
    "    service_context = ServiceContext.from_defaults(llm=llm)\n",
    "    evaluator = QueryResponseEvaluator(service_context=service_context)\n",
    "\n",
    "    evals = []\n",
    "    for query, response in zip(queries, responses):\n",
    "        evals.append(str(evaluator.evaluate(query, response)))\n",
    "    \n",
    "    return len([val == \"YES\" for val in evals]) / len(evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecd11db-ced6-433b-844d-03c6d0adc30a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "relevance_results = evaluate_response_relevance(queries=[sample[\"question\"] for sample in ten_samples], responses=rag_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4555fa-5901-49df-8f66-afbe00d27079",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "relevance_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
