{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c044803a-3fe2-4297-ad71-c93ae2e078f5",
   "metadata": {},
   "source": [
    "# Part 2: Evaluating our LLM application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc13cfb-5e8a-401d-bb04-24a5793e69be",
   "metadata": {},
   "source": [
    "So far, we've chosen typical/arbitrary values for the various parts of our RAG application. But if we were to change something, such as our chunking logic, embedding model, LLM, etc. how can we know that we have a better configuration than before. A generative task like this is very difficult to quantitatively assess and so we need to develop creative ways to do so. \n",
    "\n",
    "Because we have many moving parts in our application, we need to perform unit/component and end-to-end evaluation. Component-wise evaluation can involve evaluating our retrieval in isolation (is the best source in our set of retrieved chunks) and evaluating our LLMs response (given the best source, is the LLM able to produce a quality answer). As for end-to-end evaluation, we can assess the quality of the entire system (given all data, what is the quality of the response).\n",
    "\n",
    "<span style=\"background: yellow; color: red; font-size: 1rem;\"><b>DIAGRAM:</b></span> Component and end-to-end evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b67c46c-7650-484b-a792-7eaf62c0e82e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "119575d6-b0cc-49f3-a118-46bc3adf8189",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"ANYSCALE_API_BASE\"] = \"https://api.endpoints.anyscale.com/v1/chat/completions\"\n",
    "os.environ[\"ANYSCALE_API_KEY\"] = \"esecret_2hvvt43kbmpgzev7k2xqa9h6dv\"\n",
    "\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://api.openai.com/v1\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cedb0a-a7b3-4194-88a9-355122cd8a00",
   "metadata": {},
   "source": [
    "## Golden Context Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31583b8a-bd08-4054-95f6-2ae5256e6a21",
   "metadata": {},
   "source": [
    "In an ideal world, we would have a golden validation dataset: given a set of queries, we would have the correct sources that answer those queries, and optionally the correct answer that should be returned by the LLM.\n",
    "\n",
    "For this example, we have manually collected 177 representative user queries and identified the correct source in the documentation that answer those user queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8495831b-c4f9-4e84-a211-bc37ecf369e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "golden_dataset_path = Path(\"../datasets/eval-dataset-v1.jsonl\")\n",
    "\n",
    "with open(golden_dataset_path, \"r\") as f:\n",
    "    data = [json.loads(item) for item in list(f)]\n",
    "    \n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b187c2d5-5c4d-4b9a-9e5b-5f37c9e92b36",
   "metadata": {},
   "source": [
    "Our dataset contains 'question' and 'source' pairs. If we have a golden context dataset, it is the best option for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2300e0fc-3e65-4f43-98b6-3564bc1ccb3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'I’m struggling a bit with Ray Data type conversions when I do map_batches. Any advice?',\n",
       "  'source': 'https://docs.ray.io/en/master/data/transforming-data.html#configuring-batch-format'},\n",
       " {'question': 'How does autoscaling work in a Ray Serve application?',\n",
       "  'source': 'https://docs.ray.io/en/master/serve/scaling-and-resource-allocation.html#autoscaling'},\n",
       " {'question': 'how do I get the address of a ray node',\n",
       "  'source': 'https://docs.ray.io/en/master/ray-core/miscellaneous.html#node-information'},\n",
       " {'question': 'Does Ray support NCCL?',\n",
       "  'source': 'https://docs.ray.io/en/master/ray-more-libs/ray-collective.html'},\n",
       " {'question': 'Is Ray integrated with DeepSpeed?',\n",
       "  'source': 'https://docs.ray.io/en/master/ray-air/examples/gptj_deepspeed_fine_tuning.html#fine-tuning-the-model-with-ray-air-a-name-train-a'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6739202-90fa-46e8-bc7d-3c0ed33c8673",
   "metadata": {},
   "source": [
    "## Cold Start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20636b3e-5211-41c9-a390-0cfa23654c44",
   "metadata": {
    "tags": []
   },
   "source": [
    "We may not always have a prepared dataset of questions and the best source to answer that question readily available. To address this cold start problem, we could use an LLM to look at our documents and generate questions that the specific chunk would answer. This provides us with quality questions and the exact source the answer is in. However, this dataset generation method could be a bit noisy. The generate questions may not always be resembling of what your users may ask and the specific chunk we say is the best source may also have that exact information in other chunks. Nonetheless, this is a great way to start our development process while we collect + manually label a high quality dataset.\n",
    "\n",
    "<span style=\"background: yellow; color: red; font-size: 1rem;\"><b>DIAGRAM:</b></span> Show the synthetic data generation process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0192cc-40f7-4fe6-8175-25cedd51d9a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "We need to define a few parameters first.  \n",
    "- Notably, the chunk size determines the size of the text chunk shown to the LLM when generating hypothetical question & answer pairs. This must be set below the context window limitation of the chosen LLM.\n",
    "- We choose a subsample ratio since we just want to construct a small representative subset for the purpose of evaluation and iteration. (We choose an even smaller subset for the purpose of the demonstration here).\n",
    "- We use `gpt-3.5-turbo` since it's fast and cheap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff6951de-bf96-4e1a-a666-893e62f3eab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "RAY_DOCS_DIRECTORY = Path(\"/efs/shared_storage/amog/docs.ray.io/en/master/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cd3470-8542-4323-a70d-c3cc9fdb04a8",
   "metadata": {},
   "source": [
    "First, we load in the documents and chunk them to the appropriate sizes, creating LlamaIndex nodes. We already did the data processing in part 1, and have packaged the logic as a utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "790da50a-24c8-4d5f-8395-fb225f1c49fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-18 13:59:24,133\tINFO streaming_executor.py:93 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[FlatMap(extract_sections)] -> TaskPoolMapOperator[FlatMap(chunk_document)]\n",
      "2023-09-18 13:59:24,134\tINFO streaming_executor.py:94 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-09-18 13:59:24,134\tINFO streaming_executor.py:96 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from data import create_nodes\n",
    "\n",
    "# needs to be smaller than context window\n",
    "CHUNK_SIZE = 1024\n",
    "\n",
    "nodes = create_nodes(RAY_DOCS_DIRECTORY, chunk_size=CHUNK_SIZE, chunk_overlap=20).take_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5dcc6135-03b1-43b3-8c0a-155b8eff3037",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [node_dict[\"node\"] for node_dict in nodes]\n",
    "id_to_node = {node.node_id: node for node in nodes}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9f820b-bc94-40a4-8e66-c78d5c2120b9",
   "metadata": {},
   "source": [
    "Now, we subsample the nodes to obtain a representative subset (here we use a very small subset for a fast demonstration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cada634d-fc1e-4b1a-b1d5-d2266a518090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsampled 6836 nodes into 68 nodes\n"
     ]
    }
   ],
   "source": [
    "from utils import subsample\n",
    "\n",
    "SUBSAMPLE_RATIO = 0.01\n",
    "\n",
    "subsampled_nodes = subsample(nodes, SUBSAMPLE_RATIO)\n",
    "print('Subsampled {} nodes into {} nodes'.format(len(nodes), len(subsampled_nodes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5856201-06db-4292-990c-21b1010e12c0",
   "metadata": {},
   "source": [
    "Now, we use LlamaIndex's built in utility `generate_qa_embedding_pairs` to create synthetic query/context pairs.\n",
    "\n",
    "(We can also use this utility for fine-tuning embeddings, hence the naming. More on this in part 3!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d718383c-74a2-4518-a51b-69a7857c5cde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [02:18<00:00,  2.04s/it]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.finetuning import generate_qa_embedding_pairs\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model='gpt-3.5-turbo')\n",
    "synthetic_dataset = generate_qa_embedding_pairs(subsampled_nodes, llm=llm, num_questions_per_chunk=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87212c89-9356-45c9-8608-552b350b7e96",
   "metadata": {},
   "source": [
    "Now we will transform the shape of the data a bit to match the format of our labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bc5d468a-6a6e-468d-ae9d-95297dd2c860",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "synthetic_data = []\n",
    "for query_id, context_ids in synthetic_dataset.relevant_docs.items():\n",
    "    query = synthetic_dataset.queries[query_id]\n",
    "    golden_context = id_to_node[context_ids[0]].metadata['source']\n",
    "    entry = {\n",
    "        'question': query,\n",
    "        'source': golden_context,\n",
    "    }\n",
    "    synthetic_data.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "486593e8-f24a-4278-8704-9845a90adbb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'How can Tune experiments be stopped using metrics, trial errors, and early stopping schedulers?',\n",
       "  'source': 'https://docs.ray.io/en/master/tune/tutorials/tune-stopping.html#summary'},\n",
       " {'question': 'What steps can be taken to resume an experiment that was manually interrupted or experienced unexpected cluster failure while trials were still running?',\n",
       "  'source': 'https://docs.ray.io/en/master/tune/tutorials/tune-stopping.html#summary'},\n",
       " {'question': 'What is the purpose of the `wait_for_gpu` function in the given context?',\n",
       "  'source': 'https://docs.ray.io/en/master/tune/api/doc/ray.tune.utils.wait_for_gpu.html#ray-tune-utils-wait-for-gpu'},\n",
       " {'question': 'How can the `wait_for_gpu` function be used in the example code provided?',\n",
       "  'source': 'https://docs.ray.io/en/master/tune/api/doc/ray.tune.utils.wait_for_gpu.html#ray-tune-utils-wait-for-gpu'},\n",
       " {'question': 'What is the purpose of the `sync_down` function in the `Syncer` class?',\n",
       "  'source': 'https://docs.ray.io/en/master/tune/api/doc/ray.tune.syncer.Syncer.sync_down.html#ray-tune-syncer-syncer-sync-down'}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1bd61e90-0b15-4cca-92ec-61b76804bedd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import write_jsonl\n",
    "\n",
    "write_jsonl(\"../datasets/synthetic-eval-dataset.jsonl\", synthetic_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64373866-d0f7-4704-b03b-5c466fc59afb",
   "metadata": {},
   "source": [
    "Since we already have a dataset with representative user queries and ground truth labels, we will use that for evaluation instead of a synthetically generated dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330a7e10-18fc-495e-ab4a-da2f3ca00a97",
   "metadata": {},
   "source": [
    "## Evaluating Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6e7e79-5a2a-4247-bc3a-d093d0416761",
   "metadata": {},
   "source": [
    "The first component to evaluate in our RAG application is retrieval. Given a query, is our retriever pulling in the correct context to answer that query? Regardless of how good our LLM is, if it does not have the right context to answer the question, it cannot provide the right answer.\n",
    "\n",
    "We can use our golden context dataset to evaluate retrieval. The simplest approach is that for each query in our dataset, we can test to see if the correct source is included in any of the chunks that are retrieved by our retriever. This measures \"hit rate\".\n",
    "\n",
    "However, simply checking for existence can be misleading if we increase the number of chunks that we retrieve. Therefore, we also want to check the score that our retriever gives for the correct source. A higher score means our retriever is accurately determining the correct context. \n",
    "\n",
    "To summarize, for each query in our evaluation dataset, we will measure the following:\n",
    "1. Is the correct source included in any of the retrived chunks?\n",
    "2. What is the score our retriever gives to the correct source?\n",
    "\n",
    "<span style=\"background: yellow; color: red; font-size: 1rem;\"><b>DIAGRAM:</b></span> Show retrieval evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a5958c-74d8-4a11-b46c-a12a8fd5ad99",
   "metadata": {},
   "source": [
    "First, let's a get a retriever over the vector database. We have packaged this as a utility. It is the same as we did in notebook 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b88e0d5f-a0ed-47e9-afcf-0ac882201e57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import get_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dad0afdc-2f44-4ca7-b5c0-2c6499f4f35e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "retriever = get_retriever(similarity_top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3b5438-9930-4857-a9a5-165cbd15c79b",
   "metadata": {},
   "source": [
    "Now let's evaluate our retriever. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bb4cd2ae-17ff-4e75-904f-a577499bc8ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for entry in data:\n",
    "    query = entry[\"question\"]\n",
    "    expected_source = entry['source']\n",
    "    \n",
    "    retrieved_nodes = retriever.retrieve(query)\n",
    "    retrieved_sources = [node.metadata['source'] for node in retrieved_nodes]\n",
    "    \n",
    "    # If our label does not include a section, then any sections on the page should be considered a hit.\n",
    "    if \"#\" not in expected_source:\n",
    "        retrieved_sources = [source.split(\"#\")[0] for source in retrieved_sources]\n",
    "    \n",
    "    if expected_source in retrieved_sources:\n",
    "        is_hit = True\n",
    "        score = retrieved_nodes[retrieved_sources.index(expected_source)].score\n",
    "    else:\n",
    "        is_hit = False\n",
    "        score = 0.0\n",
    "    \n",
    "    result = {\n",
    "        \"is_hit\": is_hit,\n",
    "        \"score\": score,\n",
    "        \"retrieved\": retrieved_sources,\n",
    "        \"expected\": expected_source,\n",
    "        \"query\": query,\n",
    "    }\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a12ecc64-efae-4a67-86de-3f3fbb36820b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'is_hit': True,\n",
       "  'score': 0.9110969673181731,\n",
       "  'retrieved': ['https://docs.ray.io/en/master/data/transforming-data.html#configuring-batch-format',\n",
       "   'https://docs.ray.io/en/master/data/working-with-tensors.html#transforming-tensor-data',\n",
       "   'https://docs.ray.io/en/master/data/working-with-pytorch.html#transformations-with-torch-tensors',\n",
       "   'https://docs.ray.io/en/master/data/examples/pytorch_resnet_batch_prediction.html#preprocessing',\n",
       "   'https://docs.ray.io/en/master/data/transforming-data.html#transforming-batches-with-tasks'],\n",
       "  'expected': 'https://docs.ray.io/en/master/data/transforming-data.html#configuring-batch-format',\n",
       "  'query': 'I’m struggling a bit with Ray Data type conversions when I do map_batches. Any advice?'},\n",
       " {'is_hit': True,\n",
       "  'score': 0.9274146984119639,\n",
       "  'retrieved': ['https://docs.ray.io/en/master/serve/architecture.html#ray-serve-autoscaling',\n",
       "   'https://docs.ray.io/en/master/cluster/key-concepts.html#autoscaling',\n",
       "   'https://docs.ray.io/en/master/serve/scaling-and-resource-allocation.html#autoscaling',\n",
       "   'https://docs.ray.io/en/master/cluster/vms/user-guides/configuring-autoscaling.html#configuring-autoscaling',\n",
       "   'https://docs.ray.io/en/master/cluster/kubernetes/user-guides/configuring-autoscaling.html#kuberay-autoscaling'],\n",
       "  'expected': 'https://docs.ray.io/en/master/serve/scaling-and-resource-allocation.html#autoscaling',\n",
       "  'query': 'How does autoscaling work in a Ray Serve application?'}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1ddd39-f141-4585-93a4-593daa146f60",
   "metadata": {},
   "source": [
    "Let's see how well our retriever does. It's not great right now, but we now have a solid metric to evaluate our retriever for future optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "df7d03f6-76bf-414d-a072-2862bdc2c1a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4406779661016949"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_hits = sum(result[\"is_hit\"] for result in results)\n",
    "hit_percentage = total_hits / len(results)\n",
    "hit_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "069c754b-30d9-432a-98f0-1bcec9d0fe99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39457275827305033"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_score = sum(result[\"score\"] for result in results) / len(results)\n",
    "average_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e852dd79-247f-47d3-8ffa-97f79781a68a",
   "metadata": {},
   "source": [
    "## End-to-end evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6193ec2a-3a68-412b-bbb9-997e57b27edf",
   "metadata": {},
   "source": [
    "While we can evaluate our retriever in isolation, ultimately we want to evaluate our RAG application end-to-end, which includes the final response generated from our LLM.\n",
    "\n",
    "To effectively evaluate our generated responses, we need \"ground truth\" responses. These ground truth responses can be generated by feeding the correct context to a \"golden\" LLM. Then, we can use an LLM to evaluate our generated responses compared to the ground truth responses.\n",
    "\n",
    "<span style=\"background: yellow; color: red; font-size: 1rem;\"><b>DIAGRAM:</b></span> Show e2e evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da62c16b-605f-41cd-8894-b7f2e6ae460c",
   "metadata": {},
   "source": [
    "### Choosing a Golden LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0266f2-8a34-411e-a661-4c2fc8cbb5ef",
   "metadata": {},
   "source": [
    "To generate ground truth responses, and then to evaluate the generated responses vs. the ground truth, we need a \"golden\" LLM. But which LLM should we use? We now run into a problem: we need to determine the quality of different LLMs to choose as a \"golden\" LLM, but doing so requires a \"golden\" LLM. Leaderboards on general benchmarks provide a rough indication on which LLMs perform better, but in this case, we will go with the eye-test.\n",
    "\n",
    "Let's get responses from both GPT-4 and Llama2-70B and see for ourselves which one is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "75423140-d93e-495a-8a3a-f1cb3cbb72db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def fetch_text_from_source(source: str):\n",
    "    url, anchor = source.split(\"#\") if \"#\" in source else (source, None)\n",
    "    file_path = Path(\"/efs/shared_storage/amog/\", url.split(\"https://\")[-1])\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        html_content = file.read()\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    if anchor:\n",
    "        target_element = soup.find(id=anchor)\n",
    "        if target_element:\n",
    "            text = target_element.get_text()\n",
    "        else:\n",
    "            return fetch_text_from_source(source=url)\n",
    "    else:\n",
    "        text = soup.get_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e578a402-99d5-4dbd-804e-e17c24518744",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://docs.ray.io/en/master/data/transforming-data.html#configuring-batch-format\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nConfiguring batch format#\\nRay Data represents batches as dicts of NumPy ndarrays or pandas DataFrames. By\\ndefault, Ray Data represents batches as dicts of NumPy ndarrays.\\nTo configure the batch type, specify batch_format in\\nmap_batches(). You can return either format from your function.\\n\\n\\n\\nNumPy\\nfrom typing import Dict\\nimport numpy as np\\nimport ray\\n\\ndef increase_brightness(batch: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\\n    batch[\"image\"] = np.clip(batch[\"image\"] + 4, 0, 255)\\n    return batch\\n\\nds = (\\n    ray.data.read_images(\"s3://[email\\xa0protected]/image-datasets/simple\")\\n    .map_batches(increase_brightness, batch_format=\"numpy\")\\n)\\n\\n\\n\\n\\n\\npandas\\nimport pandas as pd\\nimport ray\\n\\ndef drop_nas(batch: pd.DataFrame) -> pd.DataFrame:\\n    return batch.dropna()\\n\\nds = (\\n    ray.data.read_csv(\"s3://[email\\xa0protected]/iris.csv\")\\n    .map_batches(drop_nas, batch_format=\"pandas\")\\n)\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_source = data[0][\"source\"]\n",
    "print(example_source)\n",
    "\n",
    "text = fetch_text_from_source(example_source)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eed04e2f-ffcd-446d-9c16-4cb9c3f226ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.response_synthesizers import get_response_synthesizer\n",
    "from llama_index.schema import TextNode, NodeWithScore\n",
    "\n",
    "def generate_responses(entries, llm, context_window=None):\n",
    "    service_context = ServiceContext.from_defaults(llm=llm, context_window=context_window)\n",
    "    rs = get_response_synthesizer(service_context=service_context)\n",
    "\n",
    "    responses = []\n",
    "    for entry in tqdm(entries):\n",
    "        query = entry[\"question\"]\n",
    "        source = entry[\"source\"]\n",
    "\n",
    "        context = fetch_text_from_source(source)\n",
    "        nodes = [NodeWithScore(node=TextNode(text=context))]\n",
    "\n",
    "        response = rs.synthesize(query, nodes=nodes)\n",
    "        responses.append(response.response)\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4747619-5678-4e8a-b59e-0576322ad727",
   "metadata": {},
   "source": [
    "Let's get responses from gpt-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e4e97c5a-2ed5-4e91-964d-80d2f7663107",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:45<00:00, 15.26s/it]\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(model='gpt-4', temperature=0.0)\n",
    "gpt4_responses = generate_responses(data[:3], llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0230ccc5-a2d2-475a-9ca7-cb594d99c9f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sure, when using the `map_batches()` function in Ray Data, you can specify the batch format by using the `batch_format` argument. If you want to work with NumPy ndarrays, you can set `batch_format=\"numpy\"`. For example, if you have a function that increases the brightness of an image, you can use it like this:\\n\\n```python\\nfrom typing import Dict\\nimport numpy as np\\nimport ray\\n\\ndef increase_brightness(batch: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\\n    batch[\"image\"] = np.clip(batch[\"image\"] + 4, 0, 255)\\n    return batch\\n\\nds = (\\n    ray.data.read_images(\"s3://[email\\xa0protected]/image-datasets/simple\")\\n    .map_batches(increase_brightness, batch_format=\"numpy\")\\n)\\n```\\n\\nOn the other hand, if you prefer to work with pandas DataFrames, you can set `batch_format=\"pandas\"`. For instance, if you have a function that drops NA values from a DataFrame, you can use it like this:\\n\\n```python\\nimport pandas as pd\\nimport ray\\n\\ndef drop_nas(batch: pd.DataFrame) -> pd.DataFrame:\\n    return batch.dropna()\\n\\nds = (\\n    ray.data.read_csv(\"s3://[email\\xa0protected]/iris.csv\")\\n    .map_batches(drop_nas, batch_format=\"pandas\")\\n)\\n```\\n\\nRemember, by default, Ray Data represents batches as dicts of NumPy ndarrays.',\n",
       " 'In a Ray Serve application, autoscaling works by adjusting to traffic spikes through observing queue sizes and making decisions to add or remove replicas. This is achieved by configuring the autoscaling_config field in the deployment options. The configuration includes parameters such as min_replicas, max_replicas, initial_replicas, and target_num_ongoing_requests_per_replica. \\n\\nThe min_replicas and max_replicas fields define the range of replicas from which the autoscaler can choose. The initial_replicas field indicates the number of replicas at the start of the deployment, and if not specified, it defaults to min_replicas. The target_num_ongoing_requests_per_replica parameter determines how aggressively the autoscaler should react to traffic. \\n\\nThe Ray Serve Autoscaler operates at the application level and sits on top of the Ray Autoscaler. It requests Ray to start a number of replica actors based on the request demand. If the Ray Autoscaler determines there aren’t enough available CPUs to place these actors, it requests more Ray nodes from the underlying cloud provider. When Ray Serve scales down and terminates some replica actors, it tries to do so in a way that results in the most nodes having no Ray actors or tasks running on them, at which point the Ray autoscaler will remove those nodes.\\n\\nThere are also parameters like downscale_delay_s and upscale_delay_s that control the frequency of autoscaling work, and smoothing_factor to speed up or slow down each autoscaling step. The metrics_interval_s parameter controls how often each replica sends metrics to the autoscaler.',\n",
       " \"You can get the address of a Ray node by accessing the 'NodeManagerAddress' field of the node's information. This field contains the private IP of the node that the raylet is on.\"]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt4_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dee70c-4b84-435b-9dc9-463ab4ce5d5e",
   "metadata": {},
   "source": [
    "Now let's get responses from LLama2-70b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c3433a5b-cbcb-44bc-8049-36c847bba3c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:34<00:00, 11.49s/it]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms import Anyscale\n",
    "from llama_index import ServiceContext\n",
    "\n",
    "llm = Anyscale(model='meta-llama/Llama-2-70b-chat-hf', temperature=0.0)\n",
    "llama_responses = generate_responses(data[:3], llm, context_window=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b39e4fd2-c6e7-4ba8-837f-543b1087d00d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' It sounds like you\\'re having trouble with converting data types when using the `map_batches` function in Ray Data. Specifically, you\\'re mentioning issues with NumPy arrays and pandas DataFrames.\\n\\nOne thing to keep in mind is that Ray Data represents batches as dictionaries of NumPy arrays or pandas DataFrames by default. This means that when you\\'re working with batches, you\\'ll need to specify the correct data type when passing them to functions that operate on batches.\\n\\nOne way to do this is by using the `batch_format` parameter in the `map_batches` function. This parameter allows you to specify whether the batches should be represented as NumPy arrays or pandas DataFrames.\\n\\nFor example, in the code snippet you provided, the `batch_format` parameter is set to `\"numpy\"` when reading images from an S3 bucket. This tells Ray Data to expect batches to be represented as NumPy arrays. Similarly, when reading a CSV file, the `batch_format` parameter is set to `\"pandas\"` to indicate that batches should be represented as pandas DataFrames.\\n\\nWhen writing functions that operate on batches, you\\'ll',\n",
       " \" Autoscaling in a Ray Serve application is handled by the Ray Serve Autoscaler, which is an application-level autoscaler that sits on top of the Ray Autoscaler. The autoscaler observes queue sizes and makes scaling decisions to add or remove replicas based on the demand. The autoscaling algorithm takes into consideration several user-specified parameters, such as min_replicas, max_replicas, target_num_ongoing_requests_per_replica, downscale_delay_s, upscale_delay_s, and smoothing_factor. These parameters control the minimum and maximum number of replicas, the target number of ongoing requests per replica, and the delay before scaling up or down. The autoscaler also takes into account the available cluster resources and relies on the Ray Autoscaler to scale up more nodes when necessary. The autoscaler decides to scale up or down by comparing the target number of ongoing requests per replica to the number of running and pending tasks on each replica. The autoscaler's response can be amplified or dampened by adjust\",\n",
       " \" You can get the address of a Ray node by using the `ray.nodes()` method and looking for the `NodeManagerAddress` field in the response. This field contains the private IP address of the node that the Raylet is running on.\\n\\nFor example, in the provided context information, the address of the Ray node with NodeID '2691a0c1aed6f45e262b2372baf58871734332d7' is '192.168.1.82'.\\n\\nYou can also use the `ray.get_node()` method to get information about a specific node, including its address. For example:\\n```\\nnode_info = ray.get_node(NodeID='2691a0c1aed6f45e262b2372baf58871734332d7')\\nprint(node_info['NodeManagerAddress'])\\n```\\nThis will print the private IP address of the specified node.\"]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df65c767-a3f5-4987-a7e6-58067b16c351",
   "metadata": {},
   "source": [
    "Now let's compare the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1d441a97-f5c6-451c-9852-4cf13328a466",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mQuery:\u001b[0m I’m struggling a bit with Ray Data type conversions when I do map_batches. Any advice?\n",
      "\u001b[1mGPT4 answer:\u001b[0m Sure, when using the `map_batches()` function in Ray Data, you can specify the batch format by using the `batch_format` argument. If you want to work with NumPy ndarrays, you can set `batch_format=\"numpy\"`. For example, if you have a function that increases the brightness of an image, you can use it like this:\n",
      "\n",
      "```python\n",
      "from typing import Dict\n",
      "import numpy as np\n",
      "import ray\n",
      "\n",
      "def increase_brightness(batch: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
      "    batch[\"image\"] = np.clip(batch[\"image\"] + 4, 0, 255)\n",
      "    return batch\n",
      "\n",
      "ds = (\n",
      "    ray.data.read_images(\"s3://[email protected]/image-datasets/simple\")\n",
      "    .map_batches(increase_brightness, batch_format=\"numpy\")\n",
      ")\n",
      "```\n",
      "\n",
      "On the other hand, if you prefer to work with pandas DataFrames, you can set `batch_format=\"pandas\"`. For instance, if you have a function that drops NA values from a DataFrame, you can use it like this:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import ray\n",
      "\n",
      "def drop_nas(batch: pd.DataFrame) -> pd.DataFrame:\n",
      "    return batch.dropna()\n",
      "\n",
      "ds = (\n",
      "    ray.data.read_csv(\"s3://[email protected]/iris.csv\")\n",
      "    .map_batches(drop_nas, batch_format=\"pandas\")\n",
      ")\n",
      "```\n",
      "\n",
      "Remember, by default, Ray Data represents batches as dicts of NumPy ndarrays.\n",
      "\u001b[1mLlama2-70B answer:\u001b[0m  It sounds like you're having trouble with converting data types when using the `map_batches` function in Ray Data. Specifically, you're mentioning issues with NumPy arrays and pandas DataFrames.\n",
      "\n",
      "One thing to keep in mind is that Ray Data represents batches as dictionaries of NumPy arrays or pandas DataFrames by default. This means that when you're working with batches, you'll need to specify the correct data type when passing them to functions that operate on batches.\n",
      "\n",
      "One way to do this is by using the `batch_format` parameter in the `map_batches` function. This parameter allows you to specify whether the batches should be represented as NumPy arrays or pandas DataFrames.\n",
      "\n",
      "For example, in the code snippet you provided, the `batch_format` parameter is set to `\"numpy\"` when reading images from an S3 bucket. This tells Ray Data to expect batches to be represented as NumPy arrays. Similarly, when reading a CSV file, the `batch_format` parameter is set to `\"pandas\"` to indicate that batches should be represented as pandas DataFrames.\n",
      "\n",
      "When writing functions that operate on batches, you'll\n",
      "\n",
      "\n",
      "\u001b[1mQuery:\u001b[0m How does autoscaling work in a Ray Serve application?\n",
      "\u001b[1mGPT4 answer:\u001b[0m In a Ray Serve application, autoscaling works by adjusting to traffic spikes through observing queue sizes and making decisions to add or remove replicas. This is achieved by configuring the autoscaling_config field in the deployment options. The configuration includes parameters such as min_replicas, max_replicas, initial_replicas, and target_num_ongoing_requests_per_replica. \n",
      "\n",
      "The min_replicas and max_replicas fields define the range of replicas from which the autoscaler can choose. The initial_replicas field indicates the number of replicas at the start of the deployment, and if not specified, it defaults to min_replicas. The target_num_ongoing_requests_per_replica parameter determines how aggressively the autoscaler should react to traffic. \n",
      "\n",
      "The Ray Serve Autoscaler operates at the application level and sits on top of the Ray Autoscaler. It requests Ray to start a number of replica actors based on the request demand. If the Ray Autoscaler determines there aren’t enough available CPUs to place these actors, it requests more Ray nodes from the underlying cloud provider. When Ray Serve scales down and terminates some replica actors, it tries to do so in a way that results in the most nodes having no Ray actors or tasks running on them, at which point the Ray autoscaler will remove those nodes.\n",
      "\n",
      "There are also parameters like downscale_delay_s and upscale_delay_s that control the frequency of autoscaling work, and smoothing_factor to speed up or slow down each autoscaling step. The metrics_interval_s parameter controls how often each replica sends metrics to the autoscaler.\n",
      "\u001b[1mLlama2-70B answer:\u001b[0m  Autoscaling in a Ray Serve application is handled by the Ray Serve Autoscaler, which is an application-level autoscaler that sits on top of the Ray Autoscaler. The autoscaler observes queue sizes and makes scaling decisions to add or remove replicas based on the demand. The autoscaling algorithm takes into consideration several user-specified parameters, such as min_replicas, max_replicas, target_num_ongoing_requests_per_replica, downscale_delay_s, upscale_delay_s, and smoothing_factor. These parameters control the minimum and maximum number of replicas, the target number of ongoing requests per replica, and the delay before scaling up or down. The autoscaler also takes into account the available cluster resources and relies on the Ray Autoscaler to scale up more nodes when necessary. The autoscaler decides to scale up or down by comparing the target number of ongoing requests per replica to the number of running and pending tasks on each replica. The autoscaler's response can be amplified or dampened by adjust\n",
      "\n",
      "\n",
      "\u001b[1mQuery:\u001b[0m how do I get the address of a ray node\n",
      "\u001b[1mGPT4 answer:\u001b[0m You can get the address of a Ray node by accessing the 'NodeManagerAddress' field of the node's information. This field contains the private IP of the node that the raylet is on.\n",
      "\u001b[1mLlama2-70B answer:\u001b[0m  You can get the address of a Ray node by using the `ray.nodes()` method and looking for the `NodeManagerAddress` field in the response. This field contains the private IP address of the node that the Raylet is running on.\n",
      "\n",
      "For example, in the provided context information, the address of the Ray node with NodeID '2691a0c1aed6f45e262b2372baf58871734332d7' is '192.168.1.82'.\n",
      "\n",
      "You can also use the `ray.get_node()` method to get information about a specific node, including its address. For example:\n",
      "```\n",
      "node_info = ray.get_node(NodeID='2691a0c1aed6f45e262b2372baf58871734332d7')\n",
      "print(node_info['NodeManagerAddress'])\n",
      "```\n",
      "This will print the private IP address of the specified node.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BOLD = '\\033[1m'\n",
    "END = '\\033[0m'\n",
    "    \n",
    "for query, gpt_response, llama_response in zip(data[:5], gpt4_responses, llama_responses):\n",
    "    print(f\"{BOLD}Query:{END} {query['question']}\")\n",
    "    print(f\"{BOLD}GPT4 answer:{END} {gpt_response}\")\n",
    "    print(f\"{BOLD}Llama2-70B answer:{END} {llama_response}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e3fc5d-13bb-4314-b148-ab4be5b98dcb",
   "metadata": {},
   "source": [
    "Based on these answers, we go with GPT-4 as our \"golden\" LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a6cfa4-8f59-43e7-a694-843b9613b1b3",
   "metadata": {},
   "source": [
    "### Generating our Golden Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f5c2a6-a5ac-4b9c-ab45-61d444b97b9b",
   "metadata": {},
   "source": [
    "Now that we have chosen which LLM to use, we can generate our reference responses. Let's generate 10 reference responses and save them to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5981d038-4b6c-4a97-8e05-ccf9121ade7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:17<00:00,  7.78s/it]\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(model='gpt-4', temperature=0.0)\n",
    "ten_samples = data[:10]\n",
    "golden_responses = generate_responses(ten_samples, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5fc8506c-0deb-43ae-9936-cd323635cc44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reference_dataset = [{\"question\": entry[\"question\"], \"source\": entry[\"source\"], \"response\": response} for entry, response in zip(ten_samples, golden_responses)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "23632aab-2be3-49cb-905b-1b26d7bde46a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"golden-responses.json\", \"w\") as file:\n",
    "    json.dump(reference_dataset, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dbf6c6-74ea-4802-be16-f96f2982a642",
   "metadata": {},
   "source": [
    "## Evaluating our Query Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef0af94-f983-464f-8a4b-a91ff72d7f87",
   "metadata": {},
   "source": [
    "Once we have reference responses, we can get our generated responses from our query engine. Then pass both responses to our golden LLM to evaluate the responses from our application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cd15459b-a679-4f1b-832c-bb38cd467d0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"golden-responses.json\", \"r\") as file:\n",
    "    golden_responses = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3f2a555f-20c0-4716-b4b2-1d9921edfa79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'I’m struggling a bit with Ray Data type conversions when I do map_batches. Any advice?',\n",
       " 'source': 'https://docs.ray.io/en/master/data/transforming-data.html#configuring-batch-format',\n",
       " 'response': 'Sure, when using the `map_batches()` function in Ray Data, you can specify the batch format by using the `batch_format` parameter. If you want to represent batches as dictionaries of NumPy ndarrays, you can set `batch_format=\"numpy\"`. For example, if you have a function like `increase_brightness` that operates on NumPy ndarrays, you can use it with `map_batches()` like this:\\n\\n```python\\nds = (\\n    ray.data.read_images(\"s3://[email\\xa0protected]/image-datasets/simple\")\\n    .map_batches(increase_brightness, batch_format=\"numpy\")\\n)\\n```\\n\\nOn the other hand, if you want to represent batches as pandas DataFrames, you can set `batch_format=\"pandas\"`. For instance, if you have a function like `drop_nas` that operates on pandas DataFrames, you can use it with `map_batches()` like this:\\n\\n```python\\nds = (\\n    ray.data.read_csv(\"s3://[email\\xa0protected]/iris.csv\")\\n    .map_batches(drop_nas, batch_format=\"pandas\")\\n)\\n```\\n\\nRemember, the function you use with `map_batches()` should be compatible with the batch format you choose.'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "golden_responses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b8a53891-cd44-4efb-b9f9-9e3cc7de3a6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import get_query_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3adebada-bce2-4600-ac2f-4ba541ce32f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:27<00:00,  8.76s/it]\n"
     ]
    }
   ],
   "source": [
    "query_engine = get_query_engine(similarity_top_k=5, llm_model_name='meta-llama/Llama-2-70b-chat-hf')\n",
    "\n",
    "# Store both the original response object and the response string.\n",
    "rag_responses = []\n",
    "rag_response_str = []\n",
    "\n",
    "for entry in tqdm(golden_responses):\n",
    "    query = entry[\"question\"]\n",
    "    response = query_engine.query(query)\n",
    "    rag_responses.append(response)\n",
    "    rag_response_str.append(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a17e76f6-7333-4ea7-9357-3bc125c59cf8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' It seems like you\\'re encountering issues with type conversions when using `map_batches` with Ray Data. Here are some tips that may help:\\n\\n1. Use the `batch_format` argument: When calling `map_batches`, you can specify the `batch_format` argument to indicate the format of the batches. If you\\'re working with NumPy arrays, set `batch_format=\"numpy\"`. If you\\'re working with PyTorch tensors, set `batch_format=\"torch\"`. This can help Ray Data handle the type conversions correctly.\\n2. Return the correct type: When writing a transformation function for `map_batches`, make sure to return a batch in the correct format. If you\\'re working with NumPy arrays, return a dictionary of NumPy arrays. If you\\'re working with PyTorch tensors, return a dictionary of PyTorch tensors.\\n3. Avoid returning lists: When using `map_batches`, avoid returning lists of arrays or tensors. Instead, return a dictionary of arrays or tensors, where each key corresponds to a batch dimension. This can help Ray Data handle the type conversions correctly.\\n4'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_response_str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a9a8456b-8a78-4e87-8110-3fd163f10864",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.evaluation import CorrectnessEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ef83fc36-e93d-4341-b7a6-d0a24a598872",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_llm = OpenAI(model='gpt-4', temperature=0.0)\n",
    "service_context = ServiceContext.from_defaults(llm=eval_llm)\n",
    "evaluator = CorrectnessEvaluator(service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "90bb5e09-08e1-40e8-9129-0f441bbce72f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m golden_answer \u001b[38;5;241m=\u001b[39m golden_response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      5\u001b[0m generated_answer \u001b[38;5;241m=\u001b[39m rag_response\n\u001b[0;32m----> 7\u001b[0m eval_result \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgolden_answer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerated_answer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m eval_results\u001b[38;5;241m.\u001b[39mappend(eval_result)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/llama_index/evaluation/base.py:44\u001b[0m, in \u001b[0;36mBaseEvaluator.evaluate\u001b[0;34m(self, query, response, contexts, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     33\u001b[0m     query: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     37\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m EvaluationResult:\n\u001b[1;32m     38\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run evaluation with query string, retrieved contexts,\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    and generated response string.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m    Subclasses can override this method to provide custom evaluation logic and\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m    take in additional arguments.\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m            \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/asyncio/runners.py:33\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m coroutines\u001b[38;5;241m.\u001b[39miscoroutine(main):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma coroutine was expected, got \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(main))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "eval_results = []\n",
    "for rag_response, golden_response in tqdm(zip(rag_response_str, golden_responses)):\n",
    "    query = golden_response[\"question\"]\n",
    "    golden_answer = golden_response[\"response\"]\n",
    "    generated_answer = rag_response\n",
    "    \n",
    "    eval_result = evaluator.evaluate(query=query, reference=golden_answer, response=generated_answer)\n",
    "    eval_results.append(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e27497-0ec6-4ae3-bc0a-1c342afb3047",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[r.score for r in eval_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3190240d-7bb1-4079-8bfa-0d6d0ed9166d",
   "metadata": {},
   "source": [
    "Let's save the query, both responses, and the score to a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc98a6d-a27a-4a42-a5c5-4574a336e679",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores = [\n",
    "    {\"question\": golden_response[\"question\"],\n",
    "     \"golden_response\": golden_response[\"response\"],\n",
    "     \"generated_response\": eval_result.response,\n",
    "     \"score\": eval_result.score,\n",
    "     \"reasoning\": eval_result.feedback,\n",
    "    }\n",
    "    for eval_result, golden_response in zip(eval_results, golden_responses)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00e6239-5c44-4c1a-89d5-7549fb0ad98a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"eval-scores.json\", \"w\") as file:\n",
    "    json.dump(scores, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095ef29b-b968-4449-9a50-4234a5b294ab",
   "metadata": {},
   "source": [
    "We can also calculate the average scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b7d3a9-d2c5-4381-b2d4-9c1aa1d21df1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "average_scores = sum(score[\"score\"] for score in scores) / len(scores)\n",
    "average_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0229cb-2256-4762-a772-55740e617951",
   "metadata": {},
   "source": [
    "## Evaluation without Golden Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43b1970-ded0-4ee4-b49e-49447fad1533",
   "metadata": {},
   "source": [
    "Generating reference responses and then using them for evaluation can give us a more accurate assesment on how our query engine is performing. However, this approach can be expensive. We have to make an initial pass through GPT4 to generate the reference response, and then we have to make another pass through GPT4 to evaluate our application's responses against the reference response.\n",
    "\n",
    "We can explore other evaluation metrics to get a better sense on how our query engine is performing, without needing to make multiple passes to GPT4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1905688d-9a0d-4e2a-81ea-40242f50905a",
   "metadata": {},
   "source": [
    "### Evaluating for faithfulness/relevancy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6662a23a-8388-42cc-b986-0062afee3b48",
   "metadata": {},
   "source": [
    "One metric we can test is relevancy, which does not require generating reference responses. With this approach, we check to see if the generated response is relevant to at least one of the retrieved sources and to the query. This ensures that our LLM is not making up a response, but rather that it is relevant to the question that is being asked, and also that is relevant to at least one of the retrieved context.\n",
    "\n",
    "This does NOT check whether the response is a correct response.\n",
    "\n",
    "This capability is built into LlamaIndex, via the various `Evaluator` modules. We use gpt-4 as the evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c767913f-fc0a-4437-932a-4a5106a5768c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.evaluation import FaithfulnessEvaluator, RelevancyEvaluator\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index import ServiceContext\n",
    "\n",
    "def evaluate(queries: list, responses: list, metric: str):\n",
    "    llm = OpenAI(model=\"gpt-4\", temperature=0.0)\n",
    "    service_context = ServiceContext.from_defaults(llm=llm)\n",
    "    \n",
    "    \n",
    "    if metric == 'faithfulness':\n",
    "        evaluator = FaithfulnessEvaluator(service_context=service_context)\n",
    "    elif metric == 'relevancy':\n",
    "        evaluator = RelevancyEvaluator(service_context=service_context)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown metric: \", metrc)\n",
    "\n",
    "    evals = []\n",
    "    for query, response in tqdm(list(zip(queries, responses))):\n",
    "        eval_result = evaluator.evaluate_response(query=query, response=response)\n",
    "        evals.append(eval_result)\n",
    "    \n",
    "    return evals\n",
    "\n",
    "def get_pass_rate(evals):\n",
    "    return len([val.passing for val in evals]) / len(evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6ecd11db-ced6-433b-844d-03c6d0adc30a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m faithfulness_results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mten_samples\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrag_responses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfaithfulness\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[72], line 19\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(queries, responses, metric)\u001b[0m\n\u001b[1;32m     17\u001b[0m evals \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m query, response \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(queries, responses))):\n\u001b[0;32m---> 19\u001b[0m     eval_result \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     evals\u001b[38;5;241m.\u001b[39mappend(eval_result)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m evals\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/llama_index/evaluation/base.py:80\u001b[0m, in \u001b[0;36mBaseEvaluator.evaluate_response\u001b[0;34m(self, query, response, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_response\u001b[39m(\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     71\u001b[0m     query: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     72\u001b[0m     response: Optional[Response] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     74\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m EvaluationResult:\n\u001b[1;32m     75\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run evaluation with query string and generated Response object.\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m    Subclasses can override this method to provide custom evaluation logic and\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m    take in additional arguments.\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maevaluate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/asyncio/runners.py:33\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m coroutines\u001b[38;5;241m.\u001b[39miscoroutine(main):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma coroutine was expected, got \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(main))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sys:1: RuntimeWarning: coroutine 'CorrectnessEvaluator.aevaluate' was never awaited\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "faithfulness_results = evaluate(queries=[sample[\"question\"] for sample in ten_samples], responses=rag_responses, metric='faithfulness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4555fa-5901-49df-8f66-afbe00d27079",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "faithfulness_score = get_pass_rate(faithfulness_results)\n",
    "faithfulness_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e8a04f-62f6-46cb-80e1-adde0195c1cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "relevancy_results = evaluate(queries=[sample[\"question\"] for sample in ten_samples], responses=rag_responses, metric='relevancy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b35f60-3989-459c-a26e-12ed35456e8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "relevancy_score = get_pass_rate(relevancy_results)\n",
    "relevancy_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
