{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f773bc8-a632-4b1e-a817-42da92594d33",
   "metadata": {},
   "source": [
    "# Part 3: Experiment and optimize the RAG application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d7edf7-cdc5-45d4-b145-28e2d5316026",
   "metadata": {
    "tags": []
   },
   "source": [
    "- GitHub repository: https://github.com/anyscale/ray-summit-2023-training/tree/main\n",
    "- Anyscale Endpoints: https://endpoints.anyscale.com/\n",
    "- Ray documentation: https://docs.ray.io/\n",
    "- LlamaIndex documentation: https://gpt-index.readthedocs.io/en/stable/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75f028a-150d-4364-9dc9-638fefc4bf13",
   "metadata": {
    "tags": []
   },
   "source": [
    "<span style=\"background: yellow; color: red; font-size: 1rem;\"><b>TODO:</b></span> assume evaluation functions have all been defined in previous part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9d2bfa-b98f-4d84-84a1-91a6494ad816",
   "metadata": {},
   "source": [
    "## Strategy 1: Search for optimal configuration for standard components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af16dc39-cae6-4c35-9619-2e149436efe7",
   "metadata": {},
   "source": [
    "## Strategy 2: Use different data representation for retrieval vs. generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe6a76a-d806-4d71-bed7-99e0f5c0295c",
   "metadata": {},
   "source": [
    "## Strategy 3: Fine-tune embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000b6a58-495c-44c8-b760-a2ce1cbed6c5",
   "metadata": {},
   "source": [
    "In this section, we explore fine-tuning embedding model to improve retrieval performance.\n",
    "We consider the \"cold start\" regime, where we haven't deployed the model, and haven't collected any user queries or labeled \"golden\" context. \n",
    "\n",
    "Therefore, we consider a synthetic data approach, where we leverage LLM to generation question, relevant context, answer pairs from our knowledge corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f656b8f3-eae3-4e68-bba5-aac91b04d8c7",
   "metadata": {},
   "source": [
    "### Experiment configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1d1fd8ae-469e-4fa6-899c-18e5150ab2e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ray/default/ray-summit-2023-training/Ray-LlamaIndex\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Since we are working with a relatively large set of documents, \n",
    "# we sub-sample the data for quicker iterations.\n",
    "SUBSAMPLE_RATIO = 0.05\n",
    "\n",
    "# can be any sentence-transformer compatible model\n",
    "# https://www.sbert.net/docs/pretrained_models.html\n",
    "BASE_MODEL = 'BAAI/bge-small-en'  \n",
    "\n",
    "# Select a chunk size that is: \n",
    "# 1) under the context window limit of the chosen model, and\n",
    "# 2) close to the desired chunk size at retrieval time\n",
    "FINETUNE_CHUNK_SIZE = 512\n",
    "\n",
    "\n",
    "ROOT_DIR = Path(os.getcwd()).parent\n",
    "print(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131f313b-03af-4596-a698-7c2921463d87",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402f4d6a-79f8-4f85-bb89-7e119bc753c7",
   "metadata": {},
   "source": [
    "We start by loading our knowledge corpus (i.e. the ray documentation webpages that we've downloaded and parsed previously)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21078dd5-f719-4445-82d4-21fe48432227",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATASETS_DIRECTORY = Path(\"/efs/shared_storage/simon/datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19e6f02c-8855-4452-878e-ba42cafc5d4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from llama_index.schema import Document\n",
    "\n",
    "def read_json(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def to_doc(entry_dict):\n",
    "    return Document(text=entry_dict['text'], metadata={'source': entry_dict['source']}) \n",
    "\n",
    "def load_corpus(filename):\n",
    "    sections = read_json(filename)\n",
    "    docs = [to_doc(dict_) for dict_ in sections]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e0ed5169-3fb5-4e18-b51b-662c9d3f94ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = load_corpus(DATASETS_DIRECTORY / 'eval_full_corpus.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007dc69e-a871-45fa-a005-0cfc481ab916",
   "metadata": {},
   "source": [
    "Now, we split the documents into chunks. The main considerations on the chunk size here are:\n",
    "1. need to fit into the context window of the embedding model that we want to finetune (512 for the sentence transformer model that we selected, similar to most open source models.)\n",
    "2. should be close to the chunk size we want to use at retrieval time (so it's best to run some experiments to determine the best chunk size for your application first, see strategy 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0535a133-5ff4-4c91-a255-a068c03de951",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f51735af11f14461ac012e54a05704de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing documents into nodes:   0%|          | 0/8944 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 8944 docs into 14242 nodes\n"
     ]
    }
   ],
   "source": [
    "from llama_index.node_parser import SimpleNodeParser\n",
    "\n",
    "parser = SimpleNodeParser.from_defaults(chunk_size=FINETUNE_CHUNK_SIZE)\n",
    "nodes = parser.get_nodes_from_documents(docs, show_progress=True)\n",
    "print('Parsed {} docs into {} nodes'.format(len(docs), len(nodes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7a2551-9c0a-4d32-8262-188da2e07fcf",
   "metadata": {},
   "source": [
    "### Subsample and create train/validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b2007f38-d093-4eed-8a0a-d9f63a1c6aa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def train_test_split(data, split_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Split a list of items into training and testing sets.\n",
    "\n",
    "    Args:\n",
    "        data (list): The list of items to be split.\n",
    "        split_ratio (float): The ratio of items to include in the training set (default is 0.8).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two lists - the training set and the testing set.\n",
    "    \"\"\"\n",
    "    if not 0 <= split_ratio <= 1:\n",
    "        raise ValueError(\"Split ratio must be between 0 and 1\")\n",
    "\n",
    "    # Shuffle the data to ensure randomness in the split\n",
    "    random.shuffle(data)\n",
    "\n",
    "    # Calculate the split indices\n",
    "    split_index = int(len(data) * split_ratio)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    train_set = data[:split_index]\n",
    "    test_set = data[split_index:]\n",
    "\n",
    "    return train_set, test_set\n",
    "\n",
    "def subsample(data, ratio):\n",
    "    \"\"\"\n",
    "    Subsample a list to a given ratio.\n",
    "\n",
    "    Args:\n",
    "        data (list): The list of items to be subsampled.\n",
    "        ratio (float): The ratio of items to retain in the subsample.\n",
    "\n",
    "    Returns:\n",
    "        list: A subsampled list containing the specified ratio of items.\n",
    "    \"\"\"\n",
    "    if not 0 <= ratio <= 1:\n",
    "        raise ValueError(\"Ratio must be between 0 and 1\")\n",
    "\n",
    "    # Calculate the number of items to retain in the subsample\n",
    "    num_items_to_retain = int(len(data) * ratio)\n",
    "\n",
    "    # Randomly select items to retain\n",
    "    subsampled_data = random.sample(data, num_items_to_retain)\n",
    "\n",
    "    return subsampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8269e608-370b-4dd2-9ad7-0e83b77abb70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split dataset into: 11393 train nodes, 2849 val nodes\n"
     ]
    }
   ],
   "source": [
    "train_nodes, val_nodes = train_test_split(nodes)\n",
    "print('Split dataset into: {} train nodes, {} val nodes'.format(len(train_nodes), len(val_nodes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f409e028-0363-4a19-8378-a2f3ceb3dace",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsampled dataset into: 569 train nodes, 142 val nodes\n"
     ]
    }
   ],
   "source": [
    "train_nodes = subsample(train_nodes, SUBSAMPLE_RATIO)\n",
    "val_nodes = subsample(val_nodes, SUBSAMPLE_RATIO)\n",
    "print('Subsampled dataset into: {} train nodes, {} val nodes'.format(len(train_nodes), len(val_nodes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3305f44d-b49d-4dd1-b3bb-c9622927ce55",
   "metadata": {},
   "source": [
    "### Generate synthetic dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a7e80a-f36b-4622-9696-373ce7a06e72",
   "metadata": {},
   "source": [
    "Now, we will generate a synthetic dataset of question, \"golden\" context, and \"golden\" answer dataset by leveraing an LLM (by default using gpt-3.5-turbo from OpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cc7562-65f1-4670-b6d3-23129a031700",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.finetuning import generate_qa_embedding_pairs\n",
    "\n",
    "train_dataset = generate_qa_embedding_pairs(train_nodes)\n",
    "val_dataset = generate_qa_embedding_pairs(val_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdd2bc9-d71c-4461-a9ac-6bd5b348b4bb",
   "metadata": {},
   "source": [
    "Let's save the dataset for future use since it's fairly time consuming to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648de625-9676-49b9-b141-a157c6535f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.save_json(Path(ROOT_DIR, \"datasets/synthetic_train_dataset.json\"))\n",
    "val_dataset.save_json(Path(ROOT_DIR, \"datasets/synthetic_val_dataset.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a373622f-ba1e-468b-946f-82d94082ca50",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Run embedding finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d1c718-75d5-4f40-9547-6e76b8b9740e",
   "metadata": {},
   "source": [
    "Now, we are ready to fine-tune our embedding model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b869fd8-34f1-4f9a-91c9-47b36024b248",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EmbeddingQAFinetuneDataset.from_json(Path(ROOT_DIR, \"datasets/synthetic_train_dataset.json\"))\n",
    "val_dataset = EmbeddingQAFinetuneDataset.from_json(Path(ROOT_DIR, \"datasets/synthetic_val_dataset.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079210b6-bbc1-4277-bb44-a41409771cec",
   "metadata": {},
   "source": [
    "We can construct a fine-tune engine, which is an easy to use interface for running fine-tuning jobs (either locally or via a API-based service).\n",
    "\n",
    "Here, we use the sentence transformer fine-tuning engine to run fine-tuning locally on the ray cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405e5e63-61ef-4920-ba1e-b9e7ee052dce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.finetuning import SentenceTransformersFinetuneEngine\n",
    "\n",
    "finetune_engine = SentenceTransformersFinetuneEngine(\n",
    "    train_dataset,\n",
    "    model_id=\"BAAI/bge-small-en\",\n",
    "    model_output_path=\"exp_finetune_test\",\n",
    "    val_dataset=val_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdad2de8-7ee7-447a-905c-9e27681e9b8f",
   "metadata": {},
   "source": [
    "For demonstration purpose, we will run the fine-tuning job for 2 epoches over our synthetic dataset.\n",
    "In practice, you should use the validation loss to determine how many epochs to fine-tune the embedding model for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a4347c-819b-4673-b03d-e950261e9fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_engine.finetune()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4705c2b6-f3d2-4404-8d41-4db76b3e600d",
   "metadata": {},
   "source": [
    "After the fine-tuning job finishes, we can easy get a referene to the fine-tuned model to be used in our LlamaIndex application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfeecc2-e3e2-4313-9c3d-3c0153799948",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = finetune_engine.get_finetuned_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc3bf8b-1943-4bbb-9166-5b068d7c179e",
   "metadata": {},
   "source": [
    "### Evaluate our fine-tuned embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36776fa-ba10-4d3b-993c-d0f2842feaf6",
   "metadata": {},
   "source": [
    "Now, we will leverage the retrieval evaluation process we built out in [part 2](www.google.com) to assess the quality of our fine-tuned embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "23b36038-21c4-4e87-83ab-f58cc49ee200",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Load labeled eval dataset\n",
    "with open(DATASETS_DIRECTORY /  \"eval-dataset-v1.jsonl\", \"r\") as f:\n",
    "    test_dataset = [json.loads(item) for item in list(f)]\n",
    "\n",
    "# Update source\n",
    "# TODO: update saved dataset to avoid this step\n",
    "for row in test_dataset:\n",
    "    row[\"source\"] = row[\"source\"].replace(\"https://docs.ray.io/en/latest/\", \"https://docs.ray.io/en/master/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b9f2b3e4-3ed7-4607-a325-e577c7db1b09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'Iâ€™m struggling a bit with Ray Data type conversions when I do map_batches. Any advice?',\n",
       "  'source': 'https://docs.ray.io/en/master/data/transforming-data.html#configuring-batch-format'},\n",
       " {'question': 'How does autoscaling work in a Ray Serve application?',\n",
       "  'source': 'https://docs.ray.io/en/master/serve/scaling-and-resource-allocation.html#autoscaling'},\n",
       " {'question': 'how do I get the address of a ray node',\n",
       "  'source': 'https://docs.ray.io/en/master/ray-core/miscellaneous.html#node-information'},\n",
       " {'question': 'Does Ray support NCCL?',\n",
       "  'source': 'https://docs.ray.io/en/master/ray-more-libs/ray-collective.html'},\n",
       " {'question': 'Is Ray integrated with DeepSpeed?',\n",
       "  'source': 'https://docs.ray.io/en/master/ray-air/examples/gptj_deepspeed_fine_tuning.html#fine-tuning-the-model-with-ray-air-a-name-train-a'}]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8be8dc2d-0b08-4eff-99a3-f1be1545428c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_retrieval(\n",
    "    labeled_dataset,\n",
    "    index,\n",
    "    top_k=5,\n",
    "    verbose=False,\n",
    "):\n",
    "    retriever = index.as_retriever(similarity_top_k=top_k)\n",
    "\n",
    "    eval_results = []\n",
    "    for entry in tqdm(labled_dataset):\n",
    "        query = entry['question']\n",
    "        expected_source = entry['source']\n",
    "        \n",
    "        retrieved_nodes = retriever.retrieve(query)\n",
    "        retrieved_sources = [node.node.metadata['source'] for node in retrieved_nodes]\n",
    "        is_hit = expected_source in retrieved_sources  # assume 1 relevant doc\n",
    "        \n",
    "        eval_result = {\n",
    "            'is_hit': is_hit,\n",
    "            'retrieved': retrieved_sources,\n",
    "            'expected': expected_source,\n",
    "            'query': query,\n",
    "        }\n",
    "        eval_results.append(eval_result)\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53472a36-e93f-4756-8b61-1fd30bc94295",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now, we can build index with our fine-tuned embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9c7221e3-c54b-4661-9daf-8668ec058bbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, ServiceContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca6eeb7-dde7-4036-b38f-c4fb27159c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(\n",
    "    embed_model=embed_model,\n",
    "    chunk_size=512,\n",
    ")\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    docs, \n",
    "    service_context=service_context, \n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56831459-6e64-4c57-9111-3b517fc4d740",
   "metadata": {},
   "outputs": [],
   "source": [
    "Run retrieval evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dab2c5f-2e7a-420c-bbec-6d7992242cfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = evaluate_retrieval(test_dataset, index, top_k=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fa5c8c-60c2-45fa-9f6b-abc357824afe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "hit_rate = df['is_hit'].mean()\n",
    "print(hit_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
