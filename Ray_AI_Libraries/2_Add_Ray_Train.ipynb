{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b6208e1",
   "metadata": {},
   "source": [
    "# 2. Scalable Training with Ray Train for ViT Image Classification\n",
    "---\n",
    "\n",
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_AI_Libraries/RAIL_Train.png\" width=\"30%\" loading=\"lazy\">\n",
    "\n",
    "**Milestone 2: Distributed ML Training with Ray Train**\n",
    "\n",
    "In this notebook, we'll adapt our initial example to leverage Ray Train, a powerful library that enables distributed training and efficient utilization of resources. This transformation marks the beginning of our journey toward building scalable machine learning pipelines with Ray.\n",
    "\n",
    "**Featured Libraries**\n",
    "* [Ray Train](https://docs.ray.io/en/latest/train/train.html)\n",
    "    * Built on top of [Ray](https://docs.ray.io/en/latest/), it's a scalable machine learning library for distributed training and fine-tuning.\n",
    "* [Hugging Face `transformers`](https://huggingface.co/docs/transformers/index)\n",
    "* [Hugging Face `datasets`](https://huggingface.co/docs/datasets/index)\n",
    "\n",
    "**Table of Contents**\n",
    "1. [Introduction to Ray Train](#1-introduction-to-ray-train)\n",
    "    * What is Ray Train and what kinds of problems does it solve?\n",
    "2. [Define the Training Logic](#2-define-the-training-logic)\n",
    "    * Create a `train_func` that will be executed on each distributed training worker.\n",
    "3. [Configure the Scale and GPUs](#3-configure-the-scale-and-gpus)\n",
    "    * Move from a laptop to a cluster in the cloud with ease and control.\n",
    "    * Learn how to make the most of multiple GPUs using Ray Train, speeding up training by parallelizing computations.\n",
    "4. [Launch Distributed Fine-Tuning](#4-launch-distributed-fine-tuning)\n",
    "    * Explore how Ray Train enables efficient data parallelism, distributing the data across multiple workers for faster training.\n",
    "5. [Access Training Results](#5-access-training-results)\n",
    "    * Inspect the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f69e1dd",
   "metadata": {},
   "source": [
    "## 1. Introduction to Ray Train <a class=\"anchor\" id=\"1-introduction-to-ray-train\"></a>\n",
    "\n",
    "Before we dive into the technical details, let's briefly understand why we're using Ray Train:\n",
    "\n",
    "- **Scalability**: Ray Train allows us to easily distribute training across multiple GPUs and different machines, making it possible to handle large datasets and complex models efficiently.\n",
    "\n",
    "- **Resource Efficiency**: With Ray Train, we can maximize the use of available resources, ensuring that our fine-tuning process is not bottlenecked by hardware limitations.\n",
    "\n",
    "- **Flexibility**: It seamlessly integrates with popular existing machine learning libraries like the PyTorch ecosystem, Tensorflow, XGBoost, and [more](https://docs.ray.io/en/latest/train/more-frameworks.html), making it straightforward to adapt your existing workflows.\n",
    "\n",
    "### Building on our foundation\n",
    "\n",
    "Our previous notebook laid the groundwork for this transition. We're already familiar with the components like that dataset, feature extractor, model, and training logic. Let's see how we can adapt that existing code to now leverage the capabilities of Ray Train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e97efd",
   "metadata": {},
   "source": [
    "## 2. Define the Training Logic <a class=\"anchor\" id=\"2-define-the-training-logic\"></a>\n",
    "\n",
    "To give you a sense of the shape of the final implementation, here's the pattern we're trying to achieve:\n",
    "\n",
    "```python\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.train import ScalingConfig\n",
    "\n",
    "def train_func(config):\n",
    "    # Your Transformers training code here.\n",
    "\n",
    "scaling_config = ScalingConfig(num_workers=2, use_gpu=True)\n",
    "trainer = TorchTrainer(train_func, scaling_config=scaling_config)\n",
    "result = trainer.fit()\n",
    "```\n",
    "In the following section, we'll implement each of these steps in detail:\n",
    "\n",
    "1. `train_func` - Wraps all of your existing training logic and will be executed on each distributed training worker.\n",
    "2. `ScalingConfig` - Specifies the number of workers and computing resources to use for each.\n",
    "3. `TorchTrainer` - Launches the distributed training job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa954af",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a76c85f-b220-4ea1-a0e4-3b23a50468e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import ray.train.huggingface.transformers\n",
    "\n",
    "\n",
    "from ray.train import ScalingConfig\n",
    "from ray.train.huggingface.transformers import prepare_trainer, RayTrainReportCallback\n",
    "from ray.train.torch import TorchTrainer\n",
    "from transformers import ViTForImageClassification, TrainingArguments, Trainer, ViTImageProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd72ed6",
   "metadata": {},
   "source": [
    "### Wrap training logic in a `train_func`\n",
    "\n",
    "We'll take the essense of the previous notebook and distill is in the most compact way in this training function.\n",
    "\n",
    "Note: You'll see here that we're loading the dataset, feature extractor, and model within this function which will be replicated across every worker. This pattern may not be ideal for large datasets and models, and we'll explore how to deal with this in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9594a1-61e6-42ea-83cb-92019414624b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_func(config):\n",
    "    from datasets import load_dataset\n",
    "    import evaluate\n",
    "    \n",
    "    # HF dataset\n",
    "    ds = load_dataset('beans')\n",
    "\n",
    "    # HF feature extractor\n",
    "    model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
    "    feature_extractor = ViTImageProcessor.from_pretrained(model_name_or_path)\n",
    "\n",
    "    # HF ViT model\n",
    "    labels = ds['train'].features['labels'].names\n",
    "    model = ViTForImageClassification.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        num_labels=len(labels),\n",
    "        id2label={str(i): c for i, c in enumerate(labels)},\n",
    "        label2id={c: str(i) for i, c in enumerate(labels)}\n",
    "    )\n",
    "\n",
    "    # Image processing\n",
    "    def transform(example_batch):\n",
    "        inputs = feature_extractor([x for x in example_batch['image']], return_tensors='pt')\n",
    "        inputs['labels'] = example_batch['labels']\n",
    "        return inputs\n",
    "\n",
    "    prepared_ds = ds.with_transform(transform)\n",
    "\n",
    "    # Evaluation metric\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "    \n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    # HF Training Args\n",
    "    training_args = TrainingArguments(\n",
    "      output_dir=\"/mnt/local_storage/output\",\n",
    "      evaluation_strategy=\"epoch\",\n",
    "      num_train_epochs=2,\n",
    "      logging_steps=10,\n",
    "      remove_unused_columns=False,\n",
    "    )\n",
    "\n",
    "    # Data collector\n",
    "    def collate_fn(batch):\n",
    "        return {\n",
    "            'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "            'labels': torch.tensor([x['labels'] for x in batch])\n",
    "        }\n",
    "\n",
    "    # HF Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=collate_fn,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=prepared_ds[\"train\"],\n",
    "        eval_dataset=prepared_ds[\"validation\"],\n",
    "    )\n",
    "\n",
    "    # Report metrics and checkpoints to Ray Train\n",
    "    callback = RayTrainReportCallback()\n",
    "    trainer.add_callback(callback)\n",
    "\n",
    "    # Prepare transformers Trainer for Ray Train; enables Ray Data integration.\n",
    "    trainer = prepare_trainer(trainer)\n",
    "\n",
    "    # Start training\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e57e793",
   "metadata": {},
   "source": [
    "## 3. Configure the Scale and GPUs <a class=\"anchor\" id=\"3-configure-the-scale-and-gpus\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cfd26e",
   "metadata": {},
   "source": [
    "### Define a `ScalingConfig`\n",
    "\n",
    "Today, we have access to two workers, each with access to 1 GPU, so we'll set the `ScalingConfig` to match. With more or varied resources, you're able to have greater freedom in specifying not only a larger cluster, but heterogeneous and custom hardware.\n",
    "\n",
    "1. `num_workers` - The number of distributed training worker processes.\n",
    "2. `use_gpu` - Whether each worker should use a GPU (or CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e06d238",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_config = ScalingConfig(num_workers=2, use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc24d4be",
   "metadata": {},
   "source": [
    "### Create a Ray Train `TorchTrainer`\n",
    "\n",
    "Note: While I won't cover the [`RunConfig`](https://docs.ray.io/en/latest/train/user-guides/persistent-storage.html#train-run-config) in great detail, know that it allows you to set things like the experiment name, storage path for results, stopping conditions, custom callbacks, checkpoint configuration, verbosity level, and logging options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad9913f-b44e-40c7-89c9-1cf61a9eecf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray_trainer = TorchTrainer(\n",
    "    train_func, scaling_config=scaling_config,\n",
    "    run_config=ray.train.RunConfig(storage_path='/mnt/cluster_storage')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e33b13a",
   "metadata": {},
   "source": [
    "## 4. Launch Distributed Fine-Tuning <a class=\"anchor\" id=\"4-launch-distributed-fine-tuning\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58205a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be0d8bd",
   "metadata": {},
   "source": [
    "## 5. Access Training Results <a class=\"anchor\" id=\"5-access-training-results\"></a>\n",
    "\n",
    "Once this job completes, you'll be able to access a `Result` object which contains more information about the training run, including the metrics and checkpoints reported during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01261b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.metrics     # The metrics reported during training.\n",
    "result.checkpoint  # The latest checkpoint reported during training.\n",
    "result.path        # The path where logs are stored.\n",
    "result.error       # The exception that was raised, if training failed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
