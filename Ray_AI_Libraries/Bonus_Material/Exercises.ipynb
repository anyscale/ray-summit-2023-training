{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Exercises\n",
    "---\n",
    "\n",
    "Welcome to the optional supplementary notebook! This notebook is designed for those who have already completed the main sequence of notebooks and are eager to dive deeper into the advanced capabilities of Ray Data and Ray Serve.\n",
    "\n",
    "**Ray Data** offers powerful distributed data processing tools, while **Ray Serve** simplifies scalable AI service deployment. In this optional module, we'll explore these components in more depth, building on the foundational knowledge you've gained.\n",
    "\n",
    "**What You'll Explore:**\n",
    "\n",
    "1. **Ray Data:**\n",
    "   - Advanced concepts and techniques for distributed data processing.\n",
    "   - Accelerating data analytics and machine learning with Ray Data.\n",
    "\n",
    "2. **Ray Serve:**\n",
    "   - Fine-tuning service deployment for low latency and high scalability.\n",
    "   - Advanced features and deployment strategies.\n",
    "\n",
    "This supplementary notebook is ideal for those looking to take their Ray skills to the next level. Feel free to dive in and explore the world of advanced distributed computing and AI service deployment with Ray Data and Ray Serve at your own pace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Datasets: Distributed Data Preprocessing\n",
    "\n",
    "Ray Datasets are the standard way to load and exchange data in Ray libraries and applications. They provide basic distributed data transformations such as maps ([`map_batches`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html#ray.data.Dataset.map_batches \"ray.data.Dataset.map_batches\")), global and grouped aggregations ([`GroupedDataset`](https://docs.ray.io/en/latest/data/api/doc/ray.data.grouped_dataset.GroupedDataset.html#ray.data.grouped_dataset.GroupedDataset \"ray.data.grouped_dataset.GroupedDataset\")), and shuffling operations ([`random_shuffle`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.random_shuffle.html#ray.data.Dataset.random_shuffle \"ray.data.Dataset.random_shuffle\"), [`sort`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.sort.html#ray.data.Dataset.sort \"ray.data.Dataset.sort\"), [`repartition`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.repartition.html#ray.data.Dataset.repartition \"ray.data.Dataset.repartition\")), and are compatible with a variety of file formats, data sources, and distributed frameworks.\n",
    "\n",
    "Here's an overview of the integrations with other processing frameworks, file formats, and supported operations, as well as a glimpse at the Ray Datasets API.\n",
    "\n",
    "Check the [Input/Output reference](https://docs.ray.io/en/latest/data/api/input_output.html#input-output) to see if your favorite format is already supported.\n",
    "\n",
    "<img src='https://docs.ray.io/en/releases-2.6.1/_images/dataset.svg' width=80%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Preprocessing for ML Training\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Use Ray Datasets to load and preprocess data for distributed [ML training pipelines](https://docs.ray.io/en/latest/train/train.html#train-docs). Compared to other loading solutions, Datasets are more flexible (e.g., can express higher-quality per-epoch global shuffles) and provides [higher overall performance](https://www.anyscale.com/blog/why-third-generation-ml-platforms-are-more-performant).\n",
    "\n",
    "Use Datasets as a last-mile bridge from storage or ETL pipeline outputs to distributed applications and libraries in Ray. Don't use it as a replacement for more general data processing systems.\n",
    "\n",
    "<img src='https://docs.ray.io/en/releases-2.6.1/_images/dataset-loading-1.png' width=50%/>\n",
    "\n",
    "To learn more about the features Datasets supports, read the [Datasets User Guide](https://docs.ray.io/en/latest/data/user-guide.html#data-user-guide).\n",
    "\n",
    "### Datasets for Parallel Compute\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Datasets also simplify general purpose parallel GPU and CPU compute in Ray; for instance, for [GPU batch inference](https://docs.ray.io/en/latest/ray-overview/use-cases.html#ref-use-cases-batch-infer). They provide a higher-level API for Ray tasks and actors for such embarrassingly parallel compute, internally handling operations like batching, pipelining, and memory management.\n",
    "\n",
    "<img src='https://docs.ray.io/en/releases-2.6.1/_images/stream-example.png' width=60%/>\n",
    "\n",
    "As part of the Ray ecosystem, Ray Datasets can leverage the full functionality of Ray's distributed scheduler, e.g., using actors for optimizing setup time and GPU scheduling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "------------------------------------------------------------------------------------------------------\n",
    "\n",
    "A Dataset consists of a list of Ray object references to *blocks*. Each block holds a set of items in either [Arrow table format](https://arrow.apache.org/docs/python/data.html#tables) or a Python list (for non-tabular data). For ML use cases, Datasets also natively supports mixing [Tensor](https://docs.ray.io/en/latest/data/dataset-tensor-support.html#datasets-tensor-support) and tabular data. Having multiple blocks in a dataset allows for parallel transformation and ingest.\n",
    "\n",
    "The following figure visualizes a tabular dataset with three blocks, each block holding 1000 rows each:\n",
    "\n",
    "<img src='https://docs.ray.io/en/releases-2.6.1/_images/dataset-arch.svg' width=50%/>\n",
    "\n",
    "Since a Dataset is just a list of Ray object references, it can be freely passed between Ray tasks, actors, and libraries like any other object reference. This flexibility is a unique characteristic of Ray Datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data[](https://docs.ray.io/en/latest/data/key-concepts.html#reading-data \"Permalink to this headline\")\n",
    "\n",
    "Datasets uses Ray tasks to read data from remote storage. When reading from a file-based datasource (e.g., S3, GCS), it creates a number of read tasks proportional to the number of CPUs in the cluster. Each read task reads its assigned files and produces an output block:\n",
    "\n",
    "<img src='https://docs.ray.io/en/releases-2.6.1/_images/dataset-read.svg' width=60%/>\n",
    "\n",
    "The parallelism can also be manually specified, but the final parallelism for a read is always capped by the number of files in the underlying dataset. See the [Creating Datasets Guide](https://docs.ray.io/en/latest/data/creating-datasets.html#creating-datasets) for an in-depth guide on creating datasets.\n",
    "\n",
    "### Transforming Data[](https://docs.ray.io/en/latest/data/key-concepts.html#transforming-data \"Permalink to this headline\")\n",
    "\n",
    "Datasets can use either Ray tasks or Ray actors to transform datasets. By default, tasks are used. Actors can be specified using `compute=ActorPoolStrategy()`, which creates an autoscaling pool of Ray actors to process transformations. Using actors allows for expensive state initialization (e.g., for GPU-based tasks) to be cached:\n",
    "\n",
    "<img src='https://docs.ray.io/en/releases-2.6.1/_images/dataset-map.svg' width=60%/>)\n",
    "\n",
    "See the [Transforming Datasets Guide](https://docs.ray.io/en/releases-2.6.1/data/transforming-datasets.html#transforming-datasets) for an in-depth guide on transforming datasets.\n",
    "\n",
    "### Shuffling Data[](https://docs.ray.io/en/latest/data/key-concepts.html#shuffling-data \"Permalink to this headline\")\n",
    "\n",
    "Certain operations like *sort* or *groupby* require data blocks to be partitioned by value, or *shuffled*. Datasets uses tasks to implement distributed shuffles in a map-reduce style, using map tasks to partition blocks by value, and then reduce tasks to merge co-partitioned blocks together.\n",
    "\n",
    "You can also change just the number of blocks of a Dataset using [`repartition()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.repartition.html#ray.data.Dataset.repartition \"ray.data.Dataset.repartition\"). Repartition has two modes:\n",
    "\n",
    "1.  `shuffle=False` - performs the minimal data movement needed to equalize block sizes\n",
    "\n",
    "2.  `shuffle=True` - performs a full distributed shuffle\n",
    "\n",
    "<img src='https://docs.ray.io/en/releases-2.6.1/_images/dataset-shuffle.svg' width=60%/>\n",
    "\n",
    "Datasets shuffle can scale to processing hundreds of terabytes of data. See the [Performance Tips Guide](https://docs.ray.io/en/latest/data/performance-tips.html#shuffle-performance-tips) for an in-depth guide on shuffle performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution mode[](https://docs.ray.io/en/latest/data/key-concepts.html#execution-mode \"Permalink to this headline\")\n",
    "\n",
    "Most transformations are lazy. They don't execute until you consume a dataset or call [`Dataset.materialize()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.materialize.html#ray.data.Dataset.materialize \"ray.data.Dataset.materialize\").\n",
    "\n",
    "The transformations are executed in a streaming way, incrementally on the data and with operators processed in parallel, see [Streaming Execution](https://docs.ray.io/en/latest/data/dataset-internals.html#datasets-streaming-execution).\n",
    "\n",
    "For an in-depth guide on Datasets execution, read [Execution](https://docs.ray.io/en/latest/data/dataset-internals.html#datasets-execution).\n",
    "\n",
    "### Fault tolerance[](https://docs.ray.io/en/latest/data/key-concepts.html#fault-tolerance \"Permalink to this headline\")\n",
    "\n",
    "Datasets performs *lineage reconstruction* to recover data. If an application error or system failure occurs, Datasets recreates lost blocks by re-executing tasks.\n",
    "\n",
    "Fault tolerance isn't supported in two cases:\n",
    "\n",
    "-   If the original worker process that created the Dataset dies. This is because the creator stores the metadata for the [objects](https://docs.ray.io/en/latest/ray-core/fault_tolerance/objects.html#object-fault-tolerance) that comprise the Dataset.\n",
    "\n",
    "-   If you specify `compute=ActorPoolStrategy()` for transformations. This is because Datasets relies on [task-based fault tolerance](https://docs.ray.io/en/latest/ray-core/fault_tolerance/tasks.html#task-fault-tolerance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example operations: Transforming Datasets\n",
    "\n",
    "Datasets transformations take in datasets and produce new datasets. For example, *map* is a transformation that applies a [user-defined function](https://docs.ray.io/en/latest/data/transforming-datasets.html#transform-datasets-writing-udfs) on each dataset record and returns a new dataset as the result. Datasets transformations can be composed to express a chain of computations.\n",
    "\n",
    "There are two main types of transformations:\n",
    "\n",
    "-   One-to-one: each input block will contribute to only one output block, such as [`ds.map_batches()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html#ray.data.Dataset.map_batches \"ray.data.Dataset.map_batches\").\n",
    "\n",
    "-   All-to-all: input blocks can contribute to multiple output blocks, such as [`ds.random_shuffle()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.random_shuffle.html#ray.data.Dataset.random_shuffle \"ray.data.Dataset.random_shuffle\").\n",
    "\n",
    "Here is a table listing some common transformations supported by Ray Datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common Ray Datasets transformations.[](https://docs.ray.io/en/latest/data/transforming-datasets.html#id2 \"Permalink to this table\")\n",
    "\n",
    "| Transformation | Type | Description |\n",
    "| --- | --- | --- |\n",
    "|[`ds.map_batches()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html#ray.data.Dataset.map_batches \"ray.data.Dataset.map_batches\")|One-to-one|Apply a given function to batches of records of this dataset.|\n",
    "|[`ds.add_column()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.add_column.html#ray.data.Dataset.add_column \"ray.data.Dataset.add_column\")|One-to-one|Apply a given function to batches of records to create a new column.|\n",
    "|[`ds.drop_columns()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.add_column.html#ray.data.Dataset.add_column \"ray.data.Dataset.add_column\")|One-to-one|Drop the given columns from the dataset.|\n",
    "|[`ds.split()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.split.html#ray.data.Dataset.split \"ray.data.Dataset.split\")|One-to-one|Split the dataset into N disjoint pieces.|\n",
    "|[`ds.repartition(shuffle=False)`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.repartition.html#ray.data.Dataset.repartition \"ray.data.Dataset.repartition\")|One-to-one|Repartition the dataset into N blocks, without shuffling the data.|\n",
    "|[`ds.repartition(shuffle=True)`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.repartition.html#ray.data.Dataset.repartition \"ray.data.Dataset.repartition\")|All-to-all|Repartition the dataset into N blocks, shuffling the data during repartition.|\n",
    "|[`ds.random_shuffle()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.random_shuffle.html#ray.data.Dataset.random_shuffle \"ray.data.Dataset.random_shuffle\")|All-to-all|Randomly shuffle the elements of this dataset.|\n",
    "|[`ds.sort()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.sort.html#ray.data.Dataset.sort \"ray.data.Dataset.sort\")|All-to-all|Sort the dataset by a sortkey.|\n",
    "|[`ds.groupby()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.groupby.html#ray.data.Dataset.groupby \"ray.data.Dataset.groupby\")|All-to-all|Group the dataset by a groupkey.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tip\n",
    ">\n",
    "> Datasets also provides the convenience transformation methods [`ds.map()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map.html#ray.data.Dataset.map \"ray.data.Dataset.map\"), [`ds.flat_map()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.flat_map.html#ray.data.Dataset.flat_map \"ray.data.Dataset.flat_map\"), and [`ds.filter()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.filter.html#ray.data.Dataset.filter \"ray.data.Dataset.filter\"), which are not vectorized (slower than [`ds.map_batches()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html#ray.data.Dataset.map_batches \"ray.data.Dataset.map_batches\")), but may be useful for development.\n",
    "\n",
    "The following is an example to make use of those transformation APIs for processing the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "ds = ray.data.read_csv(\"iris.csv\")\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.repartition(5)\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find rows with sepal.length < 5.5 and petal.length > 3.5.\n",
    "def transform_batch(batch):\n",
    "    out = (batch[\"sepal.length\"] < 5.5) & (batch[\"petal.length\"] < 3.5)\n",
    "    return { 'out' : out }\n",
    "\n",
    "# Map processing the dataset.\n",
    "ds.map_batches(transform_batch).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into 2 datasets\n",
    "ds.split(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset.\n",
    "ds = ds.random_shuffle()\n",
    "ds.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the variety.\n",
    "ds.groupby(\"variety\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing User-defined Functions (UDFs)\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "User-defined functions (UDFs) are routines that apply on one row (e.g. [`.map()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map.html#ray.data.Dataset.map \"ray.data.Dataset.map\")) or a batch of rows (e.g. [`.map_batches()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html#ray.data.Dataset.map_batches \"ray.data.Dataset.map_batches\")) of a dataset. UDFs let you express your customized business logic in transformations. Here we will focus on [`.map_batches()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html#ray.data.Dataset.map_batches \"ray.data.Dataset.map_batches\") as it's the primary mapping API in Datasets.\n",
    "\n",
    "Here are the basics that you need to know about UDFs:\n",
    "\n",
    "-   A UDF can be either a function, or if using the [actor compute strategy](https://docs.ray.io/en/latest/data/transforming-datasets.html#transform-datasets-compute-strategy), a [callable class](https://docs.ray.io/en/latest/data/transforming-datasets.html#transform-datasets-callable-classes).\n",
    "\n",
    "-   Select the UDF input [batch format](https://docs.ray.io/en/latest/data/transforming-datasets.html#transform-datasets-batch-formats) using the `batch_format` argument.\n",
    "\n",
    "-   The UDF output type determines the Dataset schema of the transformation result.\n",
    "\n",
    "### Callable Class UDFs[](https://docs.ray.io/en/latest/data/transforming-datasets.html#callable-class-udfs \"Permalink to this headline\")\n",
    "\n",
    "When using the actor compute strategy, per-row and per-batch UDFs can also be *callable classes*, i.e. classes that implement the `__call__` magic method. The constructor of the class can be used for stateful setup, and will be only invoked once per worker actor.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> These transformation APIs take the uninstantiated callable class as an argument, not an instance of the class.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ray.data.read_csv(\"iris.csv\")\n",
    "\n",
    "def is_long(record):\n",
    "    return { 'out' : record['sepal.length'] > 4.7 }\n",
    "\n",
    "ds.map(is_long).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_long_batch(batch):\n",
    "    output = batch['sepal.length'] > 4.7\n",
    "    return { 'out' : output } \n",
    "    \n",
    "ds.map_batches(is_long_batch).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class ModelUDF:\n",
    "    def __init__(self):\n",
    "        self.model = lambda d: d[\"sepal.length\"] > 4.7\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        # Apply model.\n",
    "        output = self.model(batch)\n",
    "        return { 'out' : output }\n",
    "\n",
    "ds.map_batches(ModelUDF, compute=ray.data.ActorPoolStrategy()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more info on...\n",
    "* batch formats\n",
    "* zero-copy\n",
    "* compute strategy\n",
    "\n",
    "...refer to https://docs.ray.io/en/releases-2.6.1/data/transforming-data.html#transforming-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more info on...\n",
    "* batch formats\n",
    "* zero-copy\n",
    "* compute strategy\n",
    "\n",
    "...refer to https://docs.ray.io/en/releases-2.6.1/data/transforming-data.html#transforming-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Serve\n",
    "\n",
    "Key principles behind Ray and its libraries are\n",
    "* Performance\n",
    "* Developer experience and simplicity\n",
    "\n",
    "Serve is a framework for serving ML applications\n",
    "\n",
    "<img src='https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Serve/serve_architecture.png' width=700/>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### Our First Service\n",
    "\n",
    "Let’s jump right in and get something simple up and running on Ray Serve.\n",
    "\n",
    "__Roadmap to initial chat app on serve__\n",
    "    \n",
    "1. Discover serve deployments via Hello World example\n",
    "1. Replace placeholder \"Hello World\" logic with Huggingface transformers chatbot\n",
    "1. Reserve GPU resources for our chatbot service\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import ray\n",
    "from ray.train import ScalingConfig, RunConfig\n",
    "from ray.train.xgboost import XGBoostTrainer\n",
    "from ray.train.xgboost import XGBoostPredictor\n",
    "\n",
    "from ray import serve\n",
    "import requests, json\n",
    "from starlette.requests import Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class Chat:\n",
    "    def __init__(self, msg: str):\n",
    "        self._msg = msg # initial state\n",
    "\n",
    "    async def __call__(self, request: Request) -> dict:\n",
    "        data = await request.json()\n",
    "        data = json.loads(data)\n",
    "        return {\"result\": self.get_response(data['input']) }\n",
    "    \n",
    "    def get_response(self, message: str) -> str:\n",
    "        return self._msg + message\n",
    "\n",
    "handle = serve.run(Chat.bind(msg=\"Yes... \"), name='hello_world')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test it as an HTTP endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_json = '{ \"input\" : \"hello\" }'\n",
    "requests.post(\"http://localhost:8000/\", json = sample_json).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "__Lab activity: implement a web service with Ray Serve__\n",
    "    \n",
    "The following function will calculate the approximate loan payment for a car.\n",
    "    \n",
    "```python\n",
    " def monthly_payment(total_price, rate, years_of_loan):\n",
    "    n = 365.25 # compounding periods\n",
    "    total_paid = total_price * (((1 + ((rate/100.0)/n)) ** (n*years_of_loan)))\n",
    "    per_month = total_paid / (12 * years_of_loan)\n",
    "    return per_month\n",
    "```\n",
    "   \n",
    "<br/>\n",
    "Deploy this calculator as a web service with Ray Serve!\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key APIs and concepts\n",
    "\n",
    "Using Ray Serve, a single Ray cluster can host multiple __applications__\n",
    "\n",
    "__Applications__ are coarse-grained chunks of functionality *which can be independently upgraded* (i.e., without impacting other applications on the same cluster)\n",
    "\n",
    "An __application__ is made up of one or more __deployments__\n",
    "\n",
    "<img src='https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Serve/deployment.png' width=600/>\n",
    "\n",
    "A __deployment__ is a smaller component which can\n",
    "* specify its own hardware are other resource requirements (like GPUs)\n",
    "* specify its own runtime environments (like libraries)\n",
    "* scale independently (including autoscaling)\n",
    "* maintain state (e.g., models)\n",
    "\n",
    "We can use __deployments__ to achieve *separation of concerns* -- e.g., separating different models, chunks of business logic, or data conversion\n",
    "\n",
    "__Ingress deployments__ are typically accessed via HTTP, while other supporting deployments are typically accessed at runtime via a Python `ServeHandle` -- allowing any Serve component (or Ray code) to interact directly with other components as needed\n",
    "\n",
    "We create a __deployment__ by applying the `@serve.deployment` decorator to a regular Python class or function. We create and start an __application__ by calling `serve.run` on a deployment (typically an ingress deployment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo: calling a component from Python via a ServeHandle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = handle.get_response.remote('hello')\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to support maximal performance, values from remote calls, such as our response string here, are returned as object references (a bit like futures or promises in some frameworks). If we want to block, wait for the result to be ready, and retrieve it, we can use `ray.get(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.get(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo: observing application and deployment status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! serve status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.list_deployments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the Ray dashboard as well to see more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.delete('hello_world')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab solution: car payment calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# car payment calculation (approximate)\n",
    "\n",
    "def monthly_payment(cost, rate, years_of_loan):\n",
    "    n = 365.25\n",
    "    total_paid = cost * (((1 + ((rate/100.0)/n)) ** (n*years_of_loan)))\n",
    "    per_month = total_paid / (12 * years_of_loan)\n",
    "    return per_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_payment(40000, 2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import serve\n",
    "import requests, json\n",
    "from starlette.requests import Request\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class Chat:\n",
    "\n",
    "    async def __call__(self, request: Request) -> Dict:\n",
    "        data = await request.json()\n",
    "        data = json.loads(data)\n",
    "        return {\"result\": self.monthly_payment(data['cost'], data['rate'], data['years_of_loan']) }\n",
    "    \n",
    "    def monthly_payment(self, cost, rate, years_of_loan):\n",
    "        n = 365.25\n",
    "        total_paid = cost * (((1 + ((rate/100.0)/n)) ** (n*years_of_loan)))\n",
    "        per_month = total_paid / (12 * years_of_loan)\n",
    "        return per_month\n",
    "\n",
    "handle = serve.run(Chat.bind(), name='car_payment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.get(handle.monthly_payment.remote(40_000, 7, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_json = '{ \"cost\" : 40000, \"rate\" : 7, \"years_of_loan\" : 6 }'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.post(\"http://localhost:8000/\", json = sample_json).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.delete('car_payment')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
