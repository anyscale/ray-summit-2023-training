{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a53b6ec",
   "metadata": {},
   "source": [
    "# 4. Serving Scalable AI: Deploying Our Fine-Tuned ViT Model with Ray Serve\n",
    "---\n",
    "\n",
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_AI_Libraries/RAIL_Serve.png\" width=\"30%\" loading=\"lazy\">\n",
    "\n",
    "**Milestone 4: Ray Serve for Scalable Deployments**\n",
    "\n",
    "Building a robust, scalable, and efficient deployment pipeline for machine learning models is a crucial step in bringing AI solutions to the real world.\n",
    "\n",
    "Our journey so far has laid a strong foundation. We've fine-tuned our ViT model, optimized our data pipeline, and ensured that our model training scales efficiently. In this installment, we'll bridge the gap between development and production by deploying our fine-tuned ViT with Ray Serve.\n",
    "\n",
    "By the end of this notebook, you'll have a fully deployed, scalable, and efficient serving solution for our fine-tuned ViT model.\n",
    "\n",
    "**Featured Libraries**\n",
    "* [Ray Serve](https://docs.ray.io/en/latest/serve/index.html)\n",
    "    * A scalable model serving library for building complex inference services that scale across multiple machines efficiently and cost-effectively.\n",
    "* [Ray Data](https://docs.ray.io/en/latest/data/data.html)\n",
    "* [Ray Train](https://docs.ray.io/en/latest/train/train.html)\n",
    "* [Hugging Face `transformers`](https://huggingface.co/docs/transformers/index)\n",
    "* [Hugging Face `datasets`](https://huggingface.co/docs/datasets/index)\n",
    "\n",
    "**Table of Contents**\n",
    "1. [Introduction to Ray Serve](#1-introduction-to-ray-serve)\n",
    "    * Learn how to scale our deployment and load balance requests efficiently to meet real-world demands.\n",
    "2. [Load Components](#2-load-components)\n",
    "    * Saved model checkpoint\n",
    "    * Featurizer\n",
    "    * Sample image\n",
    "3. [Create a Serve Deployment](#3-create-a-serve-deployment)\n",
    "    * Set up Ray Serve to handle model deployment and serving.\n",
    "4. [Deploy the ViT Model](#4-deploy-the-vit-model)\n",
    "    * Deploy our fine-tuned ViT model as a scalable API endpoint, making it accessible for inference.\n",
    "5. [Send Some Test Traffic](#5-send-some-test-traffic)\n",
    "    * Test locally\n",
    "    * Test via HTTP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67e2f97",
   "metadata": {},
   "source": [
    "## 1. Introduction to Ray Serve <a class=\"anchor\" id=\"1-introduction-to-ray-serve\"></a>\n",
    "\n",
    "<img src='https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Serve/serve_architecture.png' width=700/>\n",
    "\n",
    "Ray Serve is a scalable model serving library, built on top of Ray, that eases the scaling transition from development to production. Here are some features that define the library:\n",
    "\n",
    "- **Scalable Serving**: Ray Serve allows us to easily scale our model serving across multiple nodes, handling high request volumes and ensuring low latency.\n",
    "\n",
    "- **Flexible Deployment**: Ray Serve offers flexibility in deploying models as REST APIs, gRPC services, or Python functions, catering to various deployment needs.\n",
    "\n",
    "- **Performance Optimizations for LLMs**: Serving large language models offers unique challenges that Serve meets with response streaming, dynamic request batching, multi-node/multi-GPU serving, and more.\n",
    "\n",
    "- **Monitoring and Auto-Scaling**: It provides monitoring tools and auto-scaling capabilities to ensure the health and performance of our deployed model.\n",
    "\n",
    "Let's go ahead and serve our image classifier from the saved checkpoint to see how this composes with the rest of our Ray pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcad6b2-7dfd-4824-9ba9-8ceaf0598e52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from transformers import ViTForImageClassification, TrainingArguments, Trainer, ViTImageProcessor\n",
    "\n",
    "import ray\n",
    "from ray.train import ScalingConfig, RunConfig\n",
    "\n",
    "from ray import serve\n",
    "import requests, json\n",
    "from starlette.requests import Request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40444ccd",
   "metadata": {},
   "source": [
    "## 2. Load Components <a class=\"anchor\" id=\"2-load-components\"></a>\n",
    "\n",
    "Let's start by fetching the minimal set of components we need for our first Ray Serve deployment. These are the fine-tuned model, the featurizer (for transforming incoming images), and some sample image that will act as user traffic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101967fa",
   "metadata": {},
   "source": [
    "### Load our fine-tuned model\n",
    "\n",
    "From our `user_storage`, we'll be able to fetch a training checkpoint. In this way, we can test inference locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8328dfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = '/mnt/shared_storage/summit_assets/TorchTrainer_2023-09-07_18-09-24/TorchTrainer_59d51_00000_0_2023-09-07_18-09-25/checkpoint_000000/checkpoint'\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(saved_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959ef851",
   "metadata": {},
   "source": [
    "### Load the featurizer\n",
    "\n",
    "This is the same featurizer that we used to transform data for the initial fine-tuning step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20f17b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'google/vit-base-patch16-224-in21k'\n",
    "\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76641fc",
   "metadata": {},
   "source": [
    "### Load a sample image\n",
    "\n",
    "You can choose your own image. For simplicity, let's select a fish photo from our [docs](https://docs.ray.io/en/latest/_images/huggingface_vit_batch_prediction_25_4.png)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9936b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"/mnt/shared_storage/summit_assets/fish.png\")\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdd2bf4",
   "metadata": {},
   "source": [
    "And then let's apply the featurizer to the sample image and pass this to the model to ensure that it works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b630b65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    features = feature_extractor(image, return_tensors='pt')\n",
    "    out = model(features['pixel_values']).logits\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79b9221",
   "metadata": {},
   "source": [
    "## 3. Create a Serve Deployment <a class=\"anchor\" id=\"3-create-a-serve-deployment\"></a>\n",
    "\n",
    "Ray Serve abstracts the complexities of building an HTTP server for serving machine learning models. It allows you to define a deployment class, specify initialization logic, and handle incoming requests, making it easier to deploy and serve models in a production environment.\n",
    "\n",
    "<img src='https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Serve/deployment.png' width=700/>\n",
    "\n",
    "Here's how a Ray Serve deployment works:\n",
    "\n",
    "1. **Serve Deployment Class**:\n",
    "   \n",
    "   - In this code, the `Predict` class is decorated with `@serve.deployment`. This decorator tells Ray Serve that this class should be deployed as a service, making its methods accessible over HTTP.\n",
    "\n",
    "2. **Initialization**:\n",
    "\n",
    "   - In the constructor (`__init__`) of the `Predict` class, the necessary components are initialized.\n",
    "   \n",
    "     - `self._feature_extractor` is a ViT model used for feature extraction from images.\n",
    "     - `self._model` is a ViT model used for image classification.\n",
    "         * `.eval()` method is called on `self._model`. This puts the model into evaluation mode, which disables operations like dropout and batch normalization that aren't needed during inference.\n",
    "\n",
    "3. **HTTP Request Handling**:\n",
    "\n",
    "   - The `__call__` method in the `Predict` class is used to handle incoming HTTP requests to the deployed service.\n",
    "\n",
    "   - Inside `__call__`, the following steps occur:\n",
    "     \n",
    "     - Asynchronously, it collects the POST request body, which is expected to contain a pickled image.\n",
    "     - It unpickles the serialized image data to obtain the image.\n",
    "     - It then calls the `get_response` method to make predictions on the image using the pretrained models.\n",
    "     - Finally, it returns the predictions as an HTTP response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c0b0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class Predict:\n",
    "    def __init__(self, featurizer_name, model_path):        \n",
    "        self._feature_extractor = ViTImageProcessor.from_pretrained(featurizer_name)\n",
    "        self._model = ViTForImageClassification.from_pretrained(model_path)        \n",
    "        self._model.eval()\n",
    "\n",
    "    def get_response(self, image):\n",
    "        with torch.no_grad():\n",
    "            inputs = self._feature_extractor(image, return_tensors='pt')\n",
    "            return self._model(inputs['pixel_values']).logits.numpy()\n",
    "    \n",
    "    async def __call__(self, request: Request):\n",
    "        import numpy as np\n",
    "        import io\n",
    "        from imageio import v3 as iio\n",
    "        from fastapi import Response\n",
    "\n",
    "        # async collect POST body\n",
    "        body = await request.body()\n",
    "        \n",
    "        # unpickle serialized data\n",
    "        image = pickle.loads(body)\n",
    "\n",
    "        # invoke existing business logic\n",
    "        return self.get_response(image)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b052201c",
   "metadata": {},
   "source": [
    "## 4. Deploy the ViT Model <a class=\"anchor\" id=\"#4-deploy-the-vit-model\"></a>\n",
    "\n",
    "1. **Deployment**\n",
    "\n",
    "In order to actually deploy the service, we must use the `.bind` method to create an instance of the `Predict` class with specific configurations. It returns a handle to the deployed service.\n",
    "\n",
    "2. **Server Execution**\n",
    "\n",
    "The `serve.run` function starts the Ray Serve server, which listens for incoming HTTP requests at the specified endpoint (in this case, the name 'classifier')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3191b666",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_handle = serve.run(Predict.bind(featurizer_name='google/vit-base-patch16-224-in21k',\n",
    "                                   model_path=saved_model_path), name='classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918acfb5",
   "metadata": {},
   "source": [
    "## 5. Send Some Test Traffic <a class=\"anchor\" id=\"5-send-some-test-traffic\"></a>\n",
    "\n",
    "Let's see if the core logic works by passing in our sample image directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680bec99",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.get(app_handle.get_response.remote(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b809fb7",
   "metadata": {},
   "source": [
    "Now we can test it via HTTP. You can send POST requests to the server's endpoint with image data. The server will process the image using the ViT models and return the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5bc973",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\"http://localhost:8000/\", data = pickle.dumps(image)) # uncompressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee392231",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843c7562",
   "metadata": {},
   "source": [
    "Feel free to continue playing around with this deployment. When you're done, we can use the following cell to terminate any active deployments and free up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db333fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
