{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Pipeline Optimization with Ray Data and Ray Train for ViT Image Classification\n",
    "---\n",
    "\n",
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_AI_Libraries/RAIL_Data.png\" width=\"30%\" loading=\"lazy\">\n",
    "\n",
    "**Milestone 3: Handling Big Data with Ray Data**\n",
    "\n",
    "Our previous notebook introduced Ray Train, enabling distributed training and resource efficiency. We've already fine-tuned our ViT model with enhanced scalability. Now, we're poised to extend our work by optimizing our data pipeline with Ray Data.\n",
    "\n",
    "In this notebook, we'll replace the `beans` dataset with our own image data from S3 and use Ray Data for distributed data processing and you'll see how it easily composes with Ray Train to scale these two MLOps stages.\n",
    "\n",
    "**Featured Libraries**\n",
    "* [Ray Data](https://docs.ray.io/en/latest/data/data.html)\n",
    "    * A scalable data processing library for ML workloads that provides flexible and performant APIs for scaling batch inference and data preprocessing and ingest.\n",
    "* [Ray Train](https://docs.ray.io/en/latest/train/train.html)\n",
    "* [Hugging Face `transformers`](https://huggingface.co/docs/transformers/index)\n",
    "* [Hugging Face `datasets`](https://huggingface.co/docs/datasets/index)\n",
    "\n",
    "**Table of Contents**\n",
    "1. [Introduction to Ray Data](#1-introduction-to-ray-data)\n",
    "    * Distributed data processing for ML training and inference.\n",
    "2. [Create a Ray Dataset](#2-create-a-ray-dataset)\n",
    "    * Read new images from S3.\n",
    "3. [Image Preprocessing](#3-image-preprocessing)\n",
    "    * Filter images.\n",
    "    * Featurize raw images.\n",
    "4. [Set-Up Training Logic](#4-launch-distributed-fine-tuning)\n",
    "    * Prepare Hugging Face training logic for Ray Train.\n",
    "5. [Launch Distributed Fine-Tuning](#5-access-training-results)\n",
    "    * Training at scale.\n",
    "6. [Perform Batch Inference with Ray Data](#6-perform-batch-inference-with-ray-data)\n",
    "    * Load the fine-tuned model from the checkpoint to map to batches of new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Ray Data <a class=\"anchor\" id=\"1-introduction-to-ray-data\"></a>\n",
    "\n",
    "Data is the lifeblood of machine learning, and its efficient handling can significantly impact the training process. As datasets grow larger and more complex, managing data becomes increasingly challenging. This is especially true if the scaling solution for your data meets an opinionated scaling solution for training, and this manual stitching introduces a lot of operational overhead.\n",
    "\n",
    "Here's the cliffnotes introduction to Ray Data:\n",
    "\n",
    "- **Efficient Data Loading**: Ray Data offers tools and optimizations for efficient data loading, ensuring that data is readily available when needed, reducing training bottlenecks.\n",
    "\n",
    "- **Parallel Data Processing**: With Ray Data, we can easily parallelize data preprocessing, transforming, and augmentation, which is crucial for accelerating training and enhancing model performance.\n",
    "\n",
    "- **Data Pipelines**: Ray Data allows us to create data pipelines that seamlessly integrate with Ray Train, streamlining the entire machine learning workflow from ingest and preprocessing to batch inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a Ray Dataset <a class=\"anchor\" id=\"2-create-a-ray-dataset\"></a>\n",
    "\n",
    "In the initial example, we used the `beans` dataset from Hugging Face, reading it in with their convenient `load_dataset` utility. Let's now try our hand at working with some larger, messier data to demonstrate how you can use Ray Data for distributed ingest and processsing for your ML pipeline.\n",
    "\n",
    "First, we must create a Ray Dataset, which is the standard way to load and exchange data in the Ray AI Libraries. Beginning with raw images of dogs and fish stored in S3, we'll use `read_images` and `union` to generate this Ray Dataset.\n",
    "\n",
    "Note: For the sake of time in class, we're limiting the number of images retrieved, but feel free to experiment with the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_images_path = 's3://anonymous@air-example-data-2/imagenette2/train/n02102040'\n",
    "fish_images_path = 's3://anonymous@air-example-data-2/imagenette2/train/n01440764'\n",
    "\n",
    "ray_ds_images = ray.data.read_images(dog_images_path).limit(200).union(ray.data.read_images(fish_images_path).limit(200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect a few images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_ds_images.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_ds_images.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "\n",
    "im = ray_ds_images.take(1)[0]['image']\n",
    "PIL.Image.fromarray(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load labels\n",
    "\n",
    "For this binary classification task, we're distinguishing between images of dogs and images of fish. For this, we'll need to fetch the ground truth labels, move those to shared storage, and load those (in this example, as a `csv`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp /home/ray/default/ray-summit-2023-training/Ray_AI_Libraries/labels.csv /mnt/cluster_storage/labels.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray_ds_labels = ray.data.read_csv('/mnt/shared_storage/summit_assets/labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect a few labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_ds_labels.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_ds_labels.take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zip images and labels together\n",
    "\n",
    "We can use [`zip`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.zip.html#ray.data.Dataset.zip) to combine the data and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_ds = ray_ds_images.zip(ray_ds_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_ds.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Image Preprocessing <a class=\"anchor\" id=\"3-image-preprocessing\"></a>\n",
    "\n",
    "### Filtering wonky images\n",
    "\n",
    "In real-world data, there are some problematic records. In this case, there are grayscale images without a proper 3rd axis in the image data tensor.\n",
    "\n",
    "Let's count them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_ds.map(lambda record:{'dims':record['image'].ndim}).groupby('dims').count().take_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a small number, so we can probably filter them out (there are statistical considerations to whether this is a useful move in general, but for us it will work)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_labeled_ds = labeled_ds.filter(lambda record: record['image'].ndim==3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurize images\n",
    "\n",
    "Much like we've done before, we need to use the associated ViT feature extractor to transform raw images to the format that the model expects. Applying this transformation is a great example of stateful transformation with Ray Data's `map_batches`.\n",
    "\n",
    "Note: You can also extend this general pattern for batch inference where you apply, a model for example, to batches of data to generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Featurizer:\n",
    "    def __init__(self):\n",
    "        self._model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
    "        self._feature_extractor = ViTImageProcessor.from_pretrained(self._model_name_or_path)\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        inputs = self._feature_extractor([x for x in batch['image']], return_tensors='pt')\n",
    "        return { 'pixel_values' : inputs['pixel_values'], 'labels' : batch['label'] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurized_ds = filtered_labeled_ds.map_batches(Featurizer, compute=ray.data.ActorPoolStrategy(size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurized_ds.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a train/test split\n",
    "\n",
    "At this point, our dataset is more or less ready. Since we have a single labeled dataset, we'll use `train_test_split` to create train/test subsets.\n",
    "\n",
    "> Note: this feature has some performance costs -- we may want to avoid this, by externally producing training/validation/test sets where possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset = featurized_ds.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set-Up Training Logic <a class=\"anchor\" id=\"4-set-up-training-logic\"></a>\n",
    "\n",
    "Everything below is the same, except for the following lines:\n",
    "\n",
    "```python\n",
    "    train_sh = get_dataset_shard(\"train\")\n",
    "    training = train_sh.iter_torch_batches(batch_size=64)\n",
    "    \n",
    "    val_sh = get_dataset_shard(\"valid\")\n",
    "    valid = val_sh.iter_torch_batches(batch_size=64)\n",
    "```\n",
    "\n",
    "This fetches the [`DataIterator`](https://docs.ray.io/en/master/data/api/data_iterator.html#ray.data.DataIterator) shard from a Ray Dataset and uses [`iter_torch_batches`](https://docs.ray.io/en/master/data/api/doc/ray.data.DataIterator.iter_torch_batches.html#ray.data.DataIterator.iter_torch_batches) to convert to the type that our framework (Hugging Face) expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from ray.train import ScalingConfig\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.train.huggingface.transformers import prepare_trainer, RayTrainReportCallback\n",
    "from transformers import ViTForImageClassification, TrainingArguments, Trainer, ViTImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(config):\n",
    "    import evaluate\n",
    "    from ray.train import get_dataset_shard\n",
    "    \n",
    "    train_sh = get_dataset_shard(\"train\")\n",
    "    training = train_sh.iter_torch_batches(batch_size=64)\n",
    "    \n",
    "    val_sh = get_dataset_shard(\"valid\")\n",
    "    valid = val_sh.iter_torch_batches(batch_size=64)\n",
    "\n",
    "    labels = config['labels']    \n",
    "    model = ViTForImageClassification.from_pretrained(config['model'], num_labels=len(labels))\n",
    "    \n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "    \n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    # Hugging Face Training Args + Trainer\n",
    "    training_args = TrainingArguments(\n",
    "      output_dir=\"/mnt/cluster_storage/output\",\n",
    "      evaluation_strategy=\"steps\",\n",
    "      eval_steps = 3,\n",
    "      per_device_train_batch_size=128,\n",
    "      logging_steps=2,\n",
    "      save_steps=4,\n",
    "      max_steps=10,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=training,\n",
    "        eval_dataset=valid,\n",
    "    )\n",
    "\n",
    "    callback = RayTrainReportCallback()\n",
    "    trainer.add_callback(callback)\n",
    "\n",
    "    trainer = prepare_trainer(trainer)\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Launch Distributed Fine-Tuning <a class=\"anchor\" id=\"5-launch-distributed-fine-tuning\"></a>\n",
    "\n",
    "This code is similar to the piece we encountered in the previous notebook with one small addition.\n",
    "\n",
    "`datasets` here specifies the Ray Datasets we'll be using in the training loop. Before, we loaded the Hugging Face datasets in the training function directly to avoid serialization errors when transferring the objects to the workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_trainer = TorchTrainer(\n",
    "    train_loop_per_worker= train_func, \n",
    "    train_loop_config= {'model':'google/vit-base-patch16-224-in21k', 'labels':ray_ds_labels.unique('label')},\n",
    "    scaling_config=ScalingConfig(num_workers=2, use_gpu=True),\n",
    "    run_config=ray.train.RunConfig(storage_path='/mnt/cluster_storage'),\n",
    "    datasets={\"train\": train_dataset, \"valid\": valid_dataset},\n",
    ")\n",
    "\n",
    "result = ray_trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = result.checkpoint.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Perform Batch Inference with Ray Data <a class=\"anchor\" id=\"6-perform-batch-inference-with-ray-data\"></a>\n",
    "\n",
    "Now that we have a fine-tuned model, let's load it from the Ray Train checkpoint to generate some predictions on our test set. To do this, we'll use Ray Data once again to load and featurize image batches and then apply the ViT model to generate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load test set images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_test_images_path = 's3://anonymous@air-example-data-2/imagenette2/val/n02102040'\n",
    "fish_test_images_path = 's3://anonymous@air-example-data-2/imagenette2/val/n01440764'\n",
    "\n",
    "ray_ds_test_images = ray.data.read_images(dog_test_images_path, mode=\"RGB\").limit(200).union(ray.data.read_images(fish_test_images_path, mode=\"RGB\").limit(200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = '/checkpoint'\n",
    "saved_model_path = checkpoint_path + suffix\n",
    "\n",
    "BATCH_SIZE = 100 # Bump this up to the largest batch size that can fit on your GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a class allows us to put the expensive pipeline loading and initialization code in the `__init__` constructor, which will run only once. The actual model inference logic is in the `__call__` method, which will be called for each batch.\n",
    "\n",
    "The `__call__` method takes a batch of data items, instead of a single one. In this case, the batch is a dict that has one key named \"image\", and the value is a Numpy array of images represented in `np.ndarray` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassifier:\n",
    "    def __init__(self):        \n",
    "        self._feature_extractor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "        self._model = ViTForImageClassification.from_pretrained(saved_model_path)\n",
    "        self._model.eval()\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        outputs = []\n",
    "        for image_array in batch[\"image\"]:\n",
    "            image = Image.fromarray(image_array)\n",
    "            input = self._feature_extractor(image, return_tensors='pt')\n",
    "            output = self._model(input['pixel_values']).logits.numpy(force=True)\n",
    "            outputs.append(output)\n",
    "\n",
    "        return {'results': outputs}\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the [`map_batches`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html#ray.data.Dataset.map_batches \"ray.data.Dataset.map_batches\") API to apply the model to the whole dataset.\n",
    "\n",
    "The first parameter of `map_batches` is the user-defined function (UDF), which can either be a function or a class. Since we are using a class in this case, the UDF will run as long-running [Ray actors](https://docs.ray.io/en/latest/ray-core/key-concepts.html#actors). For class-based UDFs, we use the `compute` argument to specify `ActorPoolStrategy` with the number of parallel actors. And the `batch_size` argument indicates the number of images in each batch.\n",
    "\n",
    "The `num_gpus` argument specifies the number of GPUs needed for each `ImageClassifier` instance. In this case, we want 1 GPU for each model replica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = ray_ds_test_images.map_batches(\n",
    "    ImageClassifier,\n",
    "    compute=ray.data.ActorPoolStrategy(size=2), # Use 2 GPUs. Change this number based on the number of GPUs in your cluster.\n",
    "    num_gpus=1,  # Specify 1 GPU per model replica.\n",
    "    batch_size=BATCH_SIZE # Use the largest batch size that can fit on our GPUs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.take_batch(5)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
