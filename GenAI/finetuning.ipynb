{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dreambooth: Few Shot Fine-Tuning for Personalized Stable Diffusion\n",
    "---\n",
    "\n",
    "In this notebook, we'll leverage Ray's capabilities to efficiently distribute the fine-tuning and inference processes for a text-to-image generation model. This strategy draws inspiration from the pioneering work of [Dreambooth](https://arxiv.org/abs/2208.12242), which introduced a methodology to tailor diffusion models according to individual subjects.\n",
    "\n",
    "By the conclusion of this guide, you'll not only possess a finely tuned model with the ability to portray a distinctive dog within a diverse array of scenarios, but you'll also cultivate a more profound comprehension of harnessing Ray for scaling up training and inference tasks across numerous computational nodes.\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "1. [Set-Up](#1-set-up)\n",
    "    * Define the file structure\n",
    "    * Download the base model\n",
    "    * Download the few-shot input images\n",
    "    * Display example image\n",
    "2. [Augmenting the Training Dataset](#2-augmenting-the-training-dataset)\n",
    "    * Define class for generating images\n",
    "    * Generate class images with Ray Data\n",
    "    * Display example image\n",
    "3. [Creating the Training Dataset](#3-create-the-training-dataset)\n",
    "    * Write prompts\n",
    "    * Build training dataset\n",
    "4. [Run Fine-Tuning](#4-run-fine-tuning)\n",
    "    * Import packages\n",
    "    * Define training logic\n",
    "    * Launch fine-tuning\n",
    "5. [Test the Fine-Tuned Model](#5-test-the-fine-tuned-model)\n",
    "    * Use Ray Data to generate images\n",
    "    * Display example image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the file structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a NFS mounted on every node of the cluster, so it's accessible to all worker nodes.\n",
    "storage_dir = \"/mnt/cluster_storage\"\n",
    "\n",
    "# The name of the model to be pulled from Hugging Face.\n",
    "base_model_name = \"CompVis/stable-diffusion-v1-4\"\n",
    "# The specific commit hash to download from.\n",
    "base_model_hash = \"b95be7d6f134c3a9e62ee616f310733567f069ce\"\n",
    "\n",
    "# Construct the directory path for the base model and its snapshots.\n",
    "base_model_dir = f\"{storage_dir}/base-model\"\n",
    "base_model_path = f\"{base_model_dir}/models--{base_model_name.replace('/', '--')}/snapshots/{base_model_hash}\"\n",
    "\n",
    "# Create a directory path for the fine-tuned model.\n",
    "fine_tuned_model_dir = f\"{storage_dir}/fine-tuned-model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory for ~5 images of the original subject matter.\n",
    "instance_images_dir = f\"{storage_dir}/instance-images\"\n",
    "\n",
    "# Directory for ~200 images of the same class as the subject for regularization.\n",
    "class_images_dir = f\"{storage_dir}/class-images\"\n",
    "\n",
    "# Directory for newly generated images from the fine-tuned model.\n",
    "output_images_dir = f\"{storage_dir}/output-images\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we'll create the image directories. The model directories will automatically be handled by Hugging Face when you specify a `cache_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_directories = [class_images_dir, instance_images_dir, output_images_dir]\n",
    "\n",
    "for directory in image_directories:\n",
    "    if os.path.exists(directory):\n",
    "        shutil.rmtree(directory)\n",
    "    os.makedirs(directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_download(\n",
    "    repo_id=base_model_name, revision=base_model_hash, cache_dir=base_model_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the few-shot input images\n",
    "\n",
    "> **Exercise**\n",
    "> \n",
    "> We're downloading five photos of a dog to fine-tune the Stable Diffusion model, but these images could be of anything! Instead of using these stock images, replace them with your favorite object or person.\n",
    ">\n",
    "> You should only need 3-5 instances of the subject matter to achieve decent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download five images of a unique subject matter (dog).\n",
    "snapshot_download(\n",
    "    \"diffusers/dog-example\",\n",
    "    local_dir=instance_images_dir,\n",
    "    repo_type=\"dataset\",\n",
    "    ignore_patterns=\".gitattributes\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display example image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_image = os.listdir(instance_images_dir)[0]\n",
    "image_path = os.path.join(instance_images_dir, example_image)\n",
    "\n",
    "display(Image(filename=image_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Augmenting the Training Dataset\n",
    "\n",
    "**Prior Preservation Loss in Text-to-Image Generation**\n",
    "\n",
    "In text-to-image synthesis, the \"prior preservation loss\" is a technique used in personalized text-to-image diffusion models that addresses the challenge of generating diverse images of a specific subject while maintaining the diversity of images of the same class.\n",
    "\n",
    "In other words, we want the model to be able to generate varied images of *our* dog but not make *all* generated dogs look like our dog. Generic dogs are our *prior*, and we're trying to *preserve* that through the fine-tuning process.\n",
    "\n",
    "**Two-Part Training Data** \n",
    "1. **Unique Identifier and Class:** Input images are paired with prompts containing unique identifiers and class names (e.g., \"A [unqtkn] dog\").\n",
    "2. **Autogenous Class-Specific Loss:** To prevent overfitting to specific instances, we let the base model itself generate class images (e.g. \"A dog\") to feed into the training dataset.\n",
    "\n",
    "Check out the [original paper](https://arxiv.org/pdf/2208.12242.pdf) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import torch\n",
    "import numpy as np\n",
    "from diffusers import DiffusionPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define class for generating images\n",
    "\n",
    "`StableDiffusionCallable` generates images using a pre-trained generative model from a specified directory. It initializes the model, generates a specified number of images per input prompt, and returns the generated images as a list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StableDiffusionCallable:\n",
    "    def __init__(self, model_dir, num_samples_per_prompt):\n",
    "        print(f\"Loading model from {model_dir}\")\n",
    "        self.pipeline = DiffusionPipeline.from_pretrained(\n",
    "            model_dir, torch_dtype=torch.float16\n",
    "        )\n",
    "        self.pipeline.set_progress_bar_config(disable=True)\n",
    "        self._num_samples = num_samples_per_prompt\n",
    "        self.pipeline.to(\"cuda\")  # Ray will let us guarantee a GPU is available\n",
    "\n",
    "    def __call__(self, prompt):\n",
    "        images = []\n",
    "        for i in range(self._num_samples):\n",
    "            image = self.pipeline(prompt[\"item\"]).images[0]\n",
    "            images.append(\n",
    "                np.array(image)\n",
    "            )  # This needs to be in the right format for writing.\n",
    "            print(f\"Generated image {i} of {self._num_samples}\")\n",
    "        return [{\"image\": image} for image in images]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Generate class images with Ray Data\n",
    "\n",
    "We'll use the base Stable Diffusion model to generate our class images, and use Ray Data to scale this across our available GPUs (2 for today, but can be more and can be a mix of CPUs and GPUs). Since we pass `n` prompts in and get `m` images out and want a collection of all the samples at the end, we'll use `flat_map`.\n",
    "\n",
    "Interested in learning more about Ray Data's transformation semantics? Click [here](https://docs.ray.io/en/latest/data/transforming-data.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = \"dog\"  # General category of the subject matter.\n",
    "prompts_list = [f\"photo of a {class_name}\", f\"photo of a {class_name}\"]\n",
    "\n",
    "num_samples_per_prompt = 100  # Number recommended in original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Ray Dataset.\n",
    "prompt_ds = ray.data.from_items(prompts_list)\n",
    "\n",
    "# Use `flat_map` to turn `n` prompts into `m` images.\n",
    "images_ds = prompt_ds.repartition(2).flat_map(\n",
    "    StableDiffusionCallable,\n",
    "    compute=ray.data.ActorPoolStrategy(\n",
    "        size=2\n",
    "    ),  # fixed pool of 2 workers since we have 2 GPUs; no autoscaling.\n",
    "    fn_constructor_args=(base_model_path, num_samples_per_prompt),\n",
    "    num_gpus=1,  # This is per worker\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ray Datasets are lazy by default, so any transformations are only triggered when the dataset is consumed, like with `write_images()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_ds.write_images(class_images_dir, column=\"image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display example image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_class_image = os.listdir(class_images_dir)[0]\n",
    "class_image_path = os.path.join(class_images_dir, example_class_image)\n",
    "\n",
    "display(Image(filename=class_image_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create the Training Dataset\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/genai/training-data-pipeline.jpg\" width=\"100%\" loading=\"lazy\">|\n",
    "|:-:|\n",
    "|Pipeline that duplicates instance images.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_token = \"unqtkn\"  # Unique identifier for the individual subject matter.\n",
    "instance_prompt = f\"photo of {unique_token} {class_name}\"\n",
    "class_prompt = f\"photo of a {class_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_train_dataset, collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_train_dataset(\n",
    "    base_model_path,\n",
    "    instance_images_dir,\n",
    "    class_images_dir,\n",
    "    instance_prompt,\n",
    "    class_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Fine-Tuning\n",
    "\n",
    "First, we'll set up the training logic in `train_fn`for the fine-tuning loop. This involves the following:\n",
    "\n",
    "1. **Loading Pre-trained Models**: Pre-trained components, including a text encoder, noise scheduler, VAE, and UNet model, are loaded.\n",
    "\n",
    "2. **Training Preparation**: Models are configured for training, and the VAE is moved to the appropriate device.\n",
    "\n",
    "3. **Optimizer Configuration**: An AdamW optimizer is set up to optimize trainable parameters.\n",
    "\n",
    "4. **Data Loading**: Training data is loaded.\n",
    "\n",
    "5. **Training Loop**: For a set number of epochs:\n",
    "   - Data is processed in batches.\n",
    "   - Images are encoded using the VAE and scaled.\n",
    "   - Noise is added using a diffusion process.\n",
    "   - Text embeddings are generated.\n",
    "   - The UNet predicts noise residuals.\n",
    "   - Loss is computed and gradients are backpropagated.\n",
    "   - Model parameters are updated with gradient clipping.\n",
    "   - Training results are reported.\n",
    "\n",
    "6. **Pipeline Creation and Saving**: After training, a pipeline is created using trained modules. The pipeline is saved if the process is the primary one.\n",
    "\n",
    "After this, we can run distributed fine-tuning with Ray Train by wrapping the training and scaling logic in Ray's `TorchTrainer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "from diffusers import DiffusionPipeline\n",
    "from ray import train\n",
    "from ray.train import ScalingConfig\n",
    "from ray.train.torch import TorchTrainer\n",
    "\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "from utils import prior_preserving_loss, get_target, load_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training logic\n",
    "\n",
    "There's a lot going on in the following function (and even more functions tucked away in `utils.py`). The important thing to focus on, however, is that porting this training logic to Ray involves minimal code changes.\n",
    "\n",
    "Your training logic will be different according to your specific use case, and running this in a distributed setting only involves wrapping this function with a Ray `Trainer` and defining some scaling logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(config):\n",
    "\n",
    "    # Load pre-trained models.\n",
    "    (\n",
    "        text_encoder, # Pre-trained text encoder\n",
    "        noise_scheduler, # Noise scheduler for diffusion process\n",
    "        vae,  # Variational Autoencoder\n",
    "        unet,  # UNet-based generative model\n",
    "        unet_trainable_parameters,  # Trainable parameters of the UNet\n",
    "        text_trainable_parameters,  # Trainable parameters of the text encoder\n",
    "    ) = load_models(config)  # Load pre-trained models using provided configuration\n",
    "\n",
    "    # Set models in training mode.\n",
    "    text_encoder.train()\n",
    "    unet.train()\n",
    "\n",
    "    # Prepare models for training.\n",
    "    text_encoder = train.torch.prepare_model(text_encoder)\n",
    "    unet = train.torch.prepare_model(unet)\n",
    "    # Manually move VAE to device as `prepare_model` can't be used on\n",
    "    # non-training models.\n",
    "    vae = vae.to(train.torch.get_device())\n",
    "\n",
    "    # Use the AdamW optimizer to work with bfloat16 weights.\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        itertools.chain(unet_trainable_parameters, text_trainable_parameters),\n",
    "        lr=config[\"lr\"],\n",
    "    )\n",
    "\n",
    "    # Load the training dataset.\n",
    "    train_dataset = train.get_dataset_shard(\"train\")\n",
    "\n",
    "    ########################\n",
    "    # Start: Training loop #\n",
    "    ########################\n",
    "    num_train_epochs = config[\"num_epochs\"]\n",
    "\n",
    "    print(f\"Running {num_train_epochs} epochs.\")\n",
    "\n",
    "    global_step = 0\n",
    "    for _ in range(num_train_epochs):\n",
    "        if global_step >= config[\"max_train_steps\"]:\n",
    "            print(f\"Stopping training after reaching {global_step} steps...\")\n",
    "            break\n",
    "        \n",
    "        # Iterate through batches in the training dataset.\n",
    "        for _, batch in enumerate(\n",
    "            train_dataset.iter_torch_batches(\n",
    "                batch_size=config[\"train_batch_size\"],\n",
    "                device=train.torch.get_device(),\n",
    "            )\n",
    "        ):\n",
    "            # Collate the batch and convert to bfloat16 format.\n",
    "            batch = collate(batch, torch.bfloat16)\n",
    "\n",
    "            # Zero the gradients of the optimizer.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Encode images into latent space using the VAE and scale by a constant.\n",
    "            latents = vae.encode(batch[\"images\"]).latent_dist.sample() * 0.18215\n",
    "\n",
    "            # Sample noise that we'll add to the latents\n",
    "            noise = torch.randn_like(latents)\n",
    "            bsz = latents.shape[0]\n",
    "            # Sample a random timestep for each image\n",
    "            timesteps = torch.randint(\n",
    "                0,\n",
    "                noise_scheduler.config.num_train_timesteps,\n",
    "                (bsz,),\n",
    "                device=latents.device,\n",
    "            )\n",
    "            timesteps = timesteps.long()\n",
    "\n",
    "            # Add noise to the latents according to the noise magnitude at each timestep\n",
    "            # (this is the forward diffusion process)\n",
    "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "            # Get the text embedding for conditioning from the text encoder.\n",
    "            encoder_hidden_states = text_encoder(batch[\"prompt_ids\"])[0]\n",
    "\n",
    "            # Predict the noise residual using the UNet model.\n",
    "            model_pred = unet(\n",
    "                noisy_latents.to(train.torch.get_device()),\n",
    "                timesteps.to(train.torch.get_device()),\n",
    "                encoder_hidden_states.to(train.torch.get_device()),\n",
    "            ).sample\n",
    "\n",
    "            # Calculate target values for loss computation.\n",
    "            target = get_target(noise_scheduler, noise, latents, timesteps)\n",
    "\n",
    "            # Calculate the loss that preservers the model's prior.\n",
    "            loss = prior_preserving_loss(\n",
    "                model_pred, target, config[\"prior_loss_weight\"]\n",
    "            )\n",
    "\n",
    "            # Backpropogate the loss.\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping before optimizer stepping to prevent exploding gradients.\n",
    "            clip_grad_norm_(\n",
    "                itertools.chain(unet_trainable_parameters, text_trainable_parameters),\n",
    "                config[\"max_grad_norm\"],\n",
    "            )\n",
    "\n",
    "            # Update model parameters using the optimizer.\n",
    "            optimizer.step()  # Step all optimizers.\n",
    "\n",
    "            # Increment global step counter and report training results.\n",
    "            global_step += 1\n",
    "            results = {\n",
    "                \"step\": global_step,\n",
    "                \"loss\": loss.detach().item(),\n",
    "            }\n",
    "            train.report(results)\n",
    "\n",
    "            # Check if the maximum training steps have been reached.\n",
    "            if global_step >= config[\"max_train_steps\"]:\n",
    "                break\n",
    "    ######################\n",
    "    # End: Training loop #\n",
    "    ######################\n",
    "\n",
    "    # Create pipeline using the trained modules and save it.\n",
    "    if train.get_context().get_world_rank() == 0:\n",
    "        pipeline = DiffusionPipeline.from_pretrained(\n",
    "            config[\"model_dir\"],\n",
    "            text_encoder=text_encoder.module,\n",
    "            unet=unet.module,\n",
    "        )\n",
    "        pipeline.save_pretrained(config[\"output_dir\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of GPUs available\n",
    "num_workers = 2\n",
    "\n",
    "# Set hyperparameters per https://huggingface.co/blog/dreambooth\n",
    "train_loop_config = {\n",
    "    \"model_dir\": base_model_path,\n",
    "    \"output_dir\": fine_tuned_model_dir,\n",
    "    \"instance_images_dir\": instance_images_dir,\n",
    "    \"instance_prompt\": instance_prompt,\n",
    "    \"class_images_dir\": class_images_dir,\n",
    "    \"class_prompt\": class_prompt,\n",
    "    \"train_batch_size\": 2,\n",
    "    \"lr\": 5e-6,\n",
    "    \"num_epochs\": 10,\n",
    "    \"max_train_steps\": 400,\n",
    "    \"prior_loss_weight\": 1.0,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"num_workers\": num_workers,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Ray Train's TorchTrainer to wrap training and scaling logic.\n",
    "trainer = TorchTrainer(\n",
    "    train_fn,\n",
    "    train_loop_config=train_loop_config,\n",
    "    scaling_config=ScalingConfig(\n",
    "        use_gpu=True,\n",
    "        num_workers=num_workers,\n",
    "    ),\n",
    "    datasets={\n",
    "        \"train\": train_dataset,\n",
    "    },\n",
    ")\n",
    "\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test the Fine-Tuned Model\n",
    "\n",
    "Now that we have our fine-tuned model, let's try to generate a few images of our unique subject matter.\n",
    "\n",
    "> **Exercise**\n",
    ">\n",
    "> Modify the prompts list to generate the same subject matter with different contexts, properties, views, and/or accessories. See if you can build a list of prompts that qualitatively probe the performance of the fine-tuned model. Can you spot weaknesses in the training dataset methodology?\n",
    "> \n",
    "> Check out the [Dreambooth paper](https://dreambooth.github.io/) for more ideas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_list = [f\"photo of a {unique_token} {class_name}\"] # Modify me with something exciting!\n",
    "num_samples_per_prompt = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Ray Data to generate images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_ds = ray.data.from_items(prompts_list)\n",
    "\n",
    "images_ds = prompt_ds.flat_map(\n",
    "    StableDiffusionCallable,\n",
    "    compute=ray.data.ActorPoolStrategy(\n",
    "        size=2\n",
    "    ),  # fixed pool of 2 workers since we have 2 GPUs; no autoscaling.\n",
    "    fn_constructor_args=(fine_tuned_model_dir, num_samples_per_prompt),\n",
    "    num_gpus=1,  # This is per worker\n",
    ").write_images(output_images_dir, column=\"image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display example image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_image = os.listdir(output_images_dir)[0]\n",
    "image_path = os.path.join(output_images_dir, example_image)\n",
    "\n",
    "display(Image(filename=image_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
