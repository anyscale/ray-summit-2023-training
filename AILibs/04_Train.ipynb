{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a76c85f-b220-4ea1-a0e4-3b23a50468e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import ViTForImageClassification, TrainingArguments, Trainer, ViTImageProcessor\n",
    "import torch\n",
    "import numpy as np\n",
    "import ray.train.huggingface.transformers\n",
    "from ray.air import ScalingConfig\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.train.huggingface.transformers.transformers_trainer import wrap_transformers_trainer\n",
    "from ray.train.huggingface.transformers._transformers_utils import TrainReportCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a48282-da25-4b38-b617-7c47f8be3da2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc908d6-2f76-4b26-b28a-ae3fc9646887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f6b328-3122-453b-8bc2-e2f1bc961079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081e2101-0550-420d-8aa8-93a61566439c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3baa37-dd68-4881-ad62-e8a5515777a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9594a1-61e6-42ea-83cb-92019414624b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_func(config):\n",
    "    from datasets import load_dataset\n",
    "    import evaluate\n",
    "    \n",
    "    ds = load_dataset('beans')\n",
    "    \n",
    "    model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
    "    feature_extractor = ViTImageProcessor.from_pretrained(model_name_or_path)\n",
    "\n",
    "    def transform(example_batch):\n",
    "        # Take a list of PIL images and turn them to pixel values\n",
    "        inputs = feature_extractor([x for x in example_batch['image']], return_tensors='pt')\n",
    "\n",
    "        # Don't forget to include the labels!\n",
    "        inputs['labels'] = example_batch['labels']\n",
    "        return inputs\n",
    "\n",
    "    prepared_ds = ds.with_transform(transform)\n",
    "    \n",
    "    labels = ds['train'].features['labels'].names\n",
    "    \n",
    "    model = ViTForImageClassification.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        num_labels=len(labels),\n",
    "        id2label={str(i): c for i, c in enumerate(labels)},\n",
    "        label2id={c: str(i) for i, c in enumerate(labels)}\n",
    "    )\n",
    "    \n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "    \n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    # Hugging Face Training Args + training Trainer\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "      output_dir=\"/mnt/local_storage/output\",\n",
    "      #per_device_train_batch_size=16,\n",
    "      evaluation_strategy=\"epoch\",\n",
    "      num_train_epochs=2,\n",
    "      logging_steps=10,\n",
    "      remove_unused_columns=False,\n",
    "    )\n",
    "    \n",
    "    def collate_fn(batch):\n",
    "        return {\n",
    "            'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "            'labels': torch.tensor([x['labels'] for x in batch])\n",
    "        }\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=collate_fn,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=prepared_ds[\"train\"],\n",
    "        eval_dataset=prepared_ds[\"validation\"],\n",
    "    )\n",
    "\n",
    "    callback = TrainReportCallback()\n",
    "    trainer.add_callback(callback)\n",
    "\n",
    "    trainer = wrap_transformers_trainer(trainer)\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad9913f-b44e-40c7-89c9-1cf61a9eecf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray_trainer = TorchTrainer(\n",
    "    train_func, scaling_config=ScalingConfig(num_workers=2, use_gpu=True),\n",
    "    run_config=ray.air.RunConfig(storage_path='/mnt/cluster_storage')\n",
    ")\n",
    "ray_trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce288659-f97b-4a0c-9278-4e1f584d60da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f3c02f-b0a7-4ec6-9afe-78ce8ec9ea3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
