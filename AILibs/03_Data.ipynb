{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6d1a782-76ce-4280-bc8a-141fec71eeeb",
   "metadata": {},
   "source": [
    "# Ray Datasets: Distributed Data Preprocessing\n",
    "\n",
    "Ray Datasets are the standard way to load and exchange data in Ray libraries and applications. They provide basic distributed data transformations such as maps ([`map_batches`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html#ray.data.Dataset.map_batches \"ray.data.Dataset.map_batches\")), global and grouped aggregations ([`GroupedDataset`](https://docs.ray.io/en/latest/data/api/doc/ray.data.grouped_dataset.GroupedDataset.html#ray.data.grouped_dataset.GroupedDataset \"ray.data.grouped_dataset.GroupedDataset\")), and shuffling operations ([`random_shuffle`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.random_shuffle.html#ray.data.Dataset.random_shuffle \"ray.data.Dataset.random_shuffle\"), [`sort`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.sort.html#ray.data.Dataset.sort \"ray.data.Dataset.sort\"), [`repartition`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.repartition.html#ray.data.Dataset.repartition \"ray.data.Dataset.repartition\")), and are compatible with a variety of file formats, data sources, and distributed frameworks.\n",
    "\n",
    "Here's an overview of the integrations with other processing frameworks, file formats, and supported operations, as well as a glimpse at the Ray Datasets API.\n",
    "\n",
    "Check the [Input/Output reference](https://docs.ray.io/en/latest/data/api/input_output.html#input-output) to see if your favorite format is already supported.\n",
    "\n",
    "<img src='https://docs.ray.io/en/releases-2.6.1/_images/dataset.svg' width=80%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed01eea0-c80c-41cc-8e56-5bc34954d1f5",
   "metadata": {},
   "source": [
    "Data Loading and Preprocessing for ML Training[](https://docs.ray.io/en/latest/data/dataset.html#data-loading-and-preprocessing-for-ml-training \"Permalink to this headline\")\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Use Ray Datasets to load and preprocess data for distributed [ML training pipelines](https://docs.ray.io/en/latest/train/train.html#train-docs). Compared to other loading solutions, Datasets are more flexible (e.g., can express higher-quality per-epoch global shuffles) and provides [higher overall performance](https://www.anyscale.com/blog/why-third-generation-ml-platforms-are-more-performant).\n",
    "\n",
    "Use Datasets as a last-mile bridge from storage or ETL pipeline outputs to distributed applications and libraries in Ray. Don't use it as a replacement for more general data processing systems.\n",
    "\n",
    "<img src='https://docs.ray.io/en/releases-2.6.1/_images/dataset-loading-1.png' width=50%/>\n",
    "\n",
    "To learn more about the features Datasets supports, read the [Datasets User Guide](https://docs.ray.io/en/latest/data/user-guide.html#data-user-guide).\n",
    "\n",
    "Datasets for Parallel Compute[](https://docs.ray.io/en/latest/data/dataset.html#datasets-for-parallel-compute \"Permalink to this headline\")\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Datasets also simplify general purpose parallel GPU and CPU compute in Ray; for instance, for [GPU batch inference](https://docs.ray.io/en/latest/ray-overview/use-cases.html#ref-use-cases-batch-infer). They provide a higher-level API for Ray tasks and actors for such embarrassingly parallel compute, internally handling operations like batching, pipelining, and memory management.\n",
    "\n",
    "<img src='https://docs.ray.io/en/releases-2.6.1/_images/stream-example.png' width=60%/>\n",
    "\n",
    "As part of the Ray ecosystem, Ray Datasets can leverage the full functionality of Ray's distributed scheduler, e.g., using actors for optimizing setup time and GPU scheduling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed3bdea-6753-4d79-9435-8b9ccd3ff44b",
   "metadata": {},
   "source": [
    "Datasets[](https://docs.ray.io/en/latest/data/key-concepts.html#datasets \"Permalink to this headline\")\n",
    "------------------------------------------------------------------------------------------------------\n",
    "\n",
    "A Dataset consists of a list of Ray object references to *blocks*. Each block holds a set of items in either [Arrow table format](https://arrow.apache.org/docs/python/data.html#tables) or a Python list (for non-tabular data). For ML use cases, Datasets also natively supports mixing [Tensor](https://docs.ray.io/en/latest/data/dataset-tensor-support.html#datasets-tensor-support) and tabular data. Having multiple blocks in a dataset allows for parallel transformation and ingest.\n",
    "\n",
    "The following figure visualizes a tabular dataset with three blocks, each block holding 1000 rows each:\n",
    "\n",
    "<img src='https://docs.ray.io/en/releases-2.6.1/_images/dataset-arch.svg' width=50%/>\n",
    "\n",
    "Since a Dataset is just a list of Ray object references, it can be freely passed between Ray tasks, actors, and libraries like any other object reference. This flexibility is a unique characteristic of Ray Datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba1d9c7-a6b8-483a-a672-43e57e755c5b",
   "metadata": {},
   "source": [
    "### Reading Data[](https://docs.ray.io/en/latest/data/key-concepts.html#reading-data \"Permalink to this headline\")\n",
    "\n",
    "Datasets uses Ray tasks to read data from remote storage. When reading from a file-based datasource (e.g., S3, GCS), it creates a number of read tasks proportional to the number of CPUs in the cluster. Each read task reads its assigned files and produces an output block:\n",
    "\n",
    "<img src='https://docs.ray.io/en/releases-2.6.1/_images/dataset-read.svg' width=60%/>\n",
    "\n",
    "The parallelism can also be manually specified, but the final parallelism for a read is always capped by the number of files in the underlying dataset. See the [Creating Datasets Guide](https://docs.ray.io/en/latest/data/creating-datasets.html#creating-datasets) for an in-depth guide on creating datasets.\n",
    "\n",
    "### Transforming Data[](https://docs.ray.io/en/latest/data/key-concepts.html#transforming-data \"Permalink to this headline\")\n",
    "\n",
    "Datasets can use either Ray tasks or Ray actors to transform datasets. By default, tasks are used. Actors can be specified using `compute=ActorPoolStrategy()`, which creates an autoscaling pool of Ray actors to process transformations. Using actors allows for expensive state initialization (e.g., for GPU-based tasks) to be cached:\n",
    "\n",
    "<img src='https://docs.ray.io/en/releases-2.6.1/_images/dataset-map.svg' width=60%/>)\n",
    "\n",
    "See the [Transforming Datasets Guide](https://docs.ray.io/en/releases-2.6.1/data/transforming-datasets.html#transforming-datasets) for an in-depth guide on transforming datasets.\n",
    "\n",
    "### Shuffling Data[](https://docs.ray.io/en/latest/data/key-concepts.html#shuffling-data \"Permalink to this headline\")\n",
    "\n",
    "Certain operations like *sort* or *groupby* require data blocks to be partitioned by value, or *shuffled*. Datasets uses tasks to implement distributed shuffles in a map-reduce style, using map tasks to partition blocks by value, and then reduce tasks to merge co-partitioned blocks together.\n",
    "\n",
    "You can also change just the number of blocks of a Dataset using [`repartition()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.repartition.html#ray.data.Dataset.repartition \"ray.data.Dataset.repartition\"). Repartition has two modes:\n",
    "\n",
    "1.  `shuffle=False` - performs the minimal data movement needed to equalize block sizes\n",
    "\n",
    "2.  `shuffle=True` - performs a full distributed shuffle\n",
    "\n",
    "<img src='https://docs.ray.io/en/releases-2.6.1/_images/dataset-shuffle.svg' width=60%/>\n",
    "\n",
    "Datasets shuffle can scale to processing hundreds of terabytes of data. See the [Performance Tips Guide](https://docs.ray.io/en/latest/data/performance-tips.html#shuffle-performance-tips) for an in-depth guide on shuffle performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadcdb7f-940e-4922-9dfa-02e307b15787",
   "metadata": {},
   "source": [
    "### Execution mode[](https://docs.ray.io/en/latest/data/key-concepts.html#execution-mode \"Permalink to this headline\")\n",
    "\n",
    "Most transformations are lazy. They don't execute until you consume a dataset or call [`Dataset.materialize()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.materialize.html#ray.data.Dataset.materialize \"ray.data.Dataset.materialize\").\n",
    "\n",
    "The transformations are executed in a streaming way, incrementally on the data and with operators processed in parallel, see [Streaming Execution](https://docs.ray.io/en/latest/data/dataset-internals.html#datasets-streaming-execution).\n",
    "\n",
    "For an in-depth guide on Datasets execution, read [Execution](https://docs.ray.io/en/latest/data/dataset-internals.html#datasets-execution).\n",
    "\n",
    "### Fault tolerance[](https://docs.ray.io/en/latest/data/key-concepts.html#fault-tolerance \"Permalink to this headline\")\n",
    "\n",
    "Datasets performs *lineage reconstruction* to recover data. If an application error or system failure occurs, Datasets recreates lost blocks by re-executing tasks.\n",
    "\n",
    "Fault tolerance isn't supported in two cases:\n",
    "\n",
    "-   If the original worker process that created the Dataset dies. This is because the creator stores the metadata for the [objects](https://docs.ray.io/en/latest/ray-core/fault_tolerance/objects.html#object-fault-tolerance) that comprise the Dataset.\n",
    "\n",
    "-   If you specify `compute=ActorPoolStrategy()` for transformations. This is because Datasets relies on [task-based fault tolerance](https://docs.ray.io/en/latest/ray-core/fault_tolerance/tasks.html#task-fault-tolerance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99176c4-d48b-436b-9912-273df84135a3",
   "metadata": {},
   "source": [
    "## Example operations: Transforming Datasets\n",
    "\n",
    "Datasets transformations take in datasets and produce new datasets. For example, *map* is a transformation that applies a [user-defined function](https://docs.ray.io/en/latest/data/transforming-datasets.html#transform-datasets-writing-udfs) on each dataset record and returns a new dataset as the result. Datasets transformations can be composed to express a chain of computations.\n",
    "\n",
    "There are two main types of transformations:\n",
    "\n",
    "-   One-to-one: each input block will contribute to only one output block, such as [`ds.map_batches()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html#ray.data.Dataset.map_batches \"ray.data.Dataset.map_batches\").\n",
    "\n",
    "-   All-to-all: input blocks can contribute to multiple output blocks, such as [`ds.random_shuffle()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.random_shuffle.html#ray.data.Dataset.random_shuffle \"ray.data.Dataset.random_shuffle\").\n",
    "\n",
    "Here is a table listing some common transformations supported by Ray Datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d069e124-d50e-4d3c-9287-b3cd07afd93f",
   "metadata": {},
   "source": [
    "Common Ray Datasets transformations.[](https://docs.ray.io/en/latest/data/transforming-datasets.html#id2 \"Permalink to this table\")\n",
    "\n",
    "| Transformation | Type | Description |\n",
    "| --- | --- | --- |\n",
    "|[`ds.map_batches()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html#ray.data.Dataset.map_batches \"ray.data.Dataset.map_batches\")|One-to-one|Apply a given function to batches of records of this dataset.|\n",
    "|[`ds.add_column()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.add_column.html#ray.data.Dataset.add_column \"ray.data.Dataset.add_column\")|One-to-one|Apply a given function to batches of records to create a new column.|\n",
    "|[`ds.drop_columns()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.add_column.html#ray.data.Dataset.add_column \"ray.data.Dataset.add_column\")|One-to-one|Drop the given columns from the dataset.|\n",
    "|[`ds.split()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.split.html#ray.data.Dataset.split \"ray.data.Dataset.split\")|One-to-one|Split the dataset into N disjoint pieces.|\n",
    "|[`ds.repartition(shuffle=False)`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.repartition.html#ray.data.Dataset.repartition \"ray.data.Dataset.repartition\")|One-to-one|Repartition the dataset into N blocks, without shuffling the data.|\n",
    "|[`ds.repartition(shuffle=True)`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.repartition.html#ray.data.Dataset.repartition \"ray.data.Dataset.repartition\")|All-to-all|Repartition the dataset into N blocks, shuffling the data during repartition.|\n",
    "|[`ds.random_shuffle()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.random_shuffle.html#ray.data.Dataset.random_shuffle \"ray.data.Dataset.random_shuffle\")|All-to-all|Randomly shuffle the elements of this dataset.|\n",
    "|[`ds.sort()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.sort.html#ray.data.Dataset.sort \"ray.data.Dataset.sort\")|All-to-all|Sort the dataset by a sortkey.|\n",
    "|[`ds.groupby()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.groupby.html#ray.data.Dataset.groupby \"ray.data.Dataset.groupby\")|All-to-all|Group the dataset by a groupkey.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31c42db-b757-42e9-af0c-45dbf6145d97",
   "metadata": {},
   "source": [
    "> Tip\n",
    ">\n",
    "> Datasets also provides the convenience transformation methods [`ds.map()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map.html#ray.data.Dataset.map \"ray.data.Dataset.map\"), [`ds.flat_map()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.flat_map.html#ray.data.Dataset.flat_map \"ray.data.Dataset.flat_map\"), and [`ds.filter()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.filter.html#ray.data.Dataset.filter \"ray.data.Dataset.filter\"), which are not vectorized (slower than [`ds.map_batches()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html#ray.data.Dataset.map_batches \"ray.data.Dataset.map_batches\")), but may be useful for development.\n",
    "\n",
    "The following is an example to make use of those transformation APIs for processing the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2529b5eb-4313-4425-adb5-e0aaaa666e53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "ds = ray.data.read_csv(\"iris.csv\")\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeae0471-d024-4dd2-b27e-a0f4085579dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91a296d-663c-4791-82c1-1415afdc7568",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = ds.repartition(5)\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83b1157-cd49-4979-8935-7a4ff31395f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find rows with sepal.length < 5.5 and petal.length > 3.5.\n",
    "def transform_batch(batch):\n",
    "    out = (batch[\"sepal.length\"] < 5.5) & (batch[\"petal.length\"] < 3.5)\n",
    "    return { 'out' : out }\n",
    "\n",
    "# Map processing the dataset.\n",
    "ds.map_batches(transform_batch).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca901a4-a82b-4629-be62-358cb0382661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the dataset into 2 datasets\n",
    "ds.split(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b59420-eabf-4470-9281-59794c1c7b4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sort the dataset by sepal.length.\n",
    "ds = ds.sort(\"sepal.length\")\n",
    "ds.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b4776b-ed2b-471f-bf3d-2393514369d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Shuffle the dataset.\n",
    "ds = ds.random_shuffle()\n",
    "ds.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aba549e-3422-4858-8ba6-7d7c893b0808",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Group by the variety.\n",
    "ds.groupby(\"variety\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e715a601-603b-409f-8e77-260c21b1ab7c",
   "metadata": {},
   "source": [
    "Writing User-defined Functions (UDFs)[](https://docs.ray.io/en/latest/data/transforming-datasets.html#writing-user-defined-functions-udfs \"Permalink to this headline\")\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "User-defined functions (UDFs) are routines that apply on one row (e.g. [`.map()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map.html#ray.data.Dataset.map \"ray.data.Dataset.map\")) or a batch of rows (e.g. [`.map_batches()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html#ray.data.Dataset.map_batches \"ray.data.Dataset.map_batches\")) of a dataset. UDFs let you express your customized business logic in transformations. Here we will focus on [`.map_batches()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html#ray.data.Dataset.map_batches \"ray.data.Dataset.map_batches\") as it's the primary mapping API in Datasets.\n",
    "\n",
    "Here are the basics that you need to know about UDFs:\n",
    "\n",
    "-   A UDF can be either a function, or if using the [actor compute strategy](https://docs.ray.io/en/latest/data/transforming-datasets.html#transform-datasets-compute-strategy), a [callable class](https://docs.ray.io/en/latest/data/transforming-datasets.html#transform-datasets-callable-classes).\n",
    "\n",
    "-   Select the UDF input [batch format](https://docs.ray.io/en/latest/data/transforming-datasets.html#transform-datasets-batch-formats) using the `batch_format` argument.\n",
    "\n",
    "-   The UDF output type determines the Dataset schema of the transformation result.\n",
    "\n",
    "### Callable Class UDFs[](https://docs.ray.io/en/latest/data/transforming-datasets.html#callable-class-udfs \"Permalink to this headline\")\n",
    "\n",
    "When using the actor compute strategy, per-row and per-batch UDFs can also be *callable classes*, i.e. classes that implement the `__call__` magic method. The constructor of the class can be used for stateful setup, and will be only invoked once per worker actor.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> These transformation APIs take the uninstantiated callable class as an argument, not an instance of the class.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eb7185-da7f-4a68-b2aa-500a98245104",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = ray.data.read_csv(\"iris.csv\")\n",
    "\n",
    "def is_long(record):\n",
    "    return { 'out' : record['sepal.length'] > 4.7 }\n",
    "\n",
    "ds.map(is_long).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421f0314-aa55-4f53-8896-6d935372596e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_long_batch(batch):\n",
    "    output = batch['sepal.length'] > 4.7\n",
    "    return { 'out' : output } \n",
    "    \n",
    "ds.map_batches(is_long_batch).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988c7a0d-3ad1-4a70-a723-490f01251dee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class ModelUDF:\n",
    "    def __init__(self):\n",
    "        self.model = lambda d: d[\"sepal.length\"] > 4.7\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        # Apply model.\n",
    "        output = self.model(batch)\n",
    "        return { 'out' : output }\n",
    "\n",
    "ds.map_batches(ModelUDF, compute=ray.data.ActorPoolStrategy()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a40bb82-e09b-44d2-b418-2a5ba62c8a07",
   "metadata": {},
   "source": [
    "For more info on...\n",
    "* batch formats\n",
    "* zero-copy\n",
    "* compute strategy\n",
    "\n",
    "...refer to https://docs.ray.io/en/releases-2.6.1/data/transforming-data.html#transforming-data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bacb20c-a7f8-4d2e-9be7-419b6bf69146",
   "metadata": {},
   "source": [
    "## Preparing image data for a classification training\n",
    "\n",
    "In our initial example, we'll consider dataset with explicit, external labels\n",
    "\n",
    "We'll start with the raw images, and use `read_images` and `union` to generate a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2347e3-74b7-4447-81f5-67647b37bb25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dogs_train = 's3://anonymous@air-example-data-2/imagenette2/train/n02102040'\n",
    "fish_train = 's3://anonymous@air-example-data-2/imagenette2/train/n01440764'\n",
    "\n",
    "train_ds_images = ray.data.read_images(dogs_train).limit(200).union(ray.data.read_images(fish_train).limit(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2ce06c-a4d7-42a5-9348-15f33c7b9dc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds_images.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576d03fa-8bbc-416a-8247-62f3503eff35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds_images.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe0a97e-ce49-4775-8c81-deb6916dcec2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "\n",
    "im = train_ds_images.take(1)[0]['image']\n",
    "PIL.Image.fromarray(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044bf90c-1a85-4318-9305-97451eb4e674",
   "metadata": {},
   "source": [
    "We'll move labels to shared storage and load those (in this example, as a csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9039ddb3-69ad-4641-bb56-f5b5d9524a78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! cp /home/ray/default/AILibs/labels.csv /mnt/cluster_storage/labels.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5349e18b-de3c-4caf-af55-d2ae0ae4020a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds_labels = ray.data.read_csv('/mnt/cluster_storage/labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2827493-4f43-4aee-8685-ec83bfa15a78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds_labels.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fec92ce-2a48-444b-a248-a33399fdea03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds_labels.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2165e4-4f6e-4886-a8a5-8a90377d6d5c",
   "metadata": {},
   "source": [
    "We can use `zip` to combine the data and labels\n",
    "\n",
    "> Hint: there are some performance costs to this approach, so we will also discuss alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c100b8b-258f-4d86-8d1b-6e654d586771",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labeled_ds = train_ds_images.zip(train_ds_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83771fde-76ef-4f85-b302-ebacfa55c028",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labeled_ds.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8846699-1e7b-405a-9815-c54bab4eb479",
   "metadata": {},
   "source": [
    "It turns out that, like a lot of real-world data, there are some problematic records. In this case, they are grayscale images without a proper 3rd axis in the image data tensor.\n",
    "\n",
    "Let's count those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e4923a-c3f2-4874-aafb-84638a0f6a78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labeled_ds.map(lambda record:{'dims':record['image'].ndim}).groupby('dims').count().take_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baa1236-b682-4fde-a310-92c3b39df12f",
   "metadata": {},
   "source": [
    "It's a small number, so we can probably filter them out (there are statistical considerations to whether this is a useful move in general, but for us it will work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528fc20c-5580-47fe-b7ed-c097a0ee19a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_labeled_ds = labeled_ds.filter(lambda record: record['image'].ndim==3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a2a8fe-2c8f-4e69-8a5d-0018bf27a713",
   "metadata": {},
   "source": [
    "Now we can \"featurize\" the images for our vision transformer. We'll use a pretrained feature extractor from Huggingface -- which is also a great example of stateful transformation with `map_batches`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b5c545-e4d3-4445-bf30-83c815b1b56b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor\n",
    "\n",
    "class Featurizer:\n",
    "    def __init__(self):\n",
    "        self._model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
    "        self._feature_extractor = ViTImageProcessor.from_pretrained(self._model_name_or_path)\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        inputs = self._feature_extractor([x for x in batch['image']], return_tensors='pt')\n",
    "        return { 'pixel_values' : inputs['pixel_values'], 'labels' : batch['label'] }\n",
    "\n",
    "filtered_labeled_ds.map_batches(Featurizer, compute=ray.data.ActorPoolStrategy(size=2)).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7bc351-f4cc-4324-b782-c7673f5e4077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "featurized_ds = filtered_labeled_ds.map_batches(Featurizer, compute=ray.data.ActorPoolStrategy(size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a566e2-5b91-4d53-9b7c-803cc9cdea3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "featurized_ds.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47473a0-e3a3-4077-9f8f-61e900c47f98",
   "metadata": {},
   "source": [
    "At this point, our dataset is more or less ready. Since we have a single labeled dataset, we'll use `train_test_split` to create train/test subsets.\n",
    "\n",
    "> Hint: this feature has some performance costs -- we may want to avoid this, by externally producing training/validation/test sets where possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0a43e2-9b92-46e3-9216-12323ef275f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset = featurized_ds.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df75436a-d973-47cd-91f5-4fd4ad568914",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
