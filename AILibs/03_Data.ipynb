{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6d1a782-76ce-4280-bc8a-141fec71eeeb",
   "metadata": {},
   "source": [
    "# Ray Datasets: Distributed Data Preprocessing\n",
    "\n",
    "Ray Datasets are the standard way to load and exchange data in Ray libraries and applications. They provide basic distributed data transformations such as maps ([`map_batches`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html#ray.data.Dataset.map_batches \"ray.data.Dataset.map_batches\")), global and grouped aggregations ([`GroupedDataset`](https://docs.ray.io/en/latest/data/api/doc/ray.data.grouped_dataset.GroupedDataset.html#ray.data.grouped_dataset.GroupedDataset \"ray.data.grouped_dataset.GroupedDataset\")), and shuffling operations ([`random_shuffle`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.random_shuffle.html#ray.data.Dataset.random_shuffle \"ray.data.Dataset.random_shuffle\"), [`sort`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.sort.html#ray.data.Dataset.sort \"ray.data.Dataset.sort\"), [`repartition`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.repartition.html#ray.data.Dataset.repartition \"ray.data.Dataset.repartition\")), and are compatible with a variety of file formats, data sources, and distributed frameworks.\n",
    "\n",
    "Here's an overview of the integrations with other processing frameworks, file formats, and supported operations, as well as a glimpse at the Ray Datasets API.\n",
    "\n",
    "Check the [Input/Output reference](https://docs.ray.io/en/latest/data/api/input_output.html#input-output) to see if your favorite format is already supported.\n",
    "\n",
    "![../_images/dataset.svg](https://docs.ray.io/en/latest/_images/dataset.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed01eea0-c80c-41cc-8e56-5bc34954d1f5",
   "metadata": {},
   "source": [
    "Data Loading and Preprocessing for ML Training[](https://docs.ray.io/en/latest/data/dataset.html#data-loading-and-preprocessing-for-ml-training \"Permalink to this headline\")\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Use Ray Datasets to load and preprocess data for distributed [ML training pipelines](https://docs.ray.io/en/latest/train/train.html#train-docs). Compared to other loading solutions, Datasets are more flexible (e.g., can express higher-quality per-epoch global shuffles) and provides [higher overall performance](https://www.anyscale.com/blog/why-third-generation-ml-platforms-are-more-performant).\n",
    "\n",
    "Use Datasets as a last-mile bridge from storage or ETL pipeline outputs to distributed applications and libraries in Ray. Don't use it as a replacement for more general data processing systems.\n",
    "\n",
    "<img src='https://docs.ray.io/en/latest/_images/dataset-loading-1.png' width=70%/>\n",
    "\n",
    "To learn more about the features Datasets supports, read the [Datasets User Guide](https://docs.ray.io/en/latest/data/user-guide.html#data-user-guide).\n",
    "\n",
    "Datasets for Parallel Compute[](https://docs.ray.io/en/latest/data/dataset.html#datasets-for-parallel-compute \"Permalink to this headline\")\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Datasets also simplify general purpose parallel GPU and CPU compute in Ray; for instance, for [GPU batch inference](https://docs.ray.io/en/latest/ray-overview/use-cases.html#ref-use-cases-batch-infer). They provide a higher-level API for Ray tasks and actors for such embarrassingly parallel compute, internally handling operations like batching, pipelining, and memory management.\n",
    "\n",
    "<img src='https://docs.ray.io/en/latest/_images/dataset-compute-1.png' width=60%/>\n",
    "\n",
    "As part of the Ray ecosystem, Ray Datasets can leverage the full functionality of Ray's distributed scheduler, e.g., using actors for optimizing setup time and GPU scheduling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed3bdea-6753-4d79-9435-8b9ccd3ff44b",
   "metadata": {},
   "source": [
    "Datasets[](https://docs.ray.io/en/latest/data/key-concepts.html#datasets \"Permalink to this headline\")\n",
    "------------------------------------------------------------------------------------------------------\n",
    "\n",
    "A Dataset consists of a list of Ray object references to *blocks*. Each block holds a set of items in either [Arrow table format](https://arrow.apache.org/docs/python/data.html#tables) or a Python list (for non-tabular data). For ML use cases, Datasets also natively supports mixing [Tensor](https://docs.ray.io/en/latest/data/dataset-tensor-support.html#datasets-tensor-support) and tabular data. Having multiple blocks in a dataset allows for parallel transformation and ingest.\n",
    "\n",
    "Informally, we refer to:\n",
    "\n",
    "-   A Dataset with Arrow blocks as a *Tabular Dataset*,\n",
    "\n",
    "-   A Dataset with Python list blocks as a *Simple Dataset*, and\n",
    "\n",
    "-   A Tabular Dataset with one or more tensor-type columns as a *Tensor Dataset*.\n",
    "\n",
    "The following figure visualizes a tabular dataset with three blocks, each block holding 1000 rows each:\n",
    "\n",
    "<img src='https://docs.ray.io/en/latest/_images/dataset-arch.svg' width=70%/>\n",
    "\n",
    "Since a Dataset is just a list of Ray object references, it can be freely passed between Ray tasks, actors, and libraries like any other object reference. This flexibility is a unique characteristic of Ray Datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba1d9c7-a6b8-483a-a672-43e57e755c5b",
   "metadata": {},
   "source": [
    "### Reading Data[](https://docs.ray.io/en/latest/data/key-concepts.html#reading-data \"Permalink to this headline\")\n",
    "\n",
    "Datasets uses Ray tasks to read data from remote storage. When reading from a file-based datasource (e.g., S3, GCS), it creates a number of read tasks proportional to the number of CPUs in the cluster. Each read task reads its assigned files and produces an output block:\n",
    "\n",
    "![../_images/dataset-read.svg](https://docs.ray.io/en/latest/_images/dataset-read.svg)\n",
    "\n",
    "The parallelism can also be manually specified, but the final parallelism for a read is always capped by the number of files in the underlying dataset. See the [Creating Datasets Guide](https://docs.ray.io/en/latest/data/creating-datasets.html#creating-datasets) for an in-depth guide on creating datasets.\n",
    "\n",
    "### Transforming Data[](https://docs.ray.io/en/latest/data/key-concepts.html#transforming-data \"Permalink to this headline\")\n",
    "\n",
    "Datasets can use either Ray tasks or Ray actors to transform datasets. By default, tasks are used. Actors can be specified using `compute=ActorPoolStrategy()`, which creates an autoscaling pool of Ray actors to process transformations. Using actors allows for expensive state initialization (e.g., for GPU-based tasks) to be cached:\n",
    "\n",
    "![../_images/dataset-map.svg](https://docs.ray.io/en/latest/_images/dataset-map.svg)\n",
    "\n",
    "See the [Transforming Datasets Guide](https://docs.ray.io/en/latest/data/transforming-datasets.html#transforming-datasets) for an in-depth guide on transforming datasets.\n",
    "\n",
    "### Shuffling Data[](https://docs.ray.io/en/latest/data/key-concepts.html#shuffling-data \"Permalink to this headline\")\n",
    "\n",
    "Certain operations like *sort* or *groupby* require data blocks to be partitioned by value, or *shuffled*. Datasets uses tasks to implement distributed shuffles in a map-reduce style, using map tasks to partition blocks by value, and then reduce tasks to merge co-partitioned blocks together.\n",
    "\n",
    "You can also change just the number of blocks of a Dataset using [`repartition()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.repartition.html#ray.data.Dataset.repartition \"ray.data.Dataset.repartition\"). Repartition has two modes:\n",
    "\n",
    "1.  `shuffle=False` - performs the minimal data movement needed to equalize block sizes\n",
    "\n",
    "2.  `shuffle=True` - performs a full distributed shuffle\n",
    "\n",
    "![../_images/dataset-shuffle.svg](https://docs.ray.io/en/latest/_images/dataset-shuffle.svg)\n",
    "\n",
    "Datasets shuffle can scale to processing hundreds of terabytes of data. See the [Performance Tips Guide](https://docs.ray.io/en/latest/data/performance-tips.html#shuffle-performance-tips) for an in-depth guide on shuffle performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadcdb7f-940e-4922-9dfa-02e307b15787",
   "metadata": {},
   "source": [
    "### Execution mode[](https://docs.ray.io/en/latest/data/key-concepts.html#execution-mode \"Permalink to this headline\")\n",
    "\n",
    "Most transformations are lazy. They don't execute until you consume a dataset or call [`Dataset.materialize()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.materialize.html#ray.data.Dataset.materialize \"ray.data.Dataset.materialize\").\n",
    "\n",
    "The transformations are executed in a streaming way, incrementally on the data and with operators processed in parallel, see [Streaming Execution](https://docs.ray.io/en/latest/data/dataset-internals.html#datasets-streaming-execution).\n",
    "\n",
    "For an in-depth guide on Datasets execution, read [Execution](https://docs.ray.io/en/latest/data/dataset-internals.html#datasets-execution).\n",
    "\n",
    "### Fault tolerance[](https://docs.ray.io/en/latest/data/key-concepts.html#fault-tolerance \"Permalink to this headline\")\n",
    "\n",
    "Datasets performs *lineage reconstruction* to recover data. If an application error or system failure occurs, Datasets recreates lost blocks by re-executing tasks.\n",
    "\n",
    "Fault tolerance isn't supported in two cases:\n",
    "\n",
    "-   If the original worker process that created the Dataset dies. This is because the creator stores the metadata for the [objects](https://docs.ray.io/en/latest/ray-core/fault_tolerance/objects.html#object-fault-tolerance) that comprise the Dataset.\n",
    "\n",
    "-   If you specify `compute=ActorPoolStrategy()` for transformations. This is because Datasets relies on [task-based fault tolerance](https://docs.ray.io/en/latest/ray-core/fault_tolerance/tasks.html#task-fault-tolerance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99176c4-d48b-436b-9912-273df84135a3",
   "metadata": {},
   "source": [
    "## Example operations: Transforming Datasets\n",
    "\n",
    "Datasets transformations take in datasets and produce new datasets. For example, *map* is a transformation that applies a [user-defined function](https://docs.ray.io/en/latest/data/transforming-datasets.html#transform-datasets-writing-udfs) on each dataset record and returns a new dataset as the result. Datasets transformations can be composed to express a chain of computations.\n",
    "\n",
    "There are two main types of transformations:\n",
    "\n",
    "-   One-to-one: each input block will contribute to only one output block, such as [`ds.map_batches()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html#ray.data.Dataset.map_batches \"ray.data.Dataset.map_batches\").\n",
    "\n",
    "-   All-to-all: input blocks can contribute to multiple output blocks, such as [`ds.random_shuffle()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.random_shuffle.html#ray.data.Dataset.random_shuffle \"ray.data.Dataset.random_shuffle\").\n",
    "\n",
    "Here is a table listing some common transformations supported by Ray Datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d069e124-d50e-4d3c-9287-b3cd07afd93f",
   "metadata": {},
   "source": [
    "Common Ray Datasets transformations.[](https://docs.ray.io/en/latest/data/transforming-datasets.html#id2 \"Permalink to this table\")\n",
    "\n",
    "| Transformation | Type | Description |\n",
    "| --- | --- | --- |\n",
    "|[`ds.map_batches()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html#ray.data.Dataset.map_batches \"ray.data.Dataset.map_batches\")|One-to-one|Apply a given function to batches of records of this dataset.|\n",
    "|[`ds.add_column()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.add_column.html#ray.data.Dataset.add_column \"ray.data.Dataset.add_column\")|One-to-one|Apply a given function to batches of records to create a new column.|\n",
    "|[`ds.drop_columns()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.add_column.html#ray.data.Dataset.add_column \"ray.data.Dataset.add_column\")|One-to-one|Drop the given columns from the dataset.|\n",
    "|[`ds.split()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.split.html#ray.data.Dataset.split \"ray.data.Dataset.split\")|One-to-one|Split the dataset into N disjoint pieces.|\n",
    "|[`ds.repartition(shuffle=False)`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.repartition.html#ray.data.Dataset.repartition \"ray.data.Dataset.repartition\")|One-to-one|Repartition the dataset into N blocks, without shuffling the data.|\n",
    "|[`ds.repartition(shuffle=True)`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.repartition.html#ray.data.Dataset.repartition \"ray.data.Dataset.repartition\")|All-to-all|Repartition the dataset into N blocks, shuffling the data during repartition.|\n",
    "|[`ds.random_shuffle()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.random_shuffle.html#ray.data.Dataset.random_shuffle \"ray.data.Dataset.random_shuffle\")|All-to-all|Randomly shuffle the elements of this dataset.|\n",
    "|[`ds.sort()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.sort.html#ray.data.Dataset.sort \"ray.data.Dataset.sort\")|All-to-all|Sort the dataset by a sortkey.|\n",
    "|[`ds.groupby()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.groupby.html#ray.data.Dataset.groupby \"ray.data.Dataset.groupby\")|All-to-all|Group the dataset by a groupkey.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31c42db-b757-42e9-af0c-45dbf6145d97",
   "metadata": {},
   "source": [
    "> Tip\n",
    ">\n",
    "> Datasets also provides the convenience transformation methods [`ds.map()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map.html#ray.data.Dataset.map \"ray.data.Dataset.map\"), [`ds.flat_map()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.flat_map.html#ray.data.Dataset.flat_map \"ray.data.Dataset.flat_map\"), and [`ds.filter()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.filter.html#ray.data.Dataset.filter \"ray.data.Dataset.filter\"), which are not vectorized (slower than [`ds.map_batches()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html#ray.data.Dataset.map_batches \"ray.data.Dataset.map_batches\")), but may be useful for development.\n",
    "\n",
    "The following is an example to make use of those transformation APIs for processing the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2529b5eb-4313-4425-adb5-e0aaaa666e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import pandas\n",
    "\n",
    "# Create a dataset from file with Iris data.\n",
    "ds = ray.data.read_csv(\"iris.csv\")\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeae0471-d024-4dd2-b27e-a0f4085579dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91a296d-663c-4791-82c1-1415afdc7568",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.repartition(5)\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83b1157-cd49-4979-8935-7a4ff31395f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find rows with sepal.length < 5.5 and petal.length > 3.5.\n",
    "def transform_batch(df: pandas.DataFrame) -> pandas.DataFrame:\n",
    "    return df[(df[\"sepal.length\"] < 5.5) & (df[\"petal.length\"] > 3.5)]\n",
    "\n",
    "# Map processing the dataset.\n",
    "ds.map_batches(transform_batch).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca901a4-a82b-4629-be62-358cb0382661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into 2 datasets\n",
    "ds.split(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b59420-eabf-4470-9281-59794c1c7b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataset by sepal.length.\n",
    "ds = ds.sort(\"sepal.length\")\n",
    "ds.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b4776b-ed2b-471f-bf3d-2393514369d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset.\n",
    "ds = ds.random_shuffle()\n",
    "ds.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aba549e-3422-4858-8ba6-7d7c893b0808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the variety.\n",
    "ds.groupby(\"variety\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e715a601-603b-409f-8e77-260c21b1ab7c",
   "metadata": {},
   "source": [
    "Writing User-defined Functions (UDFs)[](https://docs.ray.io/en/latest/data/transforming-datasets.html#writing-user-defined-functions-udfs \"Permalink to this headline\")\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "User-defined functions (UDFs) are routines that apply on one row (e.g. [`.map()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map.html#ray.data.Dataset.map \"ray.data.Dataset.map\")) or a batch of rows (e.g. [`.map_batches()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html#ray.data.Dataset.map_batches \"ray.data.Dataset.map_batches\")) of a dataset. UDFs let you express your customized business logic in transformations. Here we will focus on [`.map_batches()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html#ray.data.Dataset.map_batches \"ray.data.Dataset.map_batches\") as it's the primary mapping API in Datasets.\n",
    "\n",
    "Here are the basics that you need to know about UDFs:\n",
    "\n",
    "-   A UDF can be either a function, or if using the [actor compute strategy](https://docs.ray.io/en/latest/data/transforming-datasets.html#transform-datasets-compute-strategy), a [callable class](https://docs.ray.io/en/latest/data/transforming-datasets.html#transform-datasets-callable-classes).\n",
    "\n",
    "-   Select the UDF input [batch format](https://docs.ray.io/en/latest/data/transforming-datasets.html#transform-datasets-batch-formats) using the `batch_format` argument.\n",
    "\n",
    "-   The UDF output type determines the Dataset schema of the transformation result.\n",
    "\n",
    "### Callable Class UDFs[](https://docs.ray.io/en/latest/data/transforming-datasets.html#callable-class-udfs \"Permalink to this headline\")\n",
    "\n",
    "When using the actor compute strategy, per-row and per-batch UDFs can also be *callable classes*, i.e. classes that implement the `__call__` magic method. The constructor of the class can be used for stateful setup, and will be only invoked once per worker actor.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> These transformation APIs take the uninstantiated callable class as an argument, not an instance of the class.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eb7185-da7f-4a68-b2aa-500a98245104",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ray.data.read_csv(\"iris.csv\")\n",
    "\n",
    "def is_long(record):\n",
    "    return record['sepal.length'] > 4.7\n",
    "\n",
    "ds.map(is_long).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421f0314-aa55-4f53-8896-6d935372596e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_long_batch(batch):\n",
    "    output = batch['sepal.length'] > 4.7\n",
    "    return output.to_frame()\n",
    "    \n",
    "ds.map_batches(is_long_batch).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988c7a0d-3ad1-4a70-a723-490f01251dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class ModelUDF:\n",
    "    def __init__(self):\n",
    "        self.model = lambda df: df[\"sepal.length\"] > 4.7\n",
    "\n",
    "    def __call__(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Apply model.\n",
    "        df[\"output\"] = self.model(df)\n",
    "        return df\n",
    "\n",
    "ds.map_batches(ModelUDF, compute=\"actors\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a40bb82-e09b-44d2-b418-2a5ba62c8a07",
   "metadata": {},
   "source": [
    "For more info on...\n",
    "* batch formats\n",
    "* zero-copy\n",
    "* compute strategy\n",
    "\n",
    "...refer to https://docs.ray.io/en/latest/data/transforming-datasets.html#udf-input-batch-format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96da390-ecf6-415b-83e5-cc42b7155e57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "import numpy as np\n",
    "\n",
    "dogs_train = 's3://anonymous@air-example-data-2/imagenette2/train/n02102040'\n",
    "dogs_val = 's3://anonymous@air-example-data-2/imagenette2/val/n02102040'\n",
    "fish_train = 's3://anonymous@air-example-data-2/imagenette2/train/n01440764'\n",
    "fish_val = 's3://anonymous@air-example-data-2/imagenette2/val/n01440764'\n",
    "\n",
    "train_ds_images = ray.data.read_images(dogs_train).limit(50).union(ray.data.read_images(fish_train).limit(50))\n",
    "\n",
    "labels = np.zeros(100, dtype=np.uint8)\n",
    "labels[50:] = 1\n",
    "\n",
    "train_ds_labels = ray.data.from_numpy(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fc87b9-f35d-4fbd-af47-5b1605390ae9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds_images.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344b8e3c-642a-4cb3-8e30-4bc8a0adf578",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "im = train_ds_images.take(1)[0]['image']\n",
    "PIL.Image.fromarray(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ec720f-7095-4f30-986a-970014d2eb76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds_labels.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a38b88a-bd28-45b5-aeaa-2f5f574af204",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds = train_ds_images.zip(train_ds_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9227bca7-a0f9-4e8d-85fc-a9815abd1626",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(model_name_or_path)\n",
    "\n",
    "    def transform(example_batch):\n",
    "        # Take a list of PIL images and turn them to pixel values\n",
    "        inputs = feature_extractor([x for x in example_batch['image']], return_tensors='pt')\n",
    "\n",
    "        # Don't forget to include the labels!\n",
    "        inputs['labels'] = example_batch['labels']\n",
    "        return inputs\n",
    "\n",
    "    prepared_ds = ds.with_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78403a2-fa4b-4820-b6df-4ef1ba9af1a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c045f2d5-3de8-40ac-ab6e-43329e6ab8fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
