{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a76c85f-b220-4ea1-a0e4-3b23a50468e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8057ff74-0b84-4604-8dce-ce99cf3f6d2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = load_dataset('beans', cache_dir='/mnt/local_storage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645b5c15-94ed-412e-b4c0-38f7db6cf2e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1325ca28-ecda-4075-8433-2e61da64c5ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform(example_batch):\n",
    "    # Take a list of PIL images and turn them to pixel values\n",
    "    inputs = feature_extractor([x for x in example_batch['image']], return_tensors='pt')\n",
    "\n",
    "    # Don't forget to include the labels!\n",
    "    inputs['labels'] = example_batch['labels']\n",
    "    return inputs\n",
    "\n",
    "prepared_ds = ds.with_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477199dd-f817-448f-8605-a583d1967918",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['labels'] for x in batch])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9eacad-bb6b-4c57-978e-bcf3efa5e542",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metric = load_metric(\"accuracy\")\n",
    "def compute_metrics(p):\n",
    "    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e331a029-1920-4ea3-8fe4-41e42cc2d3ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = ds['train'].features['labels'].names\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=len(labels),\n",
    "    id2label={str(i): c for i, c in enumerate(labels)},\n",
    "    label2id={c: str(i) for i, c in enumerate(labels)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363a9657-9d88-471a-a116-4d83d8cf5411",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "  output_dir=\"/mnt/local_storage/vit-base-beans-demo-v5\",\n",
    "  per_device_train_batch_size=16,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=4,\n",
    "  fp16=True,\n",
    "  save_steps=100,\n",
    "  eval_steps=100,\n",
    "  logging_steps=10,\n",
    "  learning_rate=2e-4,\n",
    "  save_total_limit=2,\n",
    "  remove_unused_columns=False,\n",
    "  push_to_hub=False,\n",
    "  report_to='tensorboard',\n",
    "  load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe47e52-7efb-481e-82fb-a1a84c1fc873",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=prepared_ds[\"train\"],\n",
    "    eval_dataset=prepared_ds[\"validation\"],\n",
    "    tokenizer=feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894a471a-f35d-421c-9fe7-631584d5decd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_results = trainer.train()\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce288659-f97b-4a0c-9278-4e1f584d60da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
